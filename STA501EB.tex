% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={STA501 E-Pack: Applied Statistical Methods},
  pdfauthor={Cheng Peng},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{STA501 E-Pack: Applied Statistical Methods}
\author{Cheng Peng}
\date{West Chester University}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

This \emph{E-coursepack} is a self-contained homegrown eBook that contains all topics covered in current STA 501 at WCU.

The audience of this class is graduate students from life science. The objective is to equip students with applied statistical methods that are used to analyze data generated from their fields.

The coverage of this course is from descriptive statistics to ANOVA, contingency tables, and generalized linear models.

\begin{itemize}
\tightlist
\item
  Setting up computing tools - getting started with R, RStudio, and R Markdown
\item
  Sampling and Experimental Design\\
\item
  Data Visualization and Descriptive Statistics\\
\item
  Standard scores, the Normal, t, Chi-Squared, and F Distributions\\
\item
  Sampling distributions\\
\item
  Confidence Intervals\\
\item
  Tests of a single mean/proportion and two means/proportions\\
\item
  ANOVA\\
\item
  Correlation and Simple Linear Regression\\
\item
  Multiple Linear Regression\\
\item
  Binary Categorical Regression\\
\item
  Frequency Count Regression\\
\item
  Procedures Related to Nominal Data\\
\item
  Power and Sample Size Determination
\end{itemize}

Most case studies are based on (field and laboratory) data taken from the fields of biology, ecology, clinical, health science, etc. A formal statistical programming language R is used for data analysis. Students are not assumed to have prior experience in R coding. In the meanwhile, RMarkdown is an R package that can be used to combine text, R code, and the output from the execution of that code. Therefore, we can use RMarkdown to do an analysis and report the analysis at the same time in the same document.

\hypertarget{r-rstudio-and-rmarkdown}{%
\chapter{R, RStudio and RMarkdown}\label{r-rstudio-and-rmarkdown}}

This chapter introduces open-source free computation and technical writing tools for this course: R, RStudio, and RMarkdown.

\hypertarget{r}{%
\section{R}\label{r}}

R is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. \textbf{-- Wikipedia}

The official R web page has download links: \url{https://www.r-project.org/}. You can download and install the most current version of R based on your machine's operating system.

\hypertarget{order-of-operations}{%
\subsection{Order of Operations}\label{order-of-operations}}

The order of basic operations that we will use in this class is given below.

\textbf{PEMDAS}: \textbf{P}arentheses =\textgreater{} \textbf{E}xponential =\textgreater{} \textbf{M}ultiplication =\textgreater{} \textbf{D}ivision =\textgreater{} \textbf{A}ddition =\textgreater{} \textbf{S}ubtraction!

When calculating confidence intervals and test statistics based on given formulas, please keep { PEMDAS } in mind!

\begin{verbatim}
(2+7)/(3^2)*5-1 = 9/9*5-1 = 5 - 1 = 4
\end{verbatim}

We check the answer directly by typing the following in the R Console:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DecValTok{2}\SpecialCharTok{+}\DecValTok{7}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(}\DecValTok{3}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\DecValTok{5{-}1} 
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

\hypertarget{basic-r-objects}{%
\subsection{Basic R Objects}\label{basic-r-objects}}

We introduce several basic R objects (or R data structures): vectors, matrices, lists, and data frames.

\textbf{R is case-sensitive, so name and Name will refer to different objects!}

\begin{verbatim}
Name <- 1
name <- 0
\end{verbatim}

\hypertarget{vectors}{%
\subsubsection{Vectors}\label{vectors}}

\begin{itemize}
\tightlist
\item
  An R \textbf{vector} holds a set of numerical values or character strings. For example,
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num.vec }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\FloatTok{2.1}\NormalTok{, }\FunctionTok{log}\NormalTok{(}\DecValTok{5}\NormalTok{), pi, }\FunctionTok{sin}\NormalTok{(}\DecValTok{2}\NormalTok{), }\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{), }\DecValTok{0}\NormalTok{) }\CommentTok{\# numerical vector}
\NormalTok{num.vec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.0000000 4.0000000 2.1000000 1.6094379 3.1415927 0.9092974 0.6065307 0.0000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{char.vec }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"john"}\NormalTok{, }\StringTok{"david"}\NormalTok{, }\StringTok{"jones"}\NormalTok{, }\StringTok{"kate"}\NormalTok{)           }\CommentTok{\# character vector}
\NormalTok{char.vec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "john"  "david" "jones" "kate"
\end{verbatim}

\textbf{Note} A single scalar is considered a vector (i.e., one-dimensional vector).

\hypertarget{matrices}{%
\subsubsection{Matrices}\label{matrices}}

An R \textbf{matrix} is a rectangular table that holds either numerical or character values, but not both types of values. The following are two examples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num.mtx }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(num.vec, }\AttributeTok{ncol=}\DecValTok{2}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{num.mtx}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]      [,2]
## [1,] 1.0000000 4.0000000
## [2,] 2.1000000 1.6094379
## [3,] 3.1415927 0.9092974
## [4,] 0.6065307 0.0000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{char.mtx }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(char.vec, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{char.mtx}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]    [,2]   
## [1,] "john"  "jones"
## [2,] "david" "kate"
\end{verbatim}

Note that the values in vectors and matrices must be in the same data type. A scalar is also considered as a 1-by-1 matrix.

\hypertarget{lists}{%
\subsubsection{Lists}\label{lists}}

A list is an R structure that may contain objects of any other type, including other lists. Lots of the modeling functions produce lists as their return values. We define a list to hold vectors and matrices defined in the previous sub-sections.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.list }\OtherTok{=} \FunctionTok{list}\NormalTok{(}\AttributeTok{numvec=}\NormalTok{num.vec, }\AttributeTok{charvec=}\NormalTok{char.vec, }\AttributeTok{nummtx =}\NormalTok{num.mtx, }\AttributeTok{charmtx=}\NormalTok{char.mtx )}
\NormalTok{my.list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $numvec
## [1] 1.0000000 4.0000000 2.1000000 1.6094379 3.1415927 0.9092974 0.6065307 0.0000000
## 
## $charvec
## [1] "john"  "david" "jones" "kate" 
## 
## $nummtx
##           [,1]      [,2]
## [1,] 1.0000000 4.0000000
## [2,] 2.1000000 1.6094379
## [3,] 3.1415927 0.9092974
## [4,] 0.6065307 0.0000000
## 
## $charmtx
##      [,1]    [,2]   
## [1,] "john"  "jones"
## [2,] "david" "kate"
\end{verbatim}

The following example shows how to access the objects in an R list.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.list}\SpecialCharTok{$}\NormalTok{nummtx}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]      [,2]
## [1,] 1.0000000 4.0000000
## [2,] 2.1000000 1.6094379
## [3,] 3.1415927 0.9092974
## [4,] 0.6065307 0.0000000
\end{verbatim}

\hypertarget{data-frame}{%
\subsubsection{Data Frame}\label{data-frame}}

A data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column.

The following are the characteristics of a data frame.

\begin{itemize}
\item
  The column names should be non-empty.
\item
  The row names should be unique.
\item
  The data stored in a data frame can be of numeric, factor, or character type.
\item
  Each column should contain the same number of data items.
\end{itemize}

The following is an example of a data frame

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create the data frame.}
\NormalTok{emp.data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
   \AttributeTok{emp.id =} \FunctionTok{c}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{), }
   \AttributeTok{emp.name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Rick"}\NormalTok{,}\StringTok{"Dan"}\NormalTok{,}\StringTok{"Michelle"}\NormalTok{,}\StringTok{"Ryan"}\NormalTok{,}\StringTok{"Gary"}\NormalTok{),}
   \AttributeTok{salary =} \FunctionTok{c}\NormalTok{(}\FloatTok{623.3}\NormalTok{,}\FloatTok{515.2}\NormalTok{,}\FloatTok{611.0}\NormalTok{,}\FloatTok{729.0}\NormalTok{,}\FloatTok{843.25}\NormalTok{), }
   
   \AttributeTok{start\_date =} \FunctionTok{as.Date}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"2012{-}01{-}01"}\NormalTok{, }\StringTok{"2013{-}09{-}23"}\NormalTok{, }\StringTok{"2014{-}11{-}15"}\NormalTok{, }\StringTok{"2014{-}05{-}11"}\NormalTok{,}
      \StringTok{"2015{-}03{-}27"}\NormalTok{))}
\NormalTok{)}
\CommentTok{\# Print the data frame.         }
\FunctionTok{print}\NormalTok{(emp.data) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   emp.id emp.name salary start_date
## 1      1     Rick 623.30 2012-01-01
## 2      2      Dan 515.20 2013-09-23
## 3      3 Michelle 611.00 2014-11-15
## 4      4     Ryan 729.00 2014-05-11
## 5      5     Gary 843.25 2015-03-27
\end{verbatim}

\hypertarget{rstudio}{%
\section{RStudio}\label{rstudio}}

RStudio is a must-know tool for everyone who works with the R programming language. It's used in data analysis to import, access, transform, explore, plot, model data, and make predictions on data.

\hypertarget{rstudio-gui}{%
\subsection{RStudio GUI}\label{rstudio-gui}}

The RStudio interface consists of several windows. I insert an image of a regular RStudio GUI.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img01/RStudioGUI01} 

}

\caption{List of all variables and the description of each variable}\label{fig:unnamed-chunk-12}
\end{figure}

\hypertarget{console}{%
\subsection{Console}\label{console}}

We can type commands directly into the console, or write in a text file, and then send the command to the console. It is convenient to use the console if your task involves one line of code. Otherwise, we should always use an editor to write code and then run the code in the Console.

\hypertarget{source-editor}{%
\subsection{Source Editor}\label{source-editor}}

Generally, we will want to write programs longer than a few lines. The Source Editor can help you open, edit, and execute these programs.

\hypertarget{environment-window}{%
\subsection{Environment Window}\label{environment-window}}

The Environment window shows the objects (i.e., data frames, arrays, values, and functions) in the environment (workspace). We can see the descriptive information such as the types and dimensions of the objects in your environment. We also choose data sources from the environment to view in the source window like a spreadsheet.

\hypertarget{system-and-graphic-files}{%
\subsection{System and Graphic files}\label{system-and-graphic-files}}

The Files tab has a navigable file manager, just like the file system on your operating system. The Plot tab is where the graphics you create will appear. The Packages tab shows you the packages that are installed and those that can be installed (more on this just now). The Help tab allows you to search the R documentation for help and is where the help appears when you ask for it from the Console.

\hypertarget{rstudio-offers-numerous-helpful-features}{%
\subsection{RStudio offers numerous helpful features:}\label{rstudio-offers-numerous-helpful-features}}

\begin{itemize}
\tightlist
\item
  A user-friendly interface
\item
  The ability to write and save reusable scripts
\item
  Easy access to all the imported data and created objects (like variables, functions, etc.)
\item
  Exhaustive help on any object
\item
  Code autocompletion
\item
  The ability to create projects to organize and share your work with your collaborators more efficiently
\item
  Plot previewing
\item
  Easy switching between terminal and console
\end{itemize}

After you install R on your machine, you can go to \url{https://posit.co/products/open-source/rstudio/} to download the free version of RStudio and install it. R will be automatically connected to RStudio. You can then open the Markdown through the GUI of RStudio.

\hypertarget{rmarkdown}{%
\section{RMarkdown}\label{rmarkdown}}

An R Markdown document is a text-based file format that allows you to include descriptive text, code blocks, and code output. It can be converted to other types of files such as PDF, HTML, and WORD that can include code, plots, and outputs generated from the code chunks.

\hypertarget{code-chunk}{%
\subsection{Code Chunk}\label{code-chunk}}

In R Markdown, we can embed R code in the code chunk defined by the symbol \texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{\}} and closed by \texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}}. The symbol \texttt{} `, also called \textbf{backquote} or \textbf{backtick}, can be found on the top left corner of the standard keyboard as shown in the following.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img01/Key4CodeChunk} 

}

\caption{The location of backquote on the standard keyboard}\label{fig:unnamed-chunk-13}
\end{figure}

There are two code chunks: executable and non-executable chunks. The following code chunk is non-executable since there is no argument specified in the \texttt{\{\}}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img01/Non-executable-code-chunk} 

}

\caption{Non-executable code chunk.}\label{fig:unnamed-chunk-14}
\end{figure}

\begin{verbatim}
This is a code chunk
\end{verbatim}

To write a code chunk that will be executed, we can simply put the letter \texttt{r} inside the curly bracket. If the code the code chunk is executable, you will the green arrow on the top-right corner of the chunk.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img01/Executable-code-chunk} 

}

\caption{Executable code chunk.}\label{fig:unnamed-chunk-15}
\end{figure}

We can define R objects with and without any outputs. In the above R code chunk, we define an R object under the name \texttt{x} and assign the value 5 to \texttt{x} (the first line of the code). We also request an output that prints the value of \texttt{x}. The above executable code chunk gives output \texttt{{[}1{]}\ 5} in the Markdown document. The same output in the knit output files is in a box with a transparent background in the form \texttt{\#\#\ {[}1{]}\ 5}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \DecValTok{5}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

We can also use an argument in the code chunk to control the output. For example, the following code chunk will be evaluated when kitting to other formats of files. But we can still click the green arrow inside the code chunk to evaluate the code.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img01/Executable-code-chunk-argument} 

}

\caption{Executable code chunk with control options.}\label{fig:unnamed-chunk-17}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \DecValTok{5}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\hypertarget{graphics-generated-from-r-code-chunks}{%
\subsection{Graphics Generated from R Code Chunks}\label{graphics-generated-from-r-code-chunks}}

In the previous sub-sections, we include images from external image files. In fact, can use the R function to generate graphics (other than interacting with plots, etc.) in the markdown file \& knit. For instance, we can generate the following image from R and include it in the Markdown document and the knitter output files.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-19-1.pdf}

\hypertarget{data-collection-and-data-loading}{%
\chapter{Data Collection and Data Loading}\label{data-collection-and-data-loading}}

We briefly introduce the basic sampling plans and study design to collect valid data for statistical analysis and modeling. Since all analyses will be based on R, we will demonstrate to to load data sets in different formats to R.

\hypertarget{sampling-plans}{%
\section{Sampling Plans}\label{sampling-plans}}

In general, probability sampling plans generate samples suitable for inferential statistics (such as confidence intervals and testing hypotheses).

\hypertarget{simple-random-sampling-srs}{%
\subsection{Simple Random Sampling (SRS)}\label{simple-random-sampling-srs}}

In this sampling plan, each subject is randomly chosen in such a way that each subject in the population has an equal chance, or probability, of being selected. In mathematical statistics, there is a rigorous definition of SRS plan.

\hypertarget{systematic-sampling}{%
\subsection{Systematic sampling}\label{systematic-sampling}}

In this sampling plan, we (randomly) label all subjects in a population from 1, 2, \ldots, N (population size), randomly choose a label, say \(n_0\), and then select every \(k^{th}\) subjects to include in the sample. That resulting sample is called a systematic sample. Because the

\hypertarget{stratified-sampling}{%
\subsection{Stratified sampling}\label{stratified-sampling}}

This sampling plan assumes that the target population of interest has several naturally defined sub-populations, then we take a sub-sample (SRS) from each sub-population such that the sub-sample sizes are proportional to their corresponding sub-population sizes.

\hypertarget{study-designs}{%
\section{Study Designs}\label{study-designs}}

Depending on whether the variables in the study were modified or filtered by researchers, we can categorize the study designs into observational and experimental studies.

\hypertarget{observational-studies}{%
\subsection{Observational Studies}\label{observational-studies}}

Observational studies are ones where researchers observe the effect of a risk factor, diagnostic test, treatment, or other intervention \textbf{without trying to remove confounding factors}.

\textbf{Cross-sectional studies} collect information on a population by taking a snapshot or cross-section of the population. These studies usually involve one contact with the study population and are relatively cheap to undertake.

\textbf{Pre-test/post-test} Take two cross-sectional samples taken from randomly selected subjects before and after an intervention (such as a new treatment) applied to the randomly selected subjects in the study. The objective is to see whether a characteristic of the sample changed before and after the intervention.

\textbf{Retrospective studies} use historical information to study the characteristics of a population. Sometimes current information about the population is not available or difficult to obtain, we then use the historical information \textbf{under certain assumptions}.

\textbf{Longitudinal studies} follow study subjects over a period of time with repeated data collection throughout. Most are observational studies that seek to identify a correlation among various factors. Thus, longitudinal studies do not manipulate variables and are not often able to detect causal relationships.

\hypertarget{experimental-designs}{%
\subsection{Experimental Designs}\label{experimental-designs}}

Experimental studies are ones where researchers \textbf{carefully design experiments to remove potential confounding factors} introduce an intervention and then study the effects.

\textbf{Prospective studies}, also called \textbf{follow-up study}, in which you select subjects randomly and wait for a period of time to record the formation of interest to perform statistical analysis.

\textbf{Randomized controlled trials (RCT)} in clinical studies are always prospective studies and often involve following a ``cohort'' of individuals to determine the relationship between various variables.

\textbf{Longitudinal studies} can also be considered as experimental studies.

\hypertarget{loading-data-to-r-from-external-data-files}{%
\section{Loading Data to R from External Data Files}\label{loading-data-to-r-from-external-data-files}}

Three basic types of data files are common in practice: csv, txt, and xls(x). I have uploaded the well-known **iris* data set to the course web page in the aforementioned formats. You can practice the R functions to load these external data sets to R.

\hypertarget{text-file-aka.-delimited-text-file}{%
\subsection{Text File ( aka. Delimited Text File)}\label{text-file-aka.-delimited-text-file}}

R command \textbf{read.table()} will load text files in R.

\textbf{Caution:} If the data file contains \textbf{missing values}, you need to handle the missing values before loading it to R. If you take a formal programming course, you will write several lines of code to clean the data. In this class, we only use basic R commands to do analysis.

The code in the following code chunk does not work!!!

\begin{verbatim}
placement.data = read.table("C:\\STA501\\w02\\placement.txt", header=TRUE) 
\end{verbatim}

\hypertarget{read-files-from-a-remote-web-server}{%
\subsubsection{Read files from a remote web server}\label{read-files-from-a-remote-web-server}}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\# delimited text file}
\NormalTok{irisTXT}\OtherTok{=}\StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/Data/w02{-}iris.txt"}
\NormalTok{iris.text }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(irisTXT, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\DocumentationTok{\#\#\# csv file}
\NormalTok{irisCSV }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/Data/w02{-}iris.csv"}
\NormalTok{iris.csv }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(irisCSV, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that there is no commands such as \textbf{read.table()} and \textbf{read.csv()} in the base R to read Excel file from URL. There are several R functions in different libraries, such as \textbf{read\_xlsx()} and \textbf{read\_xls()} in library \textbf{\{readxl\}}, can read Excel file from the local drive (see the example in the next sub-setion).

\hypertarget{read-files-from-local-folder}{%
\subsubsection{Read files from local folder}\label{read-files-from-local-folder}}

\begin{verbatim}
### delimited text file
iris.text.loc = read.table("C:\\STA501\\w02\\w02-iris.txt", header = TRUE)
### csv file
iris.csv.loc = read.table("C:\\STA501\\w02\\w02-iris.csv", header = TRUE)
## Excel file - no built-in command in the base R can read Excel file to R. 
## need to load a command in a R library.
library(readxl)
iris.xlsx.loc = read_xlsx("C:\\STA501\\w02\\w02-iris.xlsx")
\end{verbatim}

\hypertarget{data-frame-and-list}{%
\section{Data Frame and List}\label{data-frame-and-list}}

Data frame can hold different type of variables but requires variables to be equal in length. If you have variables in different types \textbf{and} having different length, you need to use list to hold these variables.

\hypertarget{data-frame-1}{%
\subsection{Data Frame}\label{data-frame-1}}

\begin{itemize}
\tightlist
\item
  We define following categrical and numeric vectors (data sets)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{veca }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\DecValTok{26}           \CommentTok{\#  c() is unnecessary since colon (:) is a shortcut to}
                      \CommentTok{\#  define a patterned sequence. 1, 2, 3, ..., 24, 25, 26.}
\NormalTok{vecb }\OtherTok{=} \DecValTok{101}\SpecialCharTok{:}\DecValTok{126}
\NormalTok{vecc }\OtherTok{=} \DecValTok{201}\SpecialCharTok{:}\DecValTok{226}
\NormalTok{vecd }\OtherTok{=}\NormalTok{ letters        }\CommentTok{\#  26 letters (lower case)}
\NormalTok{vece }\OtherTok{=}\NormalTok{ LETTERS        }\CommentTok{\#  upper{-}case letters}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Example 1}. Define data frame with different types of variables
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataframe01 }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{A=}\NormalTok{veca, }\AttributeTok{B =}\NormalTok{ vecb, }\AttributeTok{D =}\NormalTok{ vecd, }\AttributeTok{E =}\NormalTok{ vece)  }\CommentTok{\# 26{-}by{-}3 data frame}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Example 2}. Define a data frame using vectors with unequal width. The issue is that R will recycle the small vectors \textbf{whose lengths are factors of the length of the longest vector} to make equal lengths columns in the data frame.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# }\AlertTok{CAUTION}\DocumentationTok{: If the lengths of individual variables are different, }
\DocumentationTok{\#\# R will recycle the values in the }
\DocumentationTok{\#\#  mall data (short vector) to make the equal length across the columns}
\DocumentationTok{\#\#}
\NormalTok{dframe2 }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{A =}\NormalTok{ veca,            }\CommentTok{\# this is a vector with 100 values}
                     \AttributeTok{X =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{13}\NormalTok{,            }\CommentTok{\# this vector has 13 values}
                     \AttributeTok{c =} \FunctionTok{sum}\NormalTok{(veca}\SpecialCharTok{+}\NormalTok{vecb}\SpecialCharTok{+}\NormalTok{vecc)    }
                                  \CommentTok{\# two comments: 1. c is reserved for defining}
                                  \CommentTok{\# vectors. Should not be used a name  }
                                  \CommentTok{\# of any objects in R}
                                  \CommentTok{\# 2. this is also a single value.}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Example 3}. The following code will produce an error because Y has 4 rows and A has 26 rows!
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dframe2 }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{A =}\NormalTok{ veca,             }\CommentTok{\# this is a vector with 100 values}
                     \AttributeTok{Y =} \FunctionTok{c}\NormalTok{(}\DecValTok{33}\NormalTok{,}\DecValTok{44}\NormalTok{,}\DecValTok{55}\NormalTok{,}\DecValTok{66}\NormalTok{),   }\CommentTok{\#  4 values in vector b!}
                     \AttributeTok{W =} \FunctionTok{c}\NormalTok{(}\DecValTok{99}\NormalTok{,}\DecValTok{999}\NormalTok{)         }\CommentTok{\#  2 values in W!}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Error in data.frame(a = veca, b = c(33, 44, 55, 66), c = c(99, 999)) : arguments imply differing number of rows: 26, 4, 2

you can check whether the data frame is correctly defined. I added an option \textbf{eval = FALSE} to the following code chunk to avoid printing out the 100-by-3 data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dframe2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(dframe2)   }\CommentTok{\# no error. This may not be what you expected }
\end{Highlighting}
\end{Shaded}

\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-27-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
                \CommentTok{\# since the data frame has recycling issues.}
\end{Highlighting}
\end{Shaded}

The following code does not work since no specific column is specified to calculate the standard deviation!

\begin{verbatim}
sd(dframe2)   # sd() calculates the standard deviation of a SINGLE vector, but
              # There 3 in the data frame. The error message says that R cannot
              # cannot combine all columns to make a vector. In R, numerical 
              # values are 'double'.
\end{verbatim}

We can calculate the standard deviation of the variables in the data frame one by one. For example,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(dframe2}\SpecialCharTok{$}\NormalTok{A)    }\CommentTok{\# This will NOT generate an error since we calculate the }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7.648529
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
                 \CommentTok{\# standard deviation for a specific variable (column).}
\end{Highlighting}
\end{Shaded}

\hypertarget{descriptive-statistics}{%
\chapter{Descriptive Statistics}\label{descriptive-statistics}}

The note outlines the basic descriptive statistics.

\begin{itemize}
\tightlist
\item
  Data Types
\item
  Tabular and graphic summary of data
\item
  Numerical summary of data
\end{itemize}

\hypertarget{data-types}{%
\section{Data Types}\label{data-types}}

There are different classifications of data types. We use the following simple one

\begin{itemize}
\item
  \textbf{Categorical Variables} - The values of these types of variables do not have numerical meaning in the sense that one can not perform arithmetic operations with the values of these types of variables.

  \begin{itemize}
  \item
    \emph{Ordinal categorical variables} - the values have a natural order. For example, course letter grades: A, B, C, D, and F.
  \item
    \emph{Nominal categorical variables} - the values do not have a natural order. For example, majors in a college: Mathematics, finance, music, biology, etc.
  \end{itemize}
\item
  \textbf{Numerical Variables} - As indicated in the name, the values of numerical variables are numbers.

  \begin{itemize}
  \item
    \emph{Discrete variables} - one can find two values of such variables such there are no meaningful values that fall between the two. For example, the number of children in a household: 1, 2, 3, 4, \ldots. There is no such household that has 2.5 children.
  \item
    \emph{Continuous variables} - For any two distinct values of such variables, any value between the two is meaningful. For example, consider two arbitrarily selected human body temperatures (in Fahrenheit) 97.4 and 97.5, any number between 97.4 and 97.5 could be the temperature of someone in the population (although the person may not be part of the sample).
  \end{itemize}
\end{itemize}

\hypertarget{tabular-and-graphic-summary}{%
\section{Tabular and Graphic Summary}\label{tabular-and-graphic-summary}}

Both tabular and graphic summaries are powerful and effective tools to \textbf{visualize} the (shape of the) distribution of the data.

\hypertarget{categorical-data}{%
\subsection{Categorical Data}\label{categorical-data}}

\textbf{Example 1}: {[}Status of Endangered Species{]}: The data was extracted from the U.S. Fish \& Wildlife Service ECOS Environmental Conservation Online System. The data can be found at the following
\url{https://raw.githubusercontent.com/pengdsci/STA501/main/Data/EndangeredSpecies.csv}

We want to summarize the status of endangered species (one of the columns in the data set).

\hypertarget{frequency-table}{%
\subsubsection{Frequency Table}\label{frequency-table}}

Since the values of categorical data sets are labels, constructing frequency tables of categorical data is straightforward.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{speURL }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/Data/EndangeredSpecies.csv"}
\NormalTok{Species }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(speURL, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)   }\CommentTok{\# read the csv data from the URL }
\FunctionTok{kable}\NormalTok{(}\FunctionTok{t}\NormalTok{(}\FunctionTok{head}\NormalTok{(Species[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,])))     }\CommentTok{\# list first 4 rows of the data}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|l}
\hline
  & 1 & 2 & 3 & 4\\
\hline
scientificName & Acanthorutilus handlirschi & Accipiter fasciatus natalis & Accipiter francesii pusillus & Accipiter gentilis laingi\\
\hline
commonName & Cicek (minnow) & Christmas Island goshawk & Anjouan Island sparrowhawk & Queen Charlotte goshawk\\
\hline
criticalHabitat & N/A & N/A & N/A & N/A\\
\hline
speciesGroup & Fishes & Birds & Birds & Birds\\
\hline
Status & Endangered & Endangered & Endangered & Threatened\\
\hline
specialRules & N/A & N/A & N/A & N/A\\
\hline
whereListed & Wherever found & Wherever found & Wherever found & British Columbia Canada\\
\hline
\end{tabular}

Next, we create a frequency table to include all four types of frequencies.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{speciesGroup }\OtherTok{=}\NormalTok{ Species}\SpecialCharTok{$}\NormalTok{speciesGroup   }\CommentTok{\# extract the column of endangered species}
\NormalTok{freq }\OtherTok{=} \FunctionTok{table}\NormalTok{(speciesGroup)            }\CommentTok{\# frequency count}
\NormalTok{rel.freq }\OtherTok{=}\NormalTok{ freq}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(freq)             }\CommentTok{\# relative frequency}
\NormalTok{cum.freq }\OtherTok{=} \FunctionTok{cumsum}\NormalTok{(freq)               }\CommentTok{\# cumulative frequency}
\NormalTok{cum.rel.freq }\OtherTok{=}\NormalTok{ cum.freq}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(freq)     }\CommentTok{\# cumulative relative frequency}
\NormalTok{freq.table }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{freq =}\NormalTok{freq, }
                   \AttributeTok{rel.freq =}\NormalTok{ rel.freq,}
                   \AttributeTok{cum.freq =}\NormalTok{ cum.freq,}
                   \AttributeTok{cum.rel.freq =}\NormalTok{ cum.rel.freq)    }
\FunctionTok{kable}\NormalTok{(freq.table)     }\CommentTok{\# kable() makes a nice{-}looking table}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r}
\hline
  & freq & rel.freq & cum.freq & cum.rel.freq\\
\hline
Amphibians & 45 & 0.0307377 & 45 & 0.0307377\\
\hline
Arachnids & 17 & 0.0116120 & 62 & 0.0423497\\
\hline
Birds & 342 & 0.2336066 & 404 & 0.2759563\\
\hline
Clams & 124 & 0.0846995 & 528 & 0.3606557\\
\hline
Corals & 24 & 0.0163934 & 552 & 0.3770492\\
\hline
Crustaceans & 28 & 0.0191257 & 580 & 0.3961749\\
\hline
Fishes & 208 & 0.1420765 & 788 & 0.5382514\\
\hline
Insects & 94 & 0.0642077 & 882 & 0.6024590\\
\hline
Mammals & 381 & 0.2602459 & 1263 & 0.8627049\\
\hline
Reptiles & 146 & 0.0997268 & 1409 & 0.9624317\\
\hline
Snails & 55 & 0.0375683 & 1464 & 1.0000000\\
\hline
\end{tabular}

\hypertarget{bar-chart-and-pie-chart}{%
\subsubsection{Bar Chart and Pie Chart}\label{bar-chart-and-pie-chart}}

We use R to create both charts based on the frequency table created in the previous sub-section in the following.

We first draw a simple pie chart. You can add different colors and additional information to the chart. You can visit \url{https://www.statmethods.net/graphs/pie.html} for more examples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{freq }\OtherTok{=} \FunctionTok{table}\NormalTok{(speciesGroup)}
\NormalTok{group }\OtherTok{=} \FunctionTok{names}\NormalTok{(freq)}
\FunctionTok{pie}\NormalTok{(freq, }\AttributeTok{labels =}\NormalTok{ group, }\AttributeTok{main=}\StringTok{"Pie Chart of Species Group"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-32-1} \end{center}

Since there are too many slices in the pie chart, it is not easy to add frequencies to the chart. This is not a good visualization. Next, we create a bar chart to represent the distribution of the same data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{freq }\OtherTok{=} \FunctionTok{table}\NormalTok{(speciesGroup)}
\NormalTok{group }\OtherTok{=} \FunctionTok{names}\NormalTok{(freq)         }\CommentTok{\# categories}
\FunctionTok{barplot}\NormalTok{(freq,               }\CommentTok{\# frequency table}
        \AttributeTok{names.arg=}\NormalTok{group,    }\CommentTok{\# tick marks}
        \AttributeTok{las=}\DecValTok{3}\NormalTok{,              }\CommentTok{\# }
        \AttributeTok{main=}\StringTok{"Distribution of Species Group"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-33-1} \end{center}

The above bar plot was created using the function in Base R. We can also use relevant functions in different R packages to make a bar chart that may contain additional information. For example, the R function \textbf{BarChart()} in library \textbf{\{lessR\}} generates bar charts with more information based on the original data values. This is different from \textbf{barplot()} which uses the frequency tables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# library(lessR)        \# placed at the beginning of the document}
\FunctionTok{BarChart}\NormalTok{(speciesGroup,  }\AttributeTok{rotate\_x=}\DecValTok{45}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-34-1} \end{center}

There are more examples to use \textbf{BarChart()} in a nice blog \url{https://cran.r-project.org/web/packages/lessR/vignettes/BarChart.html}.

\hypertarget{numerical-data}{%
\subsection{Numerical Data}\label{numerical-data}}

To summarize numerical data sets, we use frequency tables and histograms to visualize the underlying distributions.

We will use the following data set to illustrate the steps to construct frequency tables and histograms using R. The data set \url{https://raw.githubusercontent.com/pengdsci/STA501/main/Data/diet.csv} was used to study the effect of three different diets on weight loss.

\hypertarget{frequency-tables}{%
\subsubsection{Frequency Tables}\label{frequency-tables}}

Unlike categorical data in which the data values are category labels, in numerical data, we need to group data values to create data groups and then construct the corresponding frequency table.

Think about creating a \textbf{data window} by the maximum and minimum data values in the data set then cut the data window into several small data windows with equal width. The data values in each small data window form a data group. In the figure, we assume there is a data set with a minimum value of 21 and a maximum value of 74. We plan to split the data window {[}21, 74{]} into five small data windows \textbf{with equal width}. The cut-off points are {[}21.0, 31.6, 42.2, 52.8, 63.4, 74.0{]} (including minimum and maximum values). We can use the R command to find these cut-offs if we provide the minimum, maximum, and number of small windows to be used for creating the frequency tables and histogram.

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-35-1} \end{center}

We can use the R function \textbf{seq(min, max, length = number-of-windows + 1)} to find cut-off points. For example, in the above figure, the following code yields the cut-off.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# round off the cut{-}offs to 1 decimal point}
\NormalTok{cutoff }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{21}\NormalTok{, }\DecValTok{74}\NormalTok{, }\AttributeTok{length =}\DecValTok{5}\SpecialCharTok{+}\DecValTok{1}\NormalTok{),}\DecValTok{1}\NormalTok{)}
\CommentTok{\# kable() produces a nice looking table in PDF }
\FunctionTok{kable}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(cutoff), }\AttributeTok{align =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)                         }
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l}
\hline
cutoff\\
\hline
21.0\\
\hline
31.6\\
\hline
42.2\\
\hline
52.8\\
\hline
63.4\\
\hline
74.0\\
\hline
\end{tabular}

\textbf{Example 2}: {[}\textbf{Effectiveness of Diets Data}{]} We are interested in creating a histogram of the weights of all participants in the study before starting the three diets. We want to create a frequency table with 6 rows. That is, We will create 6 small data windows to define 6 groups. R function \textbf{cut(x = data-set, breaks=cutoff-points)} .

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dietURL }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/Data/diet.csv"}
\NormalTok{diet }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(dietURL, }\AttributeTok{header=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{pre.weight}\OtherTok{=}\NormalTok{diet}\SpecialCharTok{$}\NormalTok{initial.weight   }\CommentTok{\# extract pre.weight from the data set.}
\DocumentationTok{\#\#\# calculate the cut{-}offs that yield 6 small data windows with equal widths}
\NormalTok{cutoff.pt }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\FunctionTok{min}\NormalTok{(pre.weight), }\FunctionTok{max}\NormalTok{(pre.weight), }\AttributeTok{length =} \DecValTok{6}\SpecialCharTok{+}\DecValTok{1}\NormalTok{) }
\NormalTok{cutoff.pt }\OtherTok{=} \FunctionTok{round}\NormalTok{(cutoff.pt, }\DecValTok{1}\NormalTok{)       }\CommentTok{\# rounding off to keep 1 decimal place}
\DocumentationTok{\#\#\# use R function **cut()** to split the data window into 6 small data windows}
\NormalTok{data.group }\OtherTok{=} \FunctionTok{cut}\NormalTok{(}\AttributeTok{x =}\NormalTok{ pre.weight, }\AttributeTok{breaks=}\NormalTok{cutoff.pt, }\AttributeTok{include.lowest =} \ConstantTok{TRUE}\NormalTok{)}
\DocumentationTok{\#\# use R function **table** to get the frequency table}
\NormalTok{freq.count}\OtherTok{=}\FunctionTok{table}\NormalTok{(data.group)    }\CommentTok{\# regular frequency counts}
\FunctionTok{kable}\NormalTok{(freq.count, }\AttributeTok{align =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l}
\hline
data.group & Freq\\
\hline
[58,63] & 12\\
\hline
(63,68] & 14\\
\hline
(68,73] & 17\\
\hline
(73,78] & 15\\
\hline
(78,83] & 11\\
\hline
(83,88] & 7\\
\hline
\end{tabular}

We can also use the same steps to find relative and cumulative frequencies.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{freq }\OtherTok{=} \FunctionTok{table}\NormalTok{(data.group)              }\CommentTok{\# frequency count}
\NormalTok{rel.freq }\OtherTok{=}\NormalTok{ freq}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(freq)             }\CommentTok{\# relative frequency}
\NormalTok{cum.freq }\OtherTok{=} \FunctionTok{cumsum}\NormalTok{(freq)               }\CommentTok{\# cumulative frequency}
\NormalTok{cum.rel.freq }\OtherTok{=}\NormalTok{ cum.freq}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(freq)     }\CommentTok{\# cumulative relative frequency}
\NormalTok{freq.table }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{freq =}\NormalTok{freq, }
                   \AttributeTok{rel.freq =} \FunctionTok{round}\NormalTok{(rel.freq,}\DecValTok{3}\NormalTok{),   }\CommentTok{\# keep 3 decimal places}
                   \AttributeTok{cum.freq =}\NormalTok{ cum.freq,}
                   \AttributeTok{cum.rel.freq =} \FunctionTok{round}\NormalTok{(cum.rel.freq ,}\DecValTok{3}\NormalTok{)) }\CommentTok{\# keep 3 decimal places}
\FunctionTok{kable}\NormalTok{(freq.table, }\AttributeTok{align =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|l}
\hline
  & freq & rel.freq & cum.freq & cum.rel.freq\\
\hline
[58,63] & 12 & 0.158 & 12 & 0.158\\
\hline
(63,68] & 14 & 0.184 & 26 & 0.342\\
\hline
(68,73] & 17 & 0.224 & 43 & 0.566\\
\hline
(73,78] & 15 & 0.197 & 58 & 0.763\\
\hline
(78,83] & 11 & 0.145 & 69 & 0.908\\
\hline
(83,88] & 7 & 0.092 & 76 & 1.000\\
\hline
\end{tabular}

\hypertarget{graphic-summary---histogram}{%
\subsubsection{Graphic Summary - Histogram}\label{graphic-summary---histogram}}

As mentioned earlier, we use a histogram to visualize the distribution of the numerical data set. R function \textbf{hist(x=data-set, breaks = cutoff)}. We still use the same \textbf{pre.weight} and the same cut-off obtained in the previous subsections to construct the histogram.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(pre.weight, }\AttributeTok{breaks =}\NormalTok{ cutoff.pt,}
     \AttributeTok{main =} \StringTok{"Histogram of Pre{-}Weight"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-38-1} \end{center}

We can see that the distribution of pre-weights is \textbf{skewed to the right} since the above histogram has a long right tail.

\hypertarget{numerical-summary-of-numerical-data}{%
\section{Numerical Summary of Numerical Data}\label{numerical-summary-of-numerical-data}}

Three family measures are outlined in this section: central tendency, variation, and location. We will still use \textbf{pre-weight} as an example to show how to basic R functions to calculate these numerical measures.

\hypertarget{central-tendency}{%
\subsection{Central Tendency}\label{central-tendency}}

We will not list all relevant measures of centers. Three three R functions \textbf{mean()} and \textbf{median()} are used to calculate the mean and median of a given data set.

\textbf{Mean} - the average of the values in the data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{avg.pre.weight }\OtherTok{=} \FunctionTok{mean}\NormalTok{(pre.weight)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(avg.pre.weight), }\AttributeTok{align =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l}
\hline
avg.pre.weight\\
\hline
72.28947\\
\hline
\end{tabular}

\textbf{Median} - a cut-off value that splits the data values into two parts (the cut-off in both parts) such that at least 50\% of data values are \textbf{greater than or equal to} and at least 50\% of data values are \textbf{less than or equal to the} cut-off value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{middle.numer }\OtherTok{=} \FunctionTok{median}\NormalTok{(pre.weight)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(middle.numer), }\AttributeTok{align =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l}
\hline
middle.numer\\
\hline
72\\
\hline
\end{tabular}

The more general quantile function \textbf{quantile()} can also be used to find the median. In fact, \textbf{quantile()} can be any percentile. The 50th percentile is the median.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quantile.mid.num }\OtherTok{=} \FunctionTok{quantile}\NormalTok{(pre.weight, }\CommentTok{\# data set name}
                            \FloatTok{0.5}\NormalTok{,        }\CommentTok{\# percentile, 0.5 = 50\%}
                            \AttributeTok{type=}\DecValTok{2}      \CommentTok{\# there are different interpolations. }
                                        \CommentTok{\# We use type 2.}
\NormalTok{                            )}
\NormalTok{fifty.percentile}\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(quantile.mid.num)}
\FunctionTok{kable}\NormalTok{(fifty.percentile, }\AttributeTok{align =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l}
\hline
  & quantile.mid.num\\
\hline
50\% & 72\\
\hline
\end{tabular}

\hypertarget{variations}{%
\subsection{Variations}\label{variations}}

We use R functions and variable \textbf{pre-weight} to calculate variance, standard deviation, and inter-quartile range (IQR).

\begin{itemize}
\tightlist
\item
  \textbf{Variance} - measure the spread of the data. R function \textbf{var()} calculates the sample variance.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample.var }\OtherTok{=} \FunctionTok{var}\NormalTok{(pre.weight)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(sample.var), }\AttributeTok{align =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l}
\hline
sample.var\\
\hline
63.59509\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  \textbf{Standard Deviation} - measures the spread of the data and is equal to the square root of the variance.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stdev }\OtherTok{=} \FunctionTok{sd}\NormalTok{(pre.weight)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(stdev), }\AttributeTok{align =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l}
\hline
stdev\\
\hline
7.974653\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  \textbf{Inter-quartile Range (IQR)} - the range of the middle 50\% data values. That is, we throw out the bottom and upper 25\% of data values and use the difference between the maximum and the minimum values to define IQR. The idea is illustrated in the following figure {[}I exclude the code in the output file. You can find the RMD document{]}.
\end{itemize}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-44-1} \end{center}

where \textbf{Q1} and \textbf{Q3} are the first and third quartiles which can be found using \textbf{quantile()}. The inter-quartile range is defined to be \textbf{IQR = Q3 - Q3}. We still use the \textbf{pre.weight} to illustrate how to find the IQR with the following code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{IQR }\OtherTok{=} \FunctionTok{quantile}\NormalTok{(pre.weight, }\FloatTok{0.75}\NormalTok{, }\AttributeTok{type =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{quantile}\NormalTok{(pre.weight, }\FloatTok{0.25}\NormalTok{, }\AttributeTok{type =} \DecValTok{2}\NormalTok{)}
\NormalTok{IQR }\OtherTok{=} \FunctionTok{as.vector}\NormalTok{(IQR)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(IQR), }\AttributeTok{align =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l}
\hline
IQR\\
\hline
12\\
\hline
\end{tabular}

\hypertarget{location}{%
\subsection{Location}\label{location}}

\hypertarget{z-score-transforamtion}{%
\subsubsection{Z-score Transforamtion}\label{z-score-transforamtion}}

The z-score transformation converts any given numerical data set to a new standardized data set such the new data set has zero mean and unit standard deviation. Let's denote the original data set to be \(X= \{x_1, x_2, \cdots, x_n\}\). let \(Z =\{z_1, z_2, \cdots, z_n \}\) be the standardized data set. The formula that transforms X to Z is given by

\[
z_i = \frac{x_i-\bar{x}}{s}
\]

where \(\bar{x}\) is the sample mean and \(s\) is the standard deviation of \(X\).

I use the toy data \(X = \{1,3,5,7,9 \}\) as an example to perform the z-score transformation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X}\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{9}\NormalTok{)     }\CommentTok{\# type in data values}
\NormalTok{xbar }\OtherTok{=} \FunctionTok{mean}\NormalTok{(X)          }\CommentTok{\# sample mean}
\NormalTok{s }\OtherTok{=} \FunctionTok{sd}\NormalTok{(X)               }\CommentTok{\# sample standard deviation}
\NormalTok{Z}\OtherTok{=}\NormalTok{(X}\SpecialCharTok{{-}}\NormalTok{xbar)}\SpecialCharTok{/}\NormalTok{s            }\CommentTok{\# z{-}score transformation}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(Z), }\AttributeTok{align =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\AttributeTok{format =} \StringTok{"pipe"}\NormalTok{)   }\CommentTok{\# make a nice looking}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Z \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
-1.2649111 \\
-0.6324555 \\
0.0000000 \\
0.6324555 \\
1.2649111 \\
\end{longtable}

\hypertarget{quantile}{%
\subsubsection{Quantile}\label{quantile}}

A k-th quantile \textbf{of a data set} (also called sample k-th quantile) is defined as a cut-off value that splits the data into two parts such that \textbf{at least} \(100k\%\) of data values are \textbf{bigger than or equal to} the cut-off and \textbf{at least} \(100(1-k)\%\) data values are \textbf{less than or equal to} the cut-off value, where \(0 < k < 100\). Special quantiles are the quartile (quarter) and percentiles (hundredth).

Please keep in mind that the calculation of quantile is based on the sorted data and involves interpolations. Several interpolations were implemented in R. There is a minor difference between these different interpolations. The simple interpolation that is commonly used is the \emph{so-called} type 2 interpolation. The type 1 interpolation is the default type in \textbf{quantile(dataset, k/100, type=2)}.

\textbf{Example} {[}\textbf{Pre-weight data}{]} - We want to find 25\% and 68\% percentiles of pre-weights.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{q}\FloatTok{.25} \OtherTok{=} \FunctionTok{quantile}\NormalTok{(pre.weight, }\FloatTok{0.25}\NormalTok{, }\AttributeTok{type =} \DecValTok{2}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(q}\FloatTok{.25}\NormalTok{), }\AttributeTok{align =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l}
\hline
  & q.25\\
\hline
25\% & 66\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{q}\FloatTok{.68} \OtherTok{=} \FunctionTok{quantile}\NormalTok{(pre.weight, }\FloatTok{0.68}\NormalTok{, }\AttributeTok{type =} \DecValTok{2}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(q}\FloatTok{.68}\NormalTok{), }\AttributeTok{align =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l}
\hline
  & q.68\\
\hline
68\% & 77\\
\hline
\end{tabular}

We can call \textbf{quantile()} to find the two quantiles simultaneously.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{q.}\FloatTok{25.68} \OtherTok{=} \FunctionTok{quantile}\NormalTok{(pre.weight, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.68}\NormalTok{), }\AttributeTok{type =} \DecValTok{2}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(q.}\FloatTok{25.68}\NormalTok{), }\AttributeTok{align =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l}
\hline
  & q.25.68\\
\hline
25\% & 66\\
\hline
68\% & 77\\
\hline
\end{tabular}

\hypertarget{five-number-summary-and-box-plot}{%
\subsubsection{Five-number Summary and Box-plot}\label{five-number-summary-and-box-plot}}

The five-number summary consists of 5 numbers: minimum (0\%), 1st quartile (25\%), 2nd quartile(50\%, median), 3rd quartile (75\%), and maximum (100\%). R function \textbf{fivenum()} is dedicated to finding the five-number summary.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fivenum}\NormalTok{(pre.weight)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 58 66 72 78 88
\end{verbatim}

We can also use \textbf{quantile()} to find the five-number summary in the following.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(pre.weight, }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{type =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   0%  25%  50%  75% 100% 
##   58   66   72   78   88
\end{verbatim}

A box plot is the graphic representation of the five-number summary.

\begin{center}\includegraphics[width=1\linewidth]{img03/w03-boxplot_explanation} \end{center}

R function \textbf{boxplot()} will make the box-plot. We only present a simple box plot in the following.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(pre.weight, }\AttributeTok{horizontal =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-53-1} \end{center}

\hypertarget{assignment---descriptive-statistics}{%
\section{Assignment - Descriptive Statistics}\label{assignment---descriptive-statistics}}

The \textbf{Diabetes} data set to be used in this assignment is taken from \href{https://hbiostat.org/data/}{Vanderbilt's Biostatistics Datasets}.

The following is the description from the web page:

These data are courtesy of Dr.~John Schorling, Department of Medicine, University of Virginia School of Medicine.
The data consists of 19 variables on 403 subjects from 1046 subjects who were interviewed in a study to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia for African Americans. According to Dr.~John Hong, Diabetes Mellitus Type II (adult-onset diabetes) is associated most strongly with obesity. The waist/hip ratio may be a predictor of diabetes and heart disease. DM II is also associated with hypertension - they may both be part of ``Syndrome X''. The 403 subjects were the ones who were actually screened for diabetes. Glycosolated hemoglobin \textgreater{} 7.0 is usually taken as a positive diagnosis of diabetes. For more information about this study see

Willems JP, Saunders JT, DE Hunt, JB Schorling: Prevalence of coronary heart disease risk factors among rural blacks: A community-based study. \emph{Southern Medical Journal} 90:814-820; 1997

Schorling JB, Roach J, Siegel M, Baturka N, Hunt DE, Guterbock TM, Stewart HL: A trial of church-based smoking cessation interventions for rural African Americans. \emph{Preventive Medicine} 26:92-101; 1997.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diaURL }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/Data/diabetes.csv"}
\NormalTok{diabetes }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(diaURL, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{t}\NormalTok{(}\FunctionTok{head}\NormalTok{(diabetes)))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|l|l|l}
\hline
  & 1 & 2 & 3 & 4 & 5 & 6\\
\hline
id & 1000 & 1001 & 1002 & 1003 & 1005 & 1008\\
\hline
chol & 203 & 165 & 228 & 78 & 249 & 248\\
\hline
stab.glu & 82 & 97 & 92 & 93 & 90 & 94\\
\hline
hdl & 56 & 24 & 37 & 12 & 28 & 69\\
\hline
ratio & 3.6 & 6.9 & 6.2 & 6.5 & 8.9 & 3.6\\
\hline
glyhb & 4.31 & 4.44 & 4.64 & 4.63 & 7.72 & 4.81\\
\hline
location & Buckingham & Buckingham & Buckingham & Buckingham & Buckingham & Buckingham\\
\hline
age & 46 & 29 & 58 & 67 & 64 & 34\\
\hline
gender & female & female & female & male & male & male\\
\hline
height & 62 & 64 & 61 & 67 & 68 & 71\\
\hline
weight & 121 & 218 & 256 & 119 & 183 & 190\\
\hline
frame & medium & large & large & large & medium & large\\
\hline
bp.1s & 118 & 112 & 190 & 110 & 138 & 132\\
\hline
bp.1d & 59 & 68 & 92 & 50 & 80 & 86\\
\hline
bp.2s & NA & NA & 185 & NA & NA & NA\\
\hline
bp.2d & NA & NA & 92 & NA & NA & NA\\
\hline
waist & 29 & 46 & 49 & 33 & 44 & 36\\
\hline
hip & 38 & 48 & 57 & 38 & 41 & 42\\
\hline
time.ppn & 720 & 360 & 180 & 480 & 300 & 195\\
\hline
\end{tabular}

We can see from the first 6 observations that there are 15 numerical variables and 3 categorical variables. Variable \textbf{bp.2s} and \textbf{bp.2d} have missing values. To complete this week's assignment, you need to choose one numerical variable and one categorical variable with NO missing values.

The following code shows how to extract variables from the data frame. I will use the two variables with missing values as an example. You can modify the code to extract your variables for the assignment.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp}\FloatTok{.2}\NormalTok{s }\OtherTok{\textless{}{-}}\NormalTok{ diabetes}\SpecialCharTok{$}\NormalTok{bp}\FloatTok{.2}\NormalTok{s}
\NormalTok{bp}\FloatTok{.2}\NormalTok{d }\OtherTok{\textless{}{-}}\NormalTok{ diabetes}\SpecialCharTok{$}\NormalTok{bp}\FloatTok{.2}\NormalTok{d}
\end{Highlighting}
\end{Shaded}

\hypertarget{summarizing-categorical-data}{%
\subsection{Summarizing Categorical Data}\label{summarizing-categorical-data}}

\begin{itemize}
\item
  Use the categorical variable you selected to perform the following analysis

  \begin{itemize}
  \tightlist
  \item
    \textbf{Construct a relative frequency table}. Write a few sentences to describe the distribution of the variable. Note that you are encouraged to construct a frequency table with all four types of frequencies as I did in the class note.
  \item
    \textbf{Construct a pie-chart} to represent the distribution of the categorical variable.
  \end{itemize}
\item
  Using the numerical variable you chose from the diabetes data to answer the following questions.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Construct a relative frequency table of the numerical variable} with 10 categories. In other words, the frequency table should have 10 rows. You are encouraged to include all 4 frequencies in the table. Please provide a brief description of the relative frequencies.
  \item
    \textbf{Construct a histogram of the numerical variable} with 10 vertical bars. In other words, the histogram is a geometric representation of the frequency table. Explain the distribution of the variable. Is it skewed to the left or the right?
  \item
    \textbf{Construct a box-plot} and explain it. That is, can you tell whether the distribution is skewed to the right or the left?
  \end{itemize}
\end{itemize}

\hypertarget{concepts-of-probability-distributions}{%
\chapter{Concepts of Probability Distributions}\label{concepts-of-probability-distributions}}

In this chapter, we provide a non-technical description of random variables and their corresponding probability distributions.

\hypertarget{concepts-of-random-variables}{%
\section{Concepts of random variables}\label{concepts-of-random-variables}}

A random variable is a variable and its value is dependent on chance. What is the difference between a ``regular'' variable we learned from middle school and a \textbf{random variable}?

\begin{itemize}
\item
  \textbf{Example 1}: Let \(x\) be a variable in the equation \(3x +4 = 8\). \(x\) is unknown before you solve for it from the equation. Most importantly, it is a \textbf{fixed} value although it is unknown.
\item
  \textbf{Example 2}: Let \(Y\) be the height of the WCU student population. \(Y\) is unknown before you measure the height of a student from this population. However, which student is selected to measure his/her height is dependent on \textbf{the chance}!
\end{itemize}

We can see from examples 1 and 2 that \(x\) and \(Y\) are variables. \(x\) is a \textbf{``regular'' variable} and \(Y\) is a \textbf{random variable}!

Because the value of a random variable is \textbf{dependent on the chance}, we need additional mathematical tools to characterize the \textbf{chance} - probability distribution. This will be described in a non-technical manner in the next section.

\hypertarget{types-of-random-variables}{%
\section{Types of random variables}\label{types-of-random-variables}}

There are basic types of random variables: discrete and continuous random variables.

\begin{itemize}
\item
  \textbf{Discrete random variables}: A random variable is said to be discrete if its value is obtained by \textbf{counting}. A discrete random variable may have either finite or infinite distinct values.

  \begin{itemize}
  \item
    \textbf{Example 3}: \emph{Coin Flipping Experiment} - Consider flipping an unfair coin (like the ones in the left panel of Figure 1) 10 times. Let \(X=\) the number of \textbf{heads} observed. \(X\) is a discrete random variable since it can take finite values (11 distinct values): 0, 1, 2, \ldots, 10. Further, it can only take more than 11 distinct values since the unfair coin was flipped 10 times! Moreover, \(X\) can never be 2.7! It is discrete!
  \item
    \textbf{Example 4}: \emph{Quadrat Sampling (right panel of Figure 1)} - Consider estimating the total number of dandelions in a field. We know or we can measure the area of the field. The area of the quadrat is fixed. We can throw the quadrat randomly to the different regions in the field multiple times and count the number of dandelions in the squared plots sampled. Since the ratio of the sampled area and the total area of the field is equal to the ratio of the total number of dandelions in the sampled area and the total number of dandelions in the field. We then can solve the equation for the estimated total number of dandelions in the field. Now, let \(Y\) be the number of dandelions inside the quadrat in each sampled region. Clearly, \(Y\) is discrete. Is \(Y\) finite? Technically speaking, the number of dandelions in the quadrat cannot be infinite no matter where it is placed. However, unlike \(X\) in Example 3 which is naturally capped by 11, no cap can be placed on \(Y\). Theoretically speaking, \(Y\) is infinite!
  \end{itemize}
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img04/w04-UnfairCoin-DandelionsSampling} 

}

\caption{ Left: unfair coins. Right: quadrat for ecology sampling - estimating the number of dandelions in a field.}\label{fig:unnamed-chunk-57}
\end{figure}

\begin{itemize}
\item
  \textbf{Continuous random variables} A random variable is continuous if its value is obtained by \textbf{measuring}.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Example 5}: Let \(Y\) be the pH of arterial plasma (i.e., the acidity of the blood) of people of a population. \(Y\) is a typical continuous random variable. It has uncountably many values \textbf{between 0 and 14}. It is continuous since any value between any selected pHs could be the pH of a person in the population.
  \end{itemize}
\end{itemize}

\hypertarget{concepts-of-probability-distributions-1}{%
\section{Concepts of probability distributions}\label{concepts-of-probability-distributions-1}}

We first briefly describe the concept of probability and then outline the probability distributions of random variables.

\hypertarget{concepts-of-probability}{%
\subsection{Concepts of probability}\label{concepts-of-probability}}

Before introducing the definition of probability, we list the following concepts.

\begin{itemize}
\item
  \textbf{(Statistical) Experiment} - a process that produces well-defined outcomes. For example, consider an experiment of flipping a fair coin, the possible outcomes of this experiment are \{heads, tails\}.
\item
  \textbf{Sample Space} - The set of all possible outcomes is called sample space. The \textbf{sample space} of the above coin-toss example is \textbf{S = \{heads, tails\}}.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Example 6}: Consider an experiment of flipping a fair coin \emph{sequentially} three times. We use \textbf{T} to denote \textbf{tails} and \textbf{H} for \textbf{heads}. The sample space of this experiment is given by \textbf{S = \{TTT, TTH, THT, HTT, HHH, HHT, HTH, THH\}} which can be explained by the following figure.
  \end{itemize}
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img04/w04-coin-tossing-exp} 

}

\caption{Sample space of the experiment of flipping a coin three times.}\label{fig:unnamed-chunk-58}
\end{figure}

\begin{itemize}
\item
  \textbf{Event} - a subset of sample space. Two extreme events are the impossible event (i.e., the subset is empty) and the sure event (i.e., the subset is equal to the sample space).

  \begin{itemize}
  \tightlist
  \item
    \textbf{Example 7}: We will define a few events based on the experiment in \textbf{Example 6} in the following.

    \begin{itemize}
    \tightlist
    \item
      E1 = \{observing at least 2 heads\} = \{HHH, HTH, THH, HHT\}.
    \item
      E2 = \{observing exactly one heads\} = \{HTT, THT, TTH\}.
    \item
      E3 = \{observing 5 heads\} = \{\} = empty set = impossible event.
    \item
      E4 = \{observing at one heads \textbf{OR} one tails\} = \textbf{S} = sure event.
    \end{itemize}
  \item
    \textbf{Example 8}: We still use the experiment in \textbf{Example 6} with sample space \textbf{S = \{TTT, TTH, THT, HTT, HHH, HHT, HTH, THH\}}. Define \(Y =\) the number of \textbf{heads} observed in the experiment. We now define \textbf{Events} based on the value of random variable \(Y\).

    \begin{itemize}
    \tightlist
    \item
      E.0 = \{Y=0\} = \{TTT\}.
    \item
      E.1 = \{Y=1\} = \{TTH, THT, HTT\}.
    \item
      E.2 = \{Y=2\} = \{THH, HTH, HHT\}.
    \item
      E.3 = \{Y=1\} = \{HHH\}.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Definitions of probability} - Two technical definitions of probability measure the chance of occurrence of an event.

  \begin{itemize}
  \item
    \emph{Classical probability (based on equally likely outcome)}: P(E) = (\# outcomes in E)/(\# outcomes in S).

    \begin{itemize}
    \tightlist
    \item
      \textbf{Example 9}: The experiment in \textbf{Example 6} is an equally likely outcome experiment. Based on the above definition, we can calculate the probability of the event in \textbf{Example 7}:

      \begin{itemize}
      \tightlist
      \item
        P(E1) = \#E1/\#S = 4/8 = 1/2.
      \item
        P(E2) = \#E2/\#S = 3/8.
      \item
        P(E3) = \#E3/\#S = 0/8 = 0. That is, an impossible event has a probability of 0.
      \item
        P(E4) = \#E4/\#S = \#S/\#S = 1. That is, a sure event has probability 1.
      \end{itemize}
    \end{itemize}
  \item
    \emph{Relative frequency approximation} - If an event is defined based on an unequally likely experiment, we need to repeat the experiment multiple times to observe the number of occurrences and then use the relative frequency to \textbf{approximate} the probability of the event. This definition is used in most practical applications.

    \begin{itemize}
    \tightlist
    \item
      \textbf{Example 10}: Chronic arsenic toxicity, which is due to low-concentration exposure over a long period of time, impairs the same organs and tissues and is a threat to public health. The following map shows the distribution of people with arsenic levels \textgreater{} 10 \(\mu g/L\) by US counties. As an example, Maine is one of the few states with a high level of arsenic. Let's consider a remote northern Maine community where no public water system is available, the drinking water is from private wells. What is the probability that a long-term resident of the community has an arsenic level of more than 10 \(\mu g/L\)? Apparently, that probability is NOT 0.5 (i.e., this is not an equally likely outcome experiment). We survey many residents to measure the arsenic level for each selected resident and record whether the arsenic level is higher than 10 \(\mu g/L\). The desired probability is approximated by the relative frequency of residents with an arsenic level higher than 10 \(\mu g/L\).
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img04/w04-CountyMapForArsenicDist} 

}

\caption{Distribution of people with arsenic level > 10 ug/L by US counties}\label{fig:unnamed-chunk-59}
\end{figure}

\hypertarget{probability-distribution-of-random-variables}{%
\section{Probability distribution of random variables}\label{probability-distribution-of-random-variables}}

The probability distribution of a population (or random variable) contains \textbf{all information} in the population (random variable). The primary questions we need to answer frequently about the distributions.

\begin{itemize}
\item
  \textbf{Finding probabilities}: For any given two values, say \(x_1\) and \(x_2\), (including one or both of the extreme values), we can find the probability \(P(x_1 < X < x_2)\).
\item
  \textbf{Finding quantiles (specific values of the random variable)}: For any value \(x\) (including one of the extreme values) and a probability, say \(p_0\), of a well-defined event, we can find the specific value of \(Y\), say \(x_0\) that was used to define the valid event, from the equation \(P(x < X <x_0) = p_0\) or \(P(x_0 < X <x) = p_0\).
\end{itemize}

There are types of random variables: discrete random variables and continuous random variables. The probability distribution of a random variable provides a way to find the probability of an event defined by a value or a set of values of the random variable.

\hypertarget{discrete-probability-distribution}{%
\subsection{Discrete probability distribution}\label{discrete-probability-distribution}}

The probability distribution of a discrete random variable is a description of the relative frequencies of the corresponding district values.

\begin{itemize}
\tightlist
\item
  \textbf{Example 11}: Refer to \textbf{Example 9}, Let \(Y\) be the number of children. \(Y\) has 4 possible values: 0, 1, 2, 3. Then the probability of each distinct value of \(Y\) is summarized in the following table.
\end{itemize}

\begin{table}

\caption{\label{tab:unnamed-chunk-60}Probability Distribution Table}
\centering
\begin{tabular}[t]{r|r}
\hline
Y & Prob\\
\hline
0 & 0.125\\
\hline
1 & 0.375\\
\hline
2 & 0.375\\
\hline
3 & 0.125\\
\hline
\end{tabular}
\end{table}

With the above table, we can find the probability of all events defined based on the values of random variable \(Y\). The above table is called the \textbf{probability distribution table}. The following graphic representation of the probability distribution table is called the \textbf{probability distribution histogram}.

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-61-1} \end{center}

For example, with the above table, we can two types of questions.

\begin{itemize}
\tightlist
\item
  \textbf{Finding probabilities}

  \begin{itemize}
  \tightlist
  \item
    \(P(Y < 2) = P(0< X < 2) = P(Y=0) + P(Y = 1) = 0.125 + 0.375 = 0.5\).
  \item
    \(P(Y > 2) = P(2 < X \le 3) = P(Y = 3) = 0.125\).
  \end{itemize}
\item
  \textbf{Finding quantiles}

  \begin{itemize}
  \tightlist
  \item
    \(P(0< X < x_0) = 0.5 \to x_0 = 1.\)
  \item
    \(P(1<X<x_0) = 0.375 \to x_0 = 0.375.\)
  \end{itemize}
\end{itemize}

\textbf{Caution}: The above two types of questions can be complicated in discrete distribution. This \href{https://valelab4.ucsf.edu/svn/3rdpartypublic/boost/libs/math/doc/sf_and_dist/html/math_toolkit/policy/pol_tutorial/understand_dis_quant.html}{blog post} explains this complexity in some detail. We will NOT use the discrete distribution directly in this class, instead, we use will the normal distribution (in the next section).

\textbf{Remark}: We can see from the above examples that the definition of an event associated with a discrete random variable is a value or set of values.

\hypertarget{continuous-probability-distribution}{%
\subsection{Continuous probability distribution}\label{continuous-probability-distribution}}

A continuous random variable has uncountably many distinct values. That is, for any two distinct values of the continuous random variable, no matter how close they are, there are still uncountably many values in between. because this property, \textbf{an event associated with a continuous random variable is defined to be an interval or the union of some intervals of the values of the random variable}.

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-62-1} \end{center}

Unlike in the case of the discrete random variable in which the height of the vertical bar in the probability histogram is defined to be the probability of observing that corresponding value, \textbf{for any continuous random variable}, we define the probability of an event that is defined based on an interval \([a, b]\) to be

\begin{itemize}
\item
  \$P(a \textless{} X \textless{} b) = \$ the area of the region defined by \(a, b\) and the density curve (see the above figure).
\item
  As a special case, \(P(X = c) = 0\). Moreover, \(P(X=c_1, c_2, c_3,\cdots )=0\). That is, \textbf{the probability of observing countably many values of a continuous random variable is ALWAYS ZERO!}.
\item
  \textbf{Two Basic Types of Questions}: finding probabilities and quantiles.

  \begin{itemize}
  \item
    \emph{Finding probabilities}: for any given two values (including possibly one of or both \(-\infty\) and \(\infty\)), say \(a, b\), then \(P(a<X<b)=\) area of the shaded region as shown in the above Figure. As a special case, if \(a = b\), then \(P(X = a) = 0\).
  \item
    \emph{Finding quantiles}: for a given value, say \(x\), of \(X\) (possibly including \(\infty\) or \(-\infty\)) and a probability, say \(p_0\), we can find the other value \(x_0\) from \(P(x < X < x_0) = p_0\) if (\(x \le x_0\)). As a special case, if the given value is \(\infty\) and \(-\infty\) (both are not valid values of \(X\)!), then \(x_0\) that satisfies \(P(X < x_0) = P(-\infty<X<x_0) = p_0\) is called the \(100p_0^{th}\) quantile (see the following figure).
  \end{itemize}
\end{itemize}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-63-1} \end{center}

In the next section, we will introduce several distributions of special continuous random variables.

\hypertarget{special-continuous-distributions}{%
\section{Special Continuous Distributions}\label{special-continuous-distributions}}

Four distributions will be used in this course. We introduce the first two of them: normal distribution and t-distribution.

\hypertarget{normal-distribution}{%
\subsection{Normal Distribution}\label{normal-distribution}}

The general normal distribution has a bell-shaped distribution as shown in the following figure. A normal distribution is uniquely determined by its mean and variance. We usually use notation \(N(\mu, \sigma^2)\), where \(\mu\) and \(\sigma^2\) are the mean and variance of the normal distribution. When \(\mu=0\) and \(\sigma^2 =1\), the normal distribution \(N(0,1)\) is called the \textbf{standard normal distribution}. The

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-64-1} \end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Two Types of Questions in Normal Distribution} are related to the left-tail area and quantile. R has two functions for finding \textbf{left-tail area} and \textbf{quantile} for any given normal distribution.
\end{itemize}

\begin{verbatim}
pnorm(quantile, mean, sd)          
# The above function finds the left-tail area for a given quantile. 
# mu = mean, sd = standard deviation.
qnorm(left.tail.prob, mean, sd)    
# The above function finds the quantile for a given left-tail area.
# mu = mean, sd = standard deviation.
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \textbf{Example 12}: We find the left-tail area of general normal distributions \(X \to N(16, 4^2)\). \(P(X < 13) = ?\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{normal.left.tail }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{pnorm}\NormalTok{(}\DecValTok{13}\NormalTok{, }\AttributeTok{mean =} \DecValTok{16}\NormalTok{, }\AttributeTok{sd =} \DecValTok{4}\NormalTok{),}\DecValTok{4}\NormalTok{)}
\NormalTok{normal.left.tail}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2266
\end{verbatim}

*\textbf{Example 13}: We find the left-tail area of the standard normal distributions \(X \to N(0, 1)\). \(P(X < 0.3) = ?\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sd.norm.left.tail }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{pnorm}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}\DecValTok{4}\NormalTok{)}
\NormalTok{sd.norm.left.tail}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6179
\end{verbatim}

We next give a graphical representation of the tail area in \textbf{Example 12} and \textbf{Example 13}.

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-67-1} \end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Example 14}: Let \(X \to N(16, 4^2)\). Find \(P(13 < X <18) = ?\)
  The following R code calculates the probability. Note that \texttt{P(X\textless{}18)\ =\ pnorm(18,\ 16,\ 4)} and \texttt{P(X\textless{}13)\ =\ pnorm(13,\ 16,\ 4)}. Therefore \texttt{P(13\ \textless{}\ X\ \textless{}\ 18)\ =\ pnorm(18,\ 16,\ 4)\ -\ pnorm(13,\ 16,\ 4)}. The probability is the area of the purple region. See the following Figure.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p}\FloatTok{.18} \OtherTok{=} \FunctionTok{pnorm}\NormalTok{(}\DecValTok{18}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{p}\FloatTok{.13} \OtherTok{=} \FunctionTok{pnorm}\NormalTok{(}\DecValTok{13}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{p.}\FloatTok{13.}\NormalTok{to}\FloatTok{.18} \OtherTok{=} \FunctionTok{round}\NormalTok{(p}\FloatTok{.18} \SpecialCharTok{{-}}\NormalTok{ p}\FloatTok{.13}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{p.}\FloatTok{13.}\NormalTok{to}\FloatTok{.18}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4648
\end{verbatim}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-70-1} \end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Example 15}: Consider the standard normal distribution \(N(0,1)\). Find \(z_0\) if \(P(Z > z_0) = 0.2345\). The answer is given in the following R code: \texttt{qnorm(0.2345,\ 0,\ 1)} or simply \texttt{qnorm(0.2345)}. The latter form does not specify \texttt{mean=} and \texttt{sd=} since the default \texttt{mean\ =\ 0} and \texttt{sd\ =\ 1}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.2345}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)     }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.7241071
\end{verbatim}

That is, \(23.45\%\) of the values in the standard normal population are less than or equal to \(-0.7241\) and \(23.45\%\) are greater than or equal to \(-0.7241\).

\hypertarget{t-distribution}{%
\subsection{t distribution}\label{t-distribution}}

The t distribution is symmetric with respect to the vertical axis with a mean of 0. The shape is \textbf{ALWAYS} flatter than the \textbf{standard normal distribution}. The shape of a t distribution is uniquely determined by the \textbf{degrees of freedom}. The following chart describes the relationship between the standard normal and t distributions.

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-72-1} \end{center}

R has two functions for finding left-tail area (also called tail probability) and quantile:

\begin{verbatim}
pt(quantile, df)         # left-tail probability (left-tail area)
qt(left-tail-area, df)   # quantile
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \textbf{Example 16}: Answer the following questions about t-distributions.

  \begin{itemize}
  \tightlist
  \item
    Consider \(t(10)\), t-distribution with 10 degrees of freedom. \(P(T<1.95) = ?\)
  \item
    Consider \(t(5)\), t-distribution with 10 degrees of freedom. What is \(t_0\) if \(P(T<t_0) = 0.575\)?
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# problem 1}
\NormalTok{problem01 }\OtherTok{=} \FunctionTok{pt}\NormalTok{(}\FloatTok{1.95}\NormalTok{, }\AttributeTok{df=}\DecValTok{10}\NormalTok{)}
\CommentTok{\# Problem 2}
\NormalTok{problem02 }\OtherTok{=} \FunctionTok{qt}\NormalTok{(}\FloatTok{0.575}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\DocumentationTok{\#\# display the two result}
\FunctionTok{cbind}\NormalTok{(}\AttributeTok{problem01 =}\NormalTok{ problem01, }\AttributeTok{problem02 =}\NormalTok{ problem02)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      problem01 problem02
## [1,] 0.9601258 0.1991374
\end{verbatim}

The above results are also reflected in the following figures.

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-74-1} \end{center}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

In this module, we introduced some basic concepts of probability, random variables, and distributions of random variables. Left-tail probability and quantile of normal and t distributions are the most important quantities that will be used in this course. We also introduced R functions to calculate left-tail the probability and the quantile of normal and t distributions. These R functions are summarized in the following.

\begin{verbatim}
# normal distribution
pnorm(quantile, mean, sd) # for the left-tail probability of normal distribution
qnorm(left.tail.prob, mean, sd) # for the quantile of normal distribution

# t-distribution
pt(quantile, df)        # for the left-tail probability of normal distribution
qt(left.tail.prob, df)  # for the quantile of t distribution
\end{verbatim}

\hypertarget{numerical-examples-based-on-normal-and-t-distributions}{%
\subsection{Numerical Examples Based on Normal and t Distributions}\label{numerical-examples-based-on-normal-and-t-distributions}}

\begin{itemize}
\item
  \textbf{Example 17}: In the United States, males between the ages of 40 and 49 eat on average 103.1 g of fat every day with a standard deviation of 4.32 g (``What we eat,'' 2012). Assume that the amount of fat a person eats is normally distributed.

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    State the random variable.
  \item
    Find the probability that a man in the age group of 40-49 in the U.S. eats more than 110 g of fat every day.
  \item
    Find the probability that a man in the age group of 40-49 in the U.S. eats less than 93 g of fat every day.
  \item
    Find the probability that a man in the age group of 40-49 in the U.S. eats less than 65 g of fat every day.
  \item
    If you found a man in the age group of 40-49 in the U.S. who says he eats less than 65 g of fat every day, would you believe him? Why or why not?
  \item
    What daily fat level do 5\% of all men in the age group of 40-49 in the U.S. eat more than?
  \end{enumerate}
\end{itemize}

\textbf{Solution}: We use R to find the answers to the above questions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(Y\) = amount of fat a person eats every day.
  Calculations of problems 2 - 6 are given in the following R code chunk.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p2 }\OtherTok{=} \DecValTok{1}\SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(}\DecValTok{110}\NormalTok{, }\AttributeTok{mean =}\FloatTok{103.1}\NormalTok{, }\AttributeTok{sd =} \FloatTok{4.32}\NormalTok{)  }\CommentTok{\# 1{-}left{-}tail{-}prob = right{-}tail{-}prob}
\NormalTok{p3 }\OtherTok{=} \FunctionTok{pnorm}\NormalTok{(}\DecValTok{93}\NormalTok{, }\AttributeTok{mean =}\FloatTok{103.1}\NormalTok{, }\AttributeTok{sd =} \FloatTok{4.32}\NormalTok{)      }\CommentTok{\# left{-}tail probability}
\NormalTok{p4 }\OtherTok{=} \FunctionTok{pnorm}\NormalTok{(}\DecValTok{65}\NormalTok{, }\AttributeTok{mean =}\FloatTok{103.1}\NormalTok{, }\AttributeTok{sd =} \FloatTok{4.32}\NormalTok{)      }\CommentTok{\# left{-}tail probability}
\NormalTok{p5 }\OtherTok{=} \FunctionTok{pnorm}\NormalTok{(}\DecValTok{65}\NormalTok{, }\AttributeTok{mean =}\FloatTok{103.1}\NormalTok{, }\AttributeTok{sd =} \FloatTok{4.32}\NormalTok{)      }\CommentTok{\# left{-}tail probability}
\NormalTok{p6 }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1}\FloatTok{{-}0.05}\NormalTok{, }\AttributeTok{mean =}\FloatTok{103.1}\NormalTok{, }\AttributeTok{sd =} \FloatTok{4.32}\NormalTok{)  }\CommentTok{\# quantile}
\NormalTok{ans }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{problem2 =} \FunctionTok{round}\NormalTok{(p2,}\DecValTok{4}\NormalTok{), }\AttributeTok{problem3 =}  \FunctionTok{round}\NormalTok{(p3,}\DecValTok{4}\NormalTok{), }
            \AttributeTok{problem4 =} \FunctionTok{round}\NormalTok{(p4,}\DecValTok{4}\NormalTok{), }\AttributeTok{problem5 =}  \FunctionTok{round}\NormalTok{(p5,}\DecValTok{4}\NormalTok{), }
            \AttributeTok{problem6=}\NormalTok{ p6)}
\FunctionTok{row.names}\NormalTok{(ans) }\OtherTok{=}\StringTok{"prob or quantile"}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{t}\NormalTok{(ans))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r}
\hline
  & prob or quantile\\
\hline
problem2 & 0.0551\\
\hline
problem3 & 0.0097\\
\hline
problem4 & 0.0000\\
\hline
problem5 & 0.0000\\
\hline
problem6 & 110.2058\\
\hline
\end{tabular}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The probability that a man in the age group of 40-49 in the U.S. eats more than 110 g of fat every day is 5.51\%.
\item
  The probability that a man in the age group of 40-49 in the U.S. eats less than 93 g of fat every day is 0.997\%.
\item
  The probability that a man in the age group of 40-49 in the U.S. eats less than 65 g of fat every day is close to 0.00\%.
\item
  I will not believe a man in the age group of 40-49 in the U.S. who says he eats less than 65 g of fat every day because the chance of eating less than 65 g in that age group is almost 0.00\%.
\item
  5\% of all men in the age group of 40-49 in the U.S. eat more than 110.2058 g.
\end{enumerate}

\begin{itemize}
\item
  \textbf{Example 18}: The mean cholesterol levels of women aged 45-59 in Ghana, Nigeria, and Seychelles is 5.1 mmol/l and the standard deviation is 1.0 mmol/l (Lawes, Hoorn, Law \& Rodgers, 2004). Assume that cholesterol levels are normally distributed.

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    State the random variable.
  \item
    Find the probability that a woman aged 45-59 in Ghana, Nigeria, or Seychelles has a cholesterol level above 6.2 mmol/l (considered a high level).
  \item
    Find the probability that a woman aged 45-59 in Ghana, Nigeria, or Seychelles has a cholesterol level below 5.2 mmol/l (considered a normal level).
  \item
    Find the probability that a woman aged 45-59 in Ghana, Nigeria, or Seychelles has a cholesterol level between 5.2 and 6.2 mmol/l (considered borderline high).
  \item
    If you found a woman aged 45-59 in Ghana, Nigeria, or Seychelles having a cholesterol level above 6.2 mmol/l, what could you conclude?
  \item
    What value do 5\% of all women ages 45-59 in Ghana, Nigeria, or Seychelles have a cholesterol level less than?
  \end{enumerate}
\end{itemize}

\textbf{Solution}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Random variable \(Y\) = mean cholesterol levels of women aged 45-59 in Ghana, Nigeria, and Seychelles.
\end{enumerate}

The calculation of problems 2 - 6 is given in the following R code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p2}\FloatTok{.2} \OtherTok{=} \DecValTok{1}\SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(}\FloatTok{6.2}\NormalTok{, }\AttributeTok{mean =} \FloatTok{5.1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)      }\CommentTok{\# 1{-}left{-}tail{-}prob = right{-}tail{-}prob}
\NormalTok{p2}\FloatTok{.3} \OtherTok{=} \FunctionTok{pnorm}\NormalTok{(}\FloatTok{5.2}\NormalTok{, }\AttributeTok{mean =} \FloatTok{5.1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)         }\CommentTok{\# left{-}tail probability}
\NormalTok{p2}\FloatTok{.4} \OtherTok{=} \FunctionTok{pnorm}\NormalTok{(}\FloatTok{6.2}\NormalTok{, }\AttributeTok{mean =} \FloatTok{5.1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{{-}}  \FunctionTok{pnorm}\NormalTok{(}\FloatTok{5.2}\NormalTok{, }\AttributeTok{mean =} \FloatTok{5.1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)    }
\NormalTok{p2}\FloatTok{.5} \OtherTok{=} \DecValTok{1} \SpecialCharTok{{-}}\FunctionTok{pnorm}\NormalTok{(}\FloatTok{6.2}\NormalTok{, }\AttributeTok{mean =} \FloatTok{5.1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)      }\CommentTok{\# left{-}tail probability}
\NormalTok{p2}\FloatTok{.6} \OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\AttributeTok{mean =} \FloatTok{5.1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)      }\CommentTok{\# quantile}
\NormalTok{ans.p2 }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{problem2 =} \FunctionTok{round}\NormalTok{(p2}\FloatTok{.2}\NormalTok{,}\DecValTok{4}\NormalTok{), }\AttributeTok{problem3 =}  \FunctionTok{round}\NormalTok{(p2}\FloatTok{.3}\NormalTok{,}\DecValTok{4}\NormalTok{), }
               \AttributeTok{problem4 =} \FunctionTok{round}\NormalTok{(p2}\FloatTok{.4}\NormalTok{,}\DecValTok{4}\NormalTok{), }\AttributeTok{problem5 =}  \FunctionTok{round}\NormalTok{(p2}\FloatTok{.5}\NormalTok{,}\DecValTok{4}\NormalTok{), }
               \AttributeTok{problem6 =}\NormalTok{ p2}\FloatTok{.6}\NormalTok{)}
\FunctionTok{row.names}\NormalTok{(ans.p2) }\OtherTok{=}\StringTok{"prob or quantile"}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{t}\NormalTok{(ans.p2))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r}
\hline
  & prob or quantile\\
\hline
problem2 & 0.135700\\
\hline
problem3 & 0.539800\\
\hline
problem4 & 0.324500\\
\hline
problem5 & 0.135700\\
\hline
problem6 & 3.455146\\
\hline
\end{tabular}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The probability that a woman aged 45-59 in Ghana, Nigeria, or Seychelles has a cholesterol level above 6.2 mmol/l is 13.57\%.
\item
  Find the probability that a woman aged 45-59 in Ghana, Nigeria, or Seychelles has a cholesterol level below 5.2 mmol/l is 53.98\%.
\item
  Find the probability that a woman aged 45-59 in Ghana, Nigeria, or Seychelles has a cholesterol level between 5.2 and 6.2 mmol/l 32.45\%.
\item
  If a woman aged 45-59 in Ghana, Nigeria, or Seychelles has a cholesterol level above 6.2 mmol/l, he has a high cholesterol level.
\item
  5\% of all women ages 45-59 in Ghana, Nigeria, or Seychelles have a cholesterol level of less than 3.455 mmol/l.
\end{enumerate}

\hypertarget{assignment---probability-distributions}{%
\section{Assignment - Probability Distributions}\label{assignment---probability-distributions}}

We focus on the concepts of random variables and their corresponding distributions. Two basic types of questions are will face very often in this course are to find (1) left-tail probability (area under the density curve) and (2) quantile under given conditions for all continuous distributions. We present numerical examples of normal and t distributions in the last part of the class note. In this assignment, you will do problems similar to the last two examples. You can use the codes provided in the class note {[}HTML or PDF{]}. I also include RMD on the course web page but not required. In case some of you want to learn more advanced R coding, the source RMD is useful.

\textbf{kable()} function is in library \textbf{\{knitr\}}. If you have not installed this package on your computer, you will receive an error \texttt{could\ not\ find\ function\ "kable"\ calls:}. The following screenshot shows how to install a package on your computer.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img04/w04-install-pack} 

}

\caption{Install Packages on computer.}\label{fig:unnamed-chunk-77}
\end{figure}

This assignment focuses on the two types of questions using normal distributions. Please prepare an R Markdown document to complete the assignment.

\begin{itemize}
\tightlist
\item
  \textbf{Problem 1}
\end{itemize}

The size of fish is very important to commercial fishing. A study conducted in 2012 found the length of Atlantic cod caught in nets in Karlskrona to have a mean of 49.9 cm and a standard deviation of 3.74 cm (Ovegard, Berndt \& Lunneryd, 2012). Assume the length of fish is normally distributed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the probability that an Atlantic cod has a length of less than 52 cm.
\item
  Find the probability that an Atlantic cod has a length of more than 74 cm.
\item
  Find the probability that an Atlantic cod has a length between 40.5 and 57.5 cm.
\item
  If you found an Atlantic cod to have a length of more than 74 cm, what could you conclude?
\item
  What length are 15\% of all Atlantic cod longer than?
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Problem 2}
\end{itemize}

The mean yearly rainfall in Sydney, Australia, is about 137 mm and the standard deviation is about 69 mm (``Annual maximums of,'' 2013). Assume rainfall is normally distributed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the probability that the yearly rainfall is less than 100 mm.
\item
  Find the probability that the yearly rainfall is more than 240 mm.
\item
  Find the probability that the yearly rainfall is between 140 and 250 mm.
\item
  If a year has a rainfall of less than 100mm, does that mean it is an unusually dry year? Why or why not?
\item
  What rainfall amounts are 90\% of all yearly rainfalls more than?
\end{enumerate}

\hypertarget{sampling-distributions}{%
\chapter{Sampling Distributions}\label{sampling-distributions}}

In this section, we study the random behavior of quantities obtained from random samples using the probability distributions introduced in the previous weeks.

Some technical terms:

\begin{itemize}
\item
  \textbf{Population} - set of all subjects of interest. For example, if we want the average height of WCU students, then the set of all WCU students is the \textbf{population}.
\item
  \textbf{Sample} - a subset of subjects of the population. For example, all students in the Department of Biology form a subset of all WCU students. In other words, the set of all students in Biology is a \textbf{sample}. \textbf{However}, this \textbf{sample} does not represent the \textbf{population} since WCU has many other majors.
\item
  \textbf{Random Sample} - a subset of subjects that represent the population. For example, we can use one of the methods introduced in week \#2 to collect a subset from the WCU student population - to obtain a random sample.
\item
  \textbf{Parameter} - numerical characteristic of the population. For example, the average height of the WCU student population is denoted by \(\mu\). Apparently, \(\mu\) is unknown but fixed.
\item
  \textbf{Statistic} - numerical characteristic of the population calculated from the random sample. For example, we select a random sample of 150 students from WCU and find the average height, denoted by \(\bar{X}\). Apparently, \(\bar{X}\) is random since its value is dependent on the random sample.
\end{itemize}

Since \(\bar{X}\) is a random variable, we use probability to characterize the random behavior of \(\bar{X}\).

\hypertarget{concepts-of-the-sampling-distribution}{%
\section{Concepts of the sampling distribution}\label{concepts-of-the-sampling-distribution}}

We see from the examples in the previous section that the sample mean is a random variable. In fact, all population parameters evaluated at a random sample taken from the population are random variables. To characterize the behavior, we need to use probability distributions.

\begin{itemize}
\tightlist
\item
  \textbf{A sampling distribution} is the distribution of a \textbf{sample statistic} such as sample mean, sample variance, sampling coefficient of variation, sample correlation coefficient, etc.
\end{itemize}

A population has many different numerical characteristics that require different probability distributions to characterize them. In this course, we focus on the mean and proportion and some coefficients of regression that affect the mean and proportion. In the next few modules, we introduce procedures for constructing confidence intervals and testing hypothesis inferences for population means and proportions.

In the next sections, we introduce sampling distributions of sample means and proportions under different assumptions.

\hypertarget{sampling-distribution-of-sampling-means}{%
\section{Sampling Distribution of Sampling Means}\label{sampling-distribution-of-sampling-means}}

Inferential statistics is all about making inferences about population parameters by using the information of individual subjects. The estimated population parameters could be used to predict the individual level.

The information in the estimate is dependent on the amount of information in the sample and the population as well. In the next few sections, we introduce the sampling distribution of sample means under different assumptions.

\hypertarget{working-data-set-plant-diversity}{%
\subsection{Working Data Set: Plant Diversity}\label{working-data-set-plant-diversity}}

This data set includes the geographic location (lat/lon) for 15,136 plots, as well as the herbaceous species richness, climate, soil pH, and other variables related to the plots.

This data set is associated with the following publications: Simkin, S., C. Clark , W. Bowman, E. Allen, J. Belnap, and L. Pardo. The conditional vulnerability of plant diversity to atmospheric nitrogen deposition across the United States. PNAS (PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES). National Academy of Sciences, WASHINGTON, DC, USA, 113(15): 4086-4091, (2016).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\StringTok{"img05/w05{-}site{-}map.jpg"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{img05/w05-site-map} \end{center}

We choose a special subset that contains data associated with \textbf{Perennial graminoid vegetation}. That has 1152 records. The distributions of 6 numerical variables are given in the following histograms.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plant}\OtherTok{=}\StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/Data/w05{-}plant{-}diversity.csv"}
\NormalTok{plant.diversity }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(plant, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-81-1} \end{center}

We can see from the above histograms that the \textbf{Critical Load of nitrogen deposition} is close to the normal distribution. Other variables are \textbf{not} normally distributed. In the rest of this note, I will assume the above data sets to be ``populations'' and take random samples from appropriate populations listed above in different examples. Some of the following examples will be based on the above populations. You may want to use the distributional information (the shapes of histograms) to choose appropriate sampling distributions of sample statistics based on the sample taken from one of the above populations.

\hypertarget{normal-population-with-given-mean-mu-and-variance-sigma2}{%
\subsection{\texorpdfstring{Normal Population with given mean (\(\mu\)) and Variance (\(\sigma^2\))}{Normal Population with given mean (\textbackslash mu) and Variance (\textbackslash sigma\^{}2)}}\label{normal-population-with-given-mean-mu-and-variance-sigma2}}

\textbf{Result \#1:} Assume that \(X\) is a normal random variable with an unknown mean \(\mu\) and the known variance \(\sigma^2\). That is,

\[
X \to N(\mu, \sigma^2)
\]

Let random sample \(\{x_1, x_2, \cdots, x_n \} \to N(\mu, \sigma_0^2)\), where population mean \(\mu\) is unknown and variance \(\sigma_0^2\). Then the sample mean

\[
\bar{X} = \frac{\sum_{i=1}^n}{n}
\]
is normally distributed with mean \(\mu\) and variance \(\sigma^2/n\). In other words,

\[
\bar{X} \to N(\mu, \sigma_0^2/n)
\]

\textbf{Remarks}: The simple comparison of \(X\) and \(\bar{X}\).

\begin{itemize}
\item
  The means of the distributions of \(X\) and \(\bar{X}\) are equal.
\item
  The variances of the distributions of \(X\) and \(\bar{X}\) are \textbf{not} equal. In fact, the variance of \(\bar{X}\) is less than the variance of \(X\).
\item
  As the sample size increases, the variance of \(\bar{X}\) decreases since the denominator of the variance of \(\bar{X}\) contains the sample size \(n\).
\item
  Since the R functions \textbf{pnorm()} and \textbf{qnorm()} require the standard deviation as an argument.
\end{itemize}

The following figure shows the variance of the sample means with different sample sizes.

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-82-1} \end{center}

The above figure shows that, as sample size increases, the variance of \(\bar{X}\) decreases (the density curve becomes skinnier as sample size increases).

\textbf{Example 1}. We use \textbf{Critical Load (CL) of nitrogen deposition} in the \textbf{plant diversity} data set. Assume that \textbf{Critical Load (CL) of nitrogen deposition} follows a normal distribution with a mean of \(\mu=10\) and known variance \(\sigma^2 = 0.2\). Use this information to answer the following questions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  If a random of 20 Critical Load (CL) of nitrogen deposition sample values are taken from the population, what is the probability that the mean critical load nitrogen deposition (\(\bar{X}\)) is greater than 10.1 kg N/ha/yr?
\item
  What is the level of critical load (CL) of nitrogen deposition that is higher than the 95\% mean level of critical load (CL) of nitrogen deposition with the same sample size 20?
\end{enumerate}

\textbf{Solution}: Since the critical load (CL) of nitrogen deposition is a normal population, therefore, the sampling distribution of \(\bar{X} \to N(10, .2/20)\). The solutions to problems are based on this sampling distribution.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(P(\bar{X} > 10.05)\) is equal to the right tail area of \(N(10, (\sqrt{0.2/20})^2\). Note that, by default, the R function \textbf{pnorm()} only gives the left-tail area. The desired probability is equal to \(1 - left.tail.area\). The R code is given in the following.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{pnorm}\NormalTok{(}\FloatTok{10.1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{10}\NormalTok{, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(}\FloatTok{0.2}\SpecialCharTok{/}\DecValTok{20}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1586553
\end{verbatim}

Therefore, \(P(\bar{X} > 11.5) = 0.1587\). The tail probability is labeled in the following figure.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  the desired cut-off if actually the \(95\%\) quantile of the distribution of \(\bar{X}\) which can be found by \textbf{qnorm()}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.95}\NormalTok{, }\AttributeTok{mean =} \DecValTok{10}\NormalTok{, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(}\FloatTok{0.2}\SpecialCharTok{/}\DecValTok{20}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10.16449
\end{verbatim}

Therefore the \(95\%\) quantile is 10.16449.

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-85-1} \end{center}

\hypertarget{normal-population-with-a-given-mean-mu-and-an-unknown-variance-sigma2}{%
\subsection{\texorpdfstring{Normal Population with a given mean (\(\mu\)) and an \textbf{Unknown} Variance (\(\sigma^2\))}{Normal Population with a given mean (\textbackslash mu) and an Unknown Variance (\textbackslash sigma\^{}2)}}\label{normal-population-with-a-given-mean-mu-and-an-unknown-variance-sigma2}}

If the population variance is unknown, then we have to use sample variance to characterize the distribution \(\bar{X}\). Unlike in the case of the normal population with known variance in which we can specify the sampling distribution \(\bar{X}\) directly, we cannot but have the following result based on the standardized statistic.

\textbf{Result \#2:} Let random sample \(\{X_1, X_2, \cdots, X_n \} \to N(\mu, \sigma^2)\) and \(\bar{X}\) and \(s^2\) be the sample mean and sample variance, respectively. then we have

\[
\frac{\bar{X}-\mu}{s/\sqrt{n}} \to t_{n-1}
\]
Where \(t_{n-1}\) is a t-distribution with \(n-1\) degrees of freedom. The two basic types of questions associated with \(t-\)distribution were discussed in the previous module.

\textbf{Example 2}. Assume that the sample of 11 levels of critical load (CL) of nitrogen deposition \(\{ 8.67, 9.38, 9.27, 8.56, 8.72, 8.83, 9.72, 8.36, 8.76, 8.91, 9.49 \}\) is randomly selected from all sites in the study. Its \textbf{sample standard deviation} is \(0.430\). What is the percent of the sample means based on the same sample size 11 will be bigger than 10.2? Note that the population mean is \(\mu = 10\).

\textbf{Solution} The question to answer is about the sample mean \(\bar{X}\) with size \(n = 11\). Since the population variance is unknown, \(\bar{X}\) is not a normal distribution.

\[
T = \frac{\bar{X} - \mu}{s/\sqrt{n}} \to t_{n-1}
\]
Therefore,

\[
P(\bar{X} > 10.2) = P\left( \frac{\bar{X}-\mu}{s/\sqrt{n}}  > \frac{10.2-10}{0.43/\sqrt{11}} \right) = P(T>1.543)
\]

As we know, \(P(T > 1.543)\) is the right-tail area of \(t_{11-1} = t_{10}\). The desired probability is \(1-\) left-tailed area. The following R code finds the above probability.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\SpecialCharTok{{-}} \FunctionTok{pt}\NormalTok{(}\FloatTok{1.543}\NormalTok{, }\AttributeTok{df =} \DecValTok{11{-}1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.07693013
\end{verbatim}

Therefore, \(P(T > 1.543) = 0.0769\).

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-88-1} \end{center}

\hypertarget{unspecified-population-with-a-given-mean-muand-an-unspecifed-variance-sigma2}{%
\subsection{\texorpdfstring{Unspecified Population with a given mean (\(\mu\))and an unspecifed variance (\(\sigma^2\))}{Unspecified Population with a given mean (\textbackslash mu)and an unspecifed variance (\textbackslash sigma\^{}2)}}\label{unspecified-population-with-a-given-mean-muand-an-unspecifed-variance-sigma2}}

In the previous two cases, we assume the population to be normal. Quite frequently we need to deal with a population without a specified distribution of real-world applications. This means that the population distribution is unknown. As usual, we assume the population mean is known. The population variance is either given or unknown. If we still want to use the sampling distribution of sample means to make inferences about the population mean, what result(s) we can use to characterize the sampling distribution of the sample mean?

In this section, we discuss this type of sampling distribution of sample means under certain conditions. Before we present the result, we perform a simulation to show the patterns of the sampling distributions of sample means using various sample sizes. We simulate a non-normal distribution with two peaks and then take samples from that population with different sample sizes, and use these sample means to estimate the corresponding density curves.

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-89-1} \end{center}

The figure shows that, as the sample size increases, the sampling distribution of the sampling mean approaches a normal distribution regardless of the distribution of the original population. The following theorem explicitly specifies the sampling distribution.

\textbf{Results \#3}: \textbf{Central Limit Theorem:} Let \(X \to (\mu, \sigma^2)\) (\textbf{caution}: the population is not necessarily to be normal.). Let \(\bar{X}\) be the sample mean with size \(n\). If \(n\) is large, the sampling \(\bar{X}\) is \textbf{approximately} normally distributed with a mean \(\mu_{\bar{X}} = \mu\) and variance \(\sigma_{\bar{X}}^2 = \sigma^2/n\). That is,

\[
\bar{X} \to N\left( \mu, \frac{\sigma^2}{n} \right).
\]

\textbf{Comments}: The following are comments related to the normal approximations.

\begin{itemize}
\item
  The baseline population is not specified in the theorem. The distribution could be discrete or continuous.
\item
  When the sample size is large, the sample mean is approximately normally distributed. The question is how large is called ``large''? As a convention, we call the sample size large if \(n > 30\).
\end{itemize}

\textbf{Example 3.} The blood cholesterol levels of a population of workers have a mean of 202 and a standard deviation of 14. If a sample of 36 workers is selected, approximate the probability that the sample mean of their blood cholesterol levels will lie between 198 and 206.

\textbf{Solution} Let \(X\) be the blood cholesterol levels of a population of workers. Then \(X \to (\mu = 202, \sigma = 14)\). Since \(n=36 > 30\), using the C.L.T, we have \(\bar{X} \to N(202, 14/\sqrt{36})\). Therefore,

\[
P(198 < \bar{X} < 206) = P(\bar{X} < 206) - P(\bar{X} < 198) =0.9135237 
\]

\begin{Shaded}
\begin{Highlighting}[]
 \FunctionTok{pnorm}\NormalTok{(}\DecValTok{206}\NormalTok{, }\AttributeTok{mean =} \DecValTok{202}\NormalTok{, }\AttributeTok{sd =} \DecValTok{14}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{36}\NormalTok{)) }\SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(}\DecValTok{198}\NormalTok{, }\AttributeTok{mean =} \DecValTok{202}\NormalTok{, }\AttributeTok{sd =} \DecValTok{14}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{36}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9135237
\end{verbatim}

The above probability is the area of the shaded area in the following figure.

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-91-1} \end{center}

\hypertarget{sampling-distribution-of-sample-proportions}{%
\section{Sampling distribution of sample proportions}\label{sampling-distribution-of-sample-proportions}}

For a population of binary data that only takes on exactly two possible values such as ``success'' vs ``failure'', ``diseased'' vs ``disease-free'', etc., its distribution is uniquely determined by the proportion of one of the categories.

Let \(X=\{x_1, x_2, \cdots, x_n\}\) be a random sample taken from a population of binary data taking on only two possible values ``success'' or ``failure''. That is, \(x_i\) = ``success'' or ``failure'', for \(i=1, 2, \cdots, n.\) If we are interested in the proportion of ``success'' of the population, we can do the following numerical coding on the sample:
1 = ``success'' and 0 = ``failure''. With this numerical coding, we calculate the sample mean

\[
\hat{p} = \frac{\sum_{i=1}^n x_i}{n} = \frac{\# 1s}{n} = \frac{\# succeses}{n} = proportion.of.successes.
\]

Therefore, the sample proportion is actually a \textbf{sample mean}. Therefore, according to the central limit theorem, \(\hat{p}\) is approximately normally distributed under certain conditions. This idea is formalized in the following result.

\textbf{Result \#4}. Let \(p\) be the proportion of one of the categories, say ``success'' and \(\hat{p}\) the sample proportion based on a random sample with size \(n\). If \(np > 5\) and \(n(1-p) > 5\), then

\[
\hat{p} \to N\left( p, \sqrt{\frac{p(1-p)}{n}}\right).
\]

In some applications, the standard deviation \(\sqrt{p(1-p)/n}\) is estimated by \(\sqrt{\hat{p}(1-\hat{p})/n}\).

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-92-1} \end{center}

The above simulated sampling distributions with samples taken from a binary population with true \(p = 10\%\) use different sample sizes. We see that as the sample size increases, the sampling distribution approaches the normal distribution.

\textbf{Example 4} The frequency of color blindness (dyschromatopsia) in the Caucasian American male population is estimated to be about 8\%. We take a random sample of size 125 from this population. What is the probability that more than 9\% of the 125 subjects have color blindness?

\textbf{Solution} Since \(np = 125\times 0.08 = 10 > 5\), \(n(1-0.08) = 115> 5\), the C.L.T can be used to approximate the sampling distribution of the sample proportion to the normal distribution as outlined in the \textbf{result \#3}. That is,

\[
\hat{p} \to N\left(0.08, \sqrt{\frac{0.08(1-0.08)}{125}} \right) =N\left(0.08, 0.0243 \right).
\]

The probability we want to find is \(P(\hat{p} > 0.09) = 1 -P(\hat{p} < 0.09) = 0.34\)

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(}\FloatTok{0.09}\NormalTok{, }\AttributeTok{mean =} \FloatTok{0.08}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.0243}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3403447
\end{verbatim}

\textbf{Example 5} Suppose 60\% of seniors who get flu shots remain healthy, and independent from one person to the next. If we selected a random sample from the complex of 100, what is the probability that the sample proportion will be greater than 50\%?

\textbf{Solution}: We are given that \(p=0.6\). Since \(np=200\times 0.6 = 120 > 5\) and \(n(1-p) = 200 \times 0.4 = 80 > 5\). Using \textbf{result \#3}, we have

\[
\hat{p} \to N\left(0.6, \sqrt{\frac{0.6(1-0.6)}{200}} \right) = N(0.6, 0.0346).
\]

Therefore, \(P(\hat{p} > 0.5) = 1 - P(\hat{p} < 0.5) = 0.2859\)

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{pnorm}\NormalTok{(}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.0346}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2859009
\end{verbatim}

\hypertarget{summary-1}{%
\section{Summary}\label{summary-1}}

In this module, we introduced the sampling distribution of sample means and proportions under various assumptions. Let random sample \(\{x_1, x_2, \cdots, x_n \}\) be taken from a population with sample mean

\[
\bar{X} = \frac{\sum_{i=1}^n x_i}{n}.
\]

The sampling distribution of \(\bar{X} (\hat{p})\) under various conditions is summarized in the following.

\begin{itemize}
\item
  \textbf{Sampling distribution of sampling means \(\bar{X}\)}

  \begin{itemize}
  \tightlist
  \item
    Population is normal with known variance (\(\sigma_0^2\)).
  \end{itemize}

  \[
  \bar{X} \to N\left( \mu, \frac{\sigma_0^2}{n} \right) \Rightarrow \frac{\bar{X}-\mu}{\sigma_0/\sqrt{n}} \to N(0,1).
  \]

  \begin{itemize}
  \tightlist
  \item
    Population is normal with unknown variance (\(\sigma^2\)).
  \end{itemize}

  \[
   \bar{X} \to N\left( \mu, \frac{\sigma^2}{n} \right) \Rightarrow \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \to N(0,1).
  \]
  However, if the sample variance \(s^2\) is used,

  \[
  \frac{\bar{X}-\mu}{s/\sqrt{n}} \to t_{n-1}.
  \]

  \begin{itemize}
  \tightlist
  \item
    Population is unspecified: \textbf{if the sample size is large},
  \end{itemize}

  \[
   \bar{X} \to N\left( \mu, \frac{\sigma^2}{n} \right) \Rightarrow \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \to N(0,1) \hspace{3mm} \mbox{and} \hspace{3mm} \frac{\bar{X}-\mu}{s/\sqrt{n}} \to N(0,1).
  \]
\end{itemize}

where \(s\) is the sample standard deviation.

\begin{itemize}
\tightlist
\item
  \textbf{Sampling distribution of proportions (\(\hat{p}\))}: if \(np > 5\) and \(n(1-p) > 5\), then
\end{itemize}

\[
\hat{p} \to N\left( p, \frac{p(1-p)}{n} \right) \Rightarrow \frac{\hat{p}-p}{\sqrt{p(1-p)/n}} \to N(0, 1).
\]

Note also that

\[
\frac{\hat{p}-p}{\sqrt{\hat{p}(1-\hat{p})/n}} \to N(0, 1).
\]

\hypertarget{assignment---sampling-distributions}{%
\section{Assignment - Sampling Distributions}\label{assignment---sampling-distributions}}

This assignment focuses on the applications of sampling distributions. Please check the assumptions of each of the four results and compare them with the information given in each of the problems in the following to determine the correct sampling distribution.

\begin{itemize}
\tightlist
\item
  \textbf{Problem 1}
\end{itemize}

Suppose it is known that in a certain large human population, cranial length is approximately
\textbf{normally distributed} with a mean of 185.6mm and a standard deviation of 12.7 mm.

(1). Find the mean and standard deviation of the sampling distribution respectively.

(2). What is the probability that a random sample of size 10 from this population will have a
mean greater than 190?

\begin{itemize}
\tightlist
\item
  \textbf{Problem 2}
\end{itemize}

The National Health and Nutrition Examination Survey of 1988--1994 (NHANES III) estimated the mean serum cholesterol level for U.S. females aged 20--74 years to be 204 mg/dl. The estimate of the standard deviation was approximately 44. Using these estimates as the mean \(\mu\) and standard deviation \(\sigma\) for the U.S. population, consider the sampling distribution of the sample mean based on samples of size 49 drawn from women in this age group.

(1). Find the mean and the standard deviation of the sampling distribution respectively

(2). Find the probability that the sample mean serum cholesterol level will be between 170 and 195.

(3). Find the probability that the sample mean serum cholesterol level will be less than 210.

(4). Find the probability that the sample mean serum cholesterol level will be bigger than 195.

\begin{itemize}
\tightlist
\item
  \textbf{Problem 3}
\end{itemize}

Smith et al.~performed a retrospective analysis of data on 782 eligible patients admitted with
myocardial infarction to a 46-bed cardiac service facility. Of these patients, 248 (32 percent) reported a past myocardial infarction. Use .32 as the population proportion. Suppose 50 subjects are chosen at random from the population.

(1). Find the mean and the standard deviation of the sampling distribution respectively.

(2). What is the probability that over 40 percent would report previous myocardial infarctions?

\hypertarget{confidence-intervals}{%
\chapter{Confidence Intervals}\label{confidence-intervals}}

\textbf{Objectives}: We want to estimate the population parameters such as mean, standard deviation, and proportion from a random sample and build a confidence interval (also called interval estimate) to show how good the estimate is, finally, we should be able to interpret the estimate.

\hypertarget{some-terms}{%
\section{Some Terms}\label{some-terms}}

\begin{itemize}
\item
  The sample is random. That is, the values in the sample are representative of the whole population.
\item
  \textbf{An estimate} is a specific value or range of values obtained from a random sample that is used to approximate a population parameter.
\item
  \textbf{A point estimate} is a single value (obtained from the random sample) that is used to approximate a population parameter.

  \textbf{Example 1}: The sample mean \(\bar{X}\) is the best point estimate of the population mean \(\mu\).

  \textbf{Example 2}: The sample variance \(s^2\) is the best point estimate of the population variance \(\sigma^2\).
\item
  \textbf{The bias of the point estimate} is equal to the difference between the estimate and the true parameter. For example, the bias of the sample mean is \(\bar{x} -\mu\). The bias of an estimate measures the accuracy of the estimate.
\end{itemize}

Since the point estimate of a parameter is obtained from an underlying random sample, it is a random variable. The variance of the point estimate measures the precision of the point estimate of the corresponding population parameter.

\textbf{The goodness of an estimate} -- no bias (accurate) and small variance (precise).

\hypertarget{what-are-the-issues-of-a-point-estimate}{%
\subsection{What are the issues of a point estimate?}\label{what-are-the-issues-of-a-point-estimate}}

With a point estimate, we can only say that it is close to the true population parameter. The question is how close is called ``close''?

\textbf{Example 1}: The distribution of heights of WCU students is approximately normal with mean \(\mu\) inches and standard deviation \(\sigma\) inches. To estimate \(\mu\), we select a random sample \(\{x_1, \cdots ,x_n\}\)

Then
\[
\bar{x} = \frac{x_1 + x_2 + \cdots + x_n}{n}
\]

is close to the unknown true average height of WCU students. For example, assuming the true mean is 69 inches, a sample of 50 heights yields a sample average of 68.5 inches.

\textbf{Is 68.5 close to 69?}

Apparently, we {cannot } answer the above question with a{ single sample mean } without any additional information.

\hypertarget{how-about-multiple-samples}{%
\subsection{How About Multiple Samples?}\label{how-about-multiple-samples}}

If I invite 100 students to help collect 100 random samples of the same size 50 from the WCU student population. Then we will have the following 100 sample means, say

\begin{verbatim}
68.2 68.3 68.3 68.4 68.4 68.5 68.6 68.6 68.6 68.6 68.6 68.7 68.7 68.7 68.7 68.7 
68.7 68.7 68.8 68.8 68.8 68.8 68.8 68.8 68.8 68.8 68.8 68.8 68.8 68.8 68.8 68.8 
68.9 68.9 68.9 68.9 68.9 68.9 68.9 68.9 68.9 68.9 68.9 68.9 69.0 69.0 69.0 69.0 
69.0 69.0 69.0 69.0 69.0 69.0 69.0 69.1 69.1 69.1 69.1 69.1 69.1 69.1 69.1 69.1 
69.1 69.1 69.1 69.2 69.2 69.2 69.2 69.2 69.2 69.2 69.2 69.2 69.2 69.3 69.3 69.3 
69.3 69.3 69.3 69.4 69.4 69.4 69.5 69.5 69.5 69.5 69.5 69.5 69.6 69.6 69.6 69.6 
69.7 69.7 69.7 69.9
\end{verbatim}

Each of the above means is supposed to be close to 69. Can we now answer the question about how close is called ``close''? Let's make a histogram of all sample means in the following.

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-96-1} \end{center}

We can see from the above histogram that most of the means are around (``close'' to) 69. In fact, 95\% of the sample means are within the interval {[}68.3, 69.7{]}. If we consider sample means within the interval to be { ``close'' }to the true mean, the above interval {[}68.3, 69.7{]} reveals the following information.

\begin{itemize}
\item
  The interval has a 95\% chance to include the true mean - accuracy and confidence;
\item
  The width of the confidence interval reflects the precision.
\end{itemize}

Therefore, this interval contains all desired information, but { What is more important is that the interval was obtained from the distribution of sample means.} In other words,{ only the sampling distribution of sample mean can provide such intervals that provide both accuracy and precision of the interval for a given confidence level! }

\hypertarget{how-to-find-the-confidence-intervals-for-means-and-proportions}{%
\section{How to find the Confidence Intervals for Means and Proportions?}\label{how-to-find-the-confidence-intervals-for-means-and-proportions}}

The above section explained the concept of the confidence interval of the population mean intuitively with an example. This section provides a slightly formal approach to confidence intervals for population means and proportions.

\hypertarget{framework-of-confidence-interval}{%
\subsection{Framework Of Confidence Interval}\label{framework-of-confidence-interval}}

The ways of constructing confidence intervals for different parameters may be slightly different from a procedural perspective, but they follow the same framework of using the distribution of a random variable that contains the parameter of interest and its point estimate.

\textbf{Definition}: a {pivotal quantity} is a random quantity of both parameters and sample statistics and its distribution is independent of the parameters.

The following are a few examples related to the distribution we learned in the previous weeks.

\textbf{Example 1}. Recall in \textbf{result \#1} of the previous note, that if a random sample is taken from a normal population with a known variance \(\sigma_0^2\), the sampling distribution of the sample mean \(bar{x}\) is given by

\[
\bar{x} \to N(\mu, \sigma_0^2/n).
\]

\(\bar{x}\) is not a pivotal quantity since its distribution \(N(\mu, \sigma_0^2/n)\) is dependent on the unknown parameter \(\mu\). However,

\[
Z_1 = \frac{\bar{x} - \mu}{\sigma_0/\sqrt{n}} \to N(0,1).
\]

Since \(Z_1\) has a standard normal distribution that is independent of parameters and statistics, \(Z_1\) is a pivotal quantity.

\textbf{Example 2}. If a random sample is taken from a normal population with unknown variance. Let \(s\) be the sample standard deviation. From \textbf{Result \#2}, we have

\[
T = \frac{\bar{x}-\mu}{s/\sqrt{n}} \to t_{n-1}
\]

\(T\) is also a pivotal quantity since its distribution is independent of the population parameters.

\textbf{Example 3}. If a random sample is taken from an unspecified population with an unknown variance. Let \(s\) be the sample standard deviation. If the sample size is large, by the C.L.T,

\[
\bar{x} \to N(\mu, \sigma^2/n) 
\]

The standardized form is independent of unknown parameters

\[
Z_2 = \frac{\bar{X}-\mu}{s/\sqrt{n}} \to N(0,1).
\]

Therefore, \(Z_2\) is a pivotal quantity.

\textbf{Example 4} Let \(\{ x_1, x_2, \cdots, x_n\}\) be a random sample taken from a binary population \(Y\) with \(P(X=1) = p\). If \(np > 5\) and \(n(1-p) >5\), according to \textbf{result \#4}, \(\hat{p} \to N(p, \sqrt{p(1-p)/n})\). The following standardized \(Z_3\) has a distribution

\[
Z_3 = \frac{\hat{p}-p}{\sqrt{\frac{p(1-p)}{n}}} \to N(0,1).
\]

\(Z_3\) is also a pivotal quantity.

From the above four examples, we can see that the pivotal quantity associated with a sample mean or sample proportion is either the standard normal distribution or a t-distribution. Both distributions are symmetric with respect to the vertical axis.

For ease of illustration, we use \(U\) to denote either one of the above four quantities (\(Z_1, Z_2, Z_3\), and \(T\)). Now, for a given confidence level of \(95\%\) (or other confidence levels), we can find two cut-off \(U\) values, denoted by \(U_1\) and \(U_2\) such that \(P(U_1 < U < U_2) = 95\%\).

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-97-1} \end{center}

The above figure gives the steps for constructing the confidence interval for population means and proportions.

\begin{itemize}
\item
  \textbf{Confidence Level} - The area of the shaded region in the above figure is called the confidence level. Note that 95\% of the U values will be in {[}\(U_1\), \(U_2\){]} is equivalent to sat the 95\% of sample means based on the same size will be in {[}LCL, UCL{]}.
\item
  \textbf{Critical Value} - \(U_2\) on the right-hand side of the density curve of the pivotal quantity is called the \textbf{critical value}. R functions \textbf{qnorm()} and \textbf{qt()} can be used to find the normal critical and t-critical values respectively.
\end{itemize}

\hypertarget{confidence-interval-of-population-mean}{%
\section{Confidence Interval of Population Mean}\label{confidence-interval-of-population-mean}}

This section introduces confidence intervals based on two sampling distributions: normal and t-distributions under different assumptions. We will use examples to illustrate how to construct confidence intervals with given confidence intervals.

{From the histogram of the mean heights of WCU students, we can see that a 95\% confidence interval of the population mean under normal sampling distribution is 2.5\% and 97.5\% quantiles. Therefore, R function \textbf{qnorm()} will be used to construct the confidence intervals of means and proportions. }

\hypertarget{normal-population-with-known-variance.}{%
\subsection{Normal Population with Known Variance.}\label{normal-population-with-known-variance.}}

According to result \#1 in the previous module, we have
\[
\bar{X} \to N(\mu, \sigma_0^2/n) \Longleftrightarrow \frac{\bar{X} -\mu}{\sigma_0/\sqrt{n}} \to N(0, 1).
\]
If the confidence interval \(1-\alpha = 95\%\), then \(U_1 = U_{0.025} = qnorm(0.025)\) and \(U_2 = U_{0.975} = qnorm(0.975)\).

\[
P\left( U_1<\frac{\bar{X} -\mu}{\sigma_0/\sqrt{n}} <U_2 \right) = 0.95 \Longleftrightarrow \\ P\left(\bar{x}+U_1\frac{\sigma_0}{\sqrt{n}} <\mu< \bar{x}+U_2\frac{\sigma_0}{\sqrt{n}}\right) = 0.95
\]
The \(95\%\) confidence interval of \(\mu\) is given by
\[
\left(\bar{x}+U_1\frac{\sigma_0}{\sqrt{n}}, \bar{x}+U_2\frac{\sigma_0}{\sqrt{n}}\right)=\left[qnorm(0.025, mean =\bar{x}, sd = \sigma_0), qnorm(0.975, mean =\bar{x}, sd = \sigma_0) \right].
\]

The left-hand side equation is the confidence interval and the right-hand side of the equation is the confidence interval expressed in R functions \textbf{qnorm()}.

\textbf{Example 1.} Suppose a researcher, interested in obtaining an estimate of the average level of some enzyme in a certain human population, takes a sample of 10 individuals, determines the level of the enzyme in each, and computes a sample mean of \(\bar{x}=22\). Suppose further it is known that the variable of interest is approximately normally distributed with a variance \(\sigma_0^2=45\) We wish to construct a confidence interval of the population mean \(\mu\) at a confidence level of 95\%.

\textbf{Solution} Let \(\bar{x}\) be the sample mean calculated based on a random sample with size \(n = 10\). Since the population is approximately distributed with a known variance \(\sigma_0^2 = 45\), according to result \#1 in the last note, we have the sampling distribution of the sample mean in the following form
\[
\bar{x} \to N(\mu, \frac{45}{10}).
\]

When we use \textbf{pnorm()} to find the quantiles, we replace the unknown mean \(\mu\) with the \(\bar{x}\). The following code is used to find the confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qnorm}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{), }\AttributeTok{mean =} \DecValTok{22}\NormalTok{, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{45}\SpecialCharTok{/}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 17.84229 26.15771
\end{verbatim}

\textbf{Conclusion}. We are 95\% confident that the interval {[}17.84, 26.16{]} includes the true mean level of some enzyme in a certain human population.

\textbf{Example 2.} Some studies of Alzheimer's disease (AD) have shown an increase in CO2 production in patients with the disease. In one such study, the following CO2 values were obtained from 16 neocortical biopsy samples from AD patients.

\begin{verbatim}
1009 1280 1180 1255 1547 2352 1956 1080 1776 1767 1680 2050 1452 2857 3100 1621
\end{verbatim}

Assume that the population of such values is normally distributed with a standard deviation of 350.

\textbf{Solution} Based on the condition, we have \(\bar{x} \to N(\mu, 350/\sqrt{16})\). The unknown population will be estimated by the sample mean \(\bar{x}\) in the following R code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{normal.sample }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1009}\NormalTok{, }\DecValTok{1280}\NormalTok{, }\DecValTok{1180}\NormalTok{, }\DecValTok{1255}\NormalTok{, }\DecValTok{1547}\NormalTok{, }\DecValTok{2352}\NormalTok{, }\DecValTok{1956}\NormalTok{, }\DecValTok{1080}\NormalTok{, }\DecValTok{1776}\NormalTok{, }\DecValTok{1767}\NormalTok{, }
                  \DecValTok{1680}\NormalTok{, }\DecValTok{2050}\NormalTok{, }\DecValTok{1452}\NormalTok{, }\DecValTok{2857}\NormalTok{, }\DecValTok{3100}\NormalTok{, }\DecValTok{1621}\NormalTok{)}
\NormalTok{sample.avg }\OtherTok{=} \FunctionTok{mean}\NormalTok{(normal.sample)    }\CommentTok{\# sample mean}
\DocumentationTok{\#\# 95\% confidence intervals}
\FunctionTok{qnorm}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{), }\AttributeTok{mean =}\NormalTok{ sample.avg, }\AttributeTok{sd =} \DecValTok{350}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{16}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1576.128 1919.122
\end{verbatim}

Conclusion. We are 95\% confident that the interval {[}1576.1, 1919.1{]} includes the true mean level of CO2 in neocortical biopsy samples from AD patients with size 16.

\hypertarget{normal-population-with-unknown-variance}{%
\subsection{Normal Population with Unknown Variance}\label{normal-population-with-unknown-variance}}

Recall in \textbf{Results \#2}, the standardized pivotal quantity \(U = T_{n-1}\), t-distribution with \(n-1\) degrees of freedom.

\[\frac{\bar{x}-\mu}{s/\sqrt{n}} \to t_{n-1}.\]

Using the same logic, we have the following \(95%
\) confidence intervals based on the t-distribution.

\[
\left(\bar{x}+qt(0.025, df=n-1)\frac{s}{\sqrt{n}}, \bar{x}+qt(0.975, df=n-1)\frac{s}{\sqrt{n}}\right)
\]

Unfortunately, there is no direct sampling distribution of \(\bar{x}\), so we have to use the above formula to find the t-confidence interval if we are given the sample mean (\(\bar{x}\)) and sample standard deviation (\(s\)). However, if we were given a data set, the R function \textbf{t.test()} can be used to generate the above confidence interval.

\textbf{Example 3} In a study of the effects of early Alzheimer's disease on non-declarative memory, the Category Fluency Test was used to establish baseline persistence semantic memory, and language abilities. The eight subjects in the sample had Category Fluency Test scores of 11, 10, 6, 3, 11, 10, 9, and 11. Assume that the eight subjects constitute a simple random sample from a normally distributed population of similar subjects with early Alzheimer's disease.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  What is the point estimate of the population mean?
\item
  What is the standard deviation of the sample?
\item
  What is the estimated standard error of the sample mean?
\item
  Construct a 95 percent confidence interval for the population mean category fluency test score.
\end{enumerate}

\textbf{Solution} The following R code gives the answers to the above questions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test.score}\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{11}\NormalTok{)}
\NormalTok{n }\OtherTok{=} \FunctionTok{length}\NormalTok{(test.score)                }\CommentTok{\# sample size}
\NormalTok{Q.a }\OtherTok{=} \FunctionTok{mean}\NormalTok{(test.score)}
\NormalTok{Q.b }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{sd}\NormalTok{(test.score),}\DecValTok{3}\NormalTok{)}
\NormalTok{Q.c }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{sd}\NormalTok{(test.score)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n),}\DecValTok{3}\NormalTok{)}
\NormalTok{Q.d }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{t.test}\NormalTok{(test.score, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}\SpecialCharTok{$}\NormalTok{conf.int,}\DecValTok{3}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{Q.a =}\NormalTok{ Q.a,}
      \AttributeTok{Q.b =}\NormalTok{ Q.b,}
      \AttributeTok{Q.c =}\NormalTok{ Q.c,}
      \AttributeTok{Q.d =} \FunctionTok{paste}\NormalTok{(}\StringTok{"["}\NormalTok{,Q.d[}\DecValTok{1}\NormalTok{], }\StringTok{", "}\NormalTok{, Q.d[}\DecValTok{2}\NormalTok{], }\StringTok{"]"}\NormalTok{))))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l}
\hline
Q.a & Q.b & Q.c & Q.d\\
\hline
8.875 & 2.9 & 1.025 & [ 6.45 ,  11.3 ]\\
\hline
\end{tabular}

The answers to questions a, b, and c are given in the above table. The 95\% confidence interval for the fluency test score is {[}6.45, 11.3{]} meaning that interval {[}6.45, 11.3{]} has a 95\% chance to include the true mean test score.

\textbf{Example 4}: A sample of 16 ten-year-old girls had a mean weight of 71.5 and a standard deviation of 12 pounds, respectively. Assuming normality, find a 99 percent confidence interval for \(\mu\).

\textbf{Solution} Based on the given assumption, the pivotal quantity has a t-distribution with 15 degrees of freedom. We cannot use \textbf{t.test()} to find the confidence interval since it requires the raw data set. We can then use the given formula to find the confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xbar }\OtherTok{=} \FloatTok{71.5}
\NormalTok{s }\OtherTok{=} \DecValTok{12}
\NormalTok{LCL }\OtherTok{=}\NormalTok{ xbar }\SpecialCharTok{+} \FunctionTok{qt}\NormalTok{(}\FloatTok{0.005}\NormalTok{, }\AttributeTok{df =} \DecValTok{15}\NormalTok{)}\SpecialCharTok{*}\NormalTok{s}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{16}\NormalTok{)}
\NormalTok{UCL }\OtherTok{=}\NormalTok{ xbar }\SpecialCharTok{+} \FunctionTok{qt}\NormalTok{(}\FloatTok{0.995}\NormalTok{, }\AttributeTok{df =} \DecValTok{15}\NormalTok{)}\SpecialCharTok{*}\NormalTok{s}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{16}\NormalTok{)}
\NormalTok{CI }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{LCL =}\NormalTok{ LCL, }\AttributeTok{UCL =}\NormalTok{ UCL)}
\NormalTok{CI }\OtherTok{=} \FunctionTok{round}\NormalTok{(CI,}\DecValTok{2}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(CI)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r}
\hline
LCL & UCL\\
\hline
62.66 & 80.34\\
\hline
\end{tabular}

The 95\% confidence interval of the mean weight is {[}62.66, 80.34{]}. Therefore, we are 99\% confident that the mean weight is in {[}62.66, 80.34{]}.

\hypertarget{unspecified-population-with-unknown-variance-but-with-large-sample-sizes}{%
\subsection{Unspecified Population with Unknown Variance but with Large Sample Sizes}\label{unspecified-population-with-unknown-variance-but-with-large-sample-sizes}}

When the sample size is large, we then use the central limit theorem (CLT). The pivotal quantity is approximately normally distributed. The confidence interval can be similarly found using the sample code as used in \textbf{Example 1}.

\textbf{Example 5} A physical therapist wished to estimate, with 99 percent confidence, the mean maximal strength of a particular muscle in a certain group of individuals. A sample of 64 subjects who participated in the experiment yielded a mean of 84.3 and a sample variance of 144.

\textbf{Solution} Since sample size \(n = 64\), according to the CLT, we have
\[
\bar{x} \to N(84.3, 144/64)
\]

We can use the above sampling distribution to find the 99\% confidence interval given in the following code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LCL }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.005}\NormalTok{, }\AttributeTok{mean =} \FloatTok{84.3}\NormalTok{, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{144}\SpecialCharTok{/}\DecValTok{64}\NormalTok{))}
\NormalTok{UCL }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.995}\NormalTok{, }\AttributeTok{mean =} \FloatTok{84.3}\NormalTok{, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{144}\SpecialCharTok{/}\DecValTok{64}\NormalTok{))}
\NormalTok{CI }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(LCL, UCL)}
\NormalTok{CI }\OtherTok{=} \FunctionTok{round}\NormalTok{(CI,}\DecValTok{2}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(CI)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r}
\hline
LCL & UCL\\
\hline
80.44 & 88.16\\
\hline
\end{tabular}

The 99\% confidence interval is {[}80.44, 88.16{]}. Therefore, we are 99\% confident that the mean maximal strength of a particular muscle in a certain group of individuals is between 80.44 and 88.16.

\hypertarget{confidence-interval-of-proportion}{%
\section{Confidence Interval of Proportion}\label{confidence-interval-of-proportion}}

According to \textbf{Result 4} in the previous section, we can find a 95\% confidence interval of the population proportion based on the following sampling distribution of \(\hat{p}\)

\[
\hat{p} \to N(p, \sqrt{\hat{p}(1-\hat{p})/n})
\]

\textbf{Example 6} To study patients who were mechanically ventilated in the intensive care unit of six hospitals in Buenos Aires, Argentina. The researchers found that of 472 mechanically ventilated patients, 63 had clinical evidence of ventilator-associated pneumonia (VAP). Construct a 95 percent confidence interval for the proportion of all mechanically ventilated patients at these hospitals who may be expected to develop VAP and interpret the confidence interval.

\textbf{Solution} Since \(n\hat{p} = 472 \times (63/472) = 63 > 5\) and \(n(1-\hat{p}) = 409 > 5\), according to \textbf{Result 4}, \(\hat{p} \to N(p, \sqrt{\hat{p}(1-\hat{p})/472})\). The following R code generates the 95\% confidence interval of the proportion.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{=} \DecValTok{472}
\NormalTok{phat }\OtherTok{=} \DecValTok{63}\SpecialCharTok{/}\DecValTok{472}
\NormalTok{s.phat }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{((}\DecValTok{63}\SpecialCharTok{/}\DecValTok{472}\NormalTok{)}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1{-}63}\SpecialCharTok{/}\DecValTok{472}\NormalTok{)}\SpecialCharTok{/}\DecValTok{472}\NormalTok{)}
\NormalTok{LCL }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\AttributeTok{mean =}\NormalTok{phat, }\AttributeTok{sd =}\NormalTok{ s.phat)}
\NormalTok{UCL }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{, }\AttributeTok{mean =}\NormalTok{phat, }\AttributeTok{sd =}\NormalTok{ s.phat)}
\NormalTok{CI}\OtherTok{=}\FunctionTok{cbind}\NormalTok{(LCL, UCL)}
\NormalTok{CI }\OtherTok{=} \FunctionTok{round}\NormalTok{(CI,}\DecValTok{2}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(CI)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r}
\hline
LCL & UCL\\
\hline
0.1 & 0.16\\
\hline
\end{tabular}

The 95\% confidence interval of the proportion of all mechanically ventilated patients at these hospitals who may be expected to develop VAP is {[}0.1, 0.16{]}. Therefore, {[}0.10, 0.16{]} has a 95\% chance to include the true proportion.

\hypertarget{two-sample-problems---comparing-two-population-means}{%
\section{Two Sample Problems - Comparing Two Population Means}\label{two-sample-problems---comparing-two-population-means}}

We only introduce two sample problems based on independent samples with large sample sizes. One important fact is that the \texttt{variances} of two \textbf{independent} variables are additive. To be more specific, let \(X\) and \(Y\) be independent random variables, then \(Var(X \pm Y) = Var(X) + Var(Y)\). \textbf{However}, standard deviations are \textbf{NOT} additive, \(sd(X \pm Y) = sd(X) + sd(Y)\).

In real-world applications, one of the practical questions is to compare the difference between two population means (\(\mu_1 - \mu_2\)). One way to address this issue is to construct a confidence interval for \(\mu_1 - \mu_2\).

\hypertarget{confidence-interval-of-the-difference-of-two-unspecified-populations-means}{%
\subsection{Confidence Interval of the Difference of Two Unspecified Populations Means}\label{confidence-interval-of-the-difference-of-two-unspecified-populations-means}}

Let \(\bar{x}_1\) and \(\bar{x}_2\) be sample means of two independent populations. The corresponding sample standard deviations are \(s_1\) and \(s_2\), and \(n_1 > 30\) and \(n_2 >30\) are sample sizes. The sampling distribution of \(\bar{x}_1 - \bar{x}_2\) is given by

\[
\bar{x}_1 - \bar{x}_2 \to N\left(\mu_1 - \mu_2, \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}} \right)
\]

Using the above sampling distribution, we can construct a 95\% confidence interval of the difference between two population means.

\textbf{Example 7} Despite common knowledge of the adverse effects of doing so, many women continue to smoke while pregnant. To examine the effectiveness of a smoking cessation program for pregnant women. The mean number of cigarettes smoked daily at the close of the program by the 328 women who completed the program was 4.3 with a standard deviation of 5.22. Among 64 women who did not complete the program, the mean number of cigarettes smoked per day at the close of the program was 13 with a standard deviation of 8.97. We wish to construct a 99 percent confidence interval for the difference between the means of the populations from which the samples may be presumed to have been selected.

\textbf{Solution} Since \(n_1 = 328 > 30\) and \(n_2 = 64 > 30\), then
\[
\bar{x}_1 - \bar{x}_2 \to N\left(\mu_1 - \mu_2, \sqrt{\frac{5.22^2}{328} + \frac{8.97^2}{64}}\right)
\]
The 95\% confidence interval is given by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n1 }\OtherTok{=} \DecValTok{328}
\NormalTok{n2 }\OtherTok{=} \DecValTok{64}
\NormalTok{x1bar }\OtherTok{=} \FloatTok{4.3}
\NormalTok{x2bar }\OtherTok{=} \DecValTok{13}
\NormalTok{s1 }\OtherTok{=} \FloatTok{5.22}
\NormalTok{s2 }\OtherTok{=} \FloatTok{8.97}
\NormalTok{LCL }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.005}\NormalTok{, }\AttributeTok{mean =}\NormalTok{ x1bar }\SpecialCharTok{{-}}\NormalTok{x2bar, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(s1}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{n1 }\SpecialCharTok{+}\NormalTok{ s2}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{n2))}
\NormalTok{UCL }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.995}\NormalTok{, }\AttributeTok{mean =}\NormalTok{ x1bar }\SpecialCharTok{{-}}\NormalTok{x2bar, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(s1}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{n1 }\SpecialCharTok{+}\NormalTok{ s2}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{n2))}
\NormalTok{CI}\OtherTok{=}\FunctionTok{cbind}\NormalTok{(LCL, UCL)}
\NormalTok{CI}\OtherTok{=}\FunctionTok{round}\NormalTok{(CI,}\DecValTok{2}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(CI)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r}
\hline
LCL & UCL\\
\hline
-11.68 & -5.72\\
\hline
\end{tabular}

Therefore, we are 99\% confident that the difference between the two population means \(\mu_1-\mu_2\) is in {[}-11.68, -5.72{]}. Since both confidence limits are negative, we can claim that \(\mu_1-\mu_2 < 0\), that is, \(\mu_2 > \mu_1\).

\hypertarget{confidence-interval-of-the-difference-of-two-normal-populations-means}{%
\subsection{Confidence Interval of the Difference of Two Normal Populations Means}\label{confidence-interval-of-the-difference-of-two-normal-populations-means}}

We only discuss a special case in which

\begin{itemize}
\item
  both populations are normal.
\item
  population variances are unknown but equal.
\end{itemize}

Because the two population variances are equal, we combine two samples to estimate the common variance using the following formula.

\[
s_{pool} = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2} }
\]

With the above pooled standard deviation, we have

\[
\frac{(\bar{x}_1 - \bar{x}_2)-(\mu_{1} - \mu_{2}) }{s_{pool}\sqrt{1/n_1 + 1/n_2}} \to t_{n_1 + n_2 -2}
\]

Let \(U_1 = qt(0.025, df =n_1+n_2 -2)\) and \(U_2 = qt(0.975, df =n_1+n_2 -2)\), then the 95\% confidence interval of \(\mu_1 - \mu_2\) is given by

\[
\left[(\bar{x}_1 - \bar{x}_2)+U_1\frac{s_{pool}}{\sqrt{1/n_1 + 1/n_2}}, (\bar{x}_1 - \bar{x}_2)+U_2\frac{s_{pool}}{\sqrt{1/n_1 + 1/n_2}}\right]
\]

\textbf{Example 8} To determine the effectiveness of an integrated outpatient dual-diagnosis treatment program for mentally ill subjects. The authors were addressing the problem of substance abuse issues among people with severe mental disorders. A retrospective chart review was performed on 50 consecutive patient referrals to the Substance Abuse/Mental Illness program at the VA San Diego Healthcare System. One of the outcome variables examined was the number of inpatient treatment days for a psychiatric disorder during the year following the end of the program. Among 18 subjects with schizophrenia, the mean number of treatment days was 4.7 with a standard deviation of 9.3. For 10 subjects with bipolar disorder, the mean number of psychiatric disorder treatment days was 8.8 with a standard deviation of 11.5. We wish to construct a 95 percent confidence interval for the difference between the means of the populations represented by these two samples. We assume that both populations are normal and have equal variances.

\textbf{Solution} Based on the given information, we pivotal quantity is given by

\[
\frac{(\bar{x}_1 - \bar{x}_2)-(\mu_1-\mu_2)}{\sqrt{s_1^2/n_1 + s_2^2/n_2}} \to t_{n_1+n_1 - 2}
\]

Therefore, the t-confidence interval of the difference of the two population means is given by the following code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n1 }\OtherTok{=} \DecValTok{18}
\NormalTok{n2}\OtherTok{=}\DecValTok{10}
\NormalTok{x1bar }\OtherTok{=} \FloatTok{4.7}
\NormalTok{x2bar }\OtherTok{=} \FloatTok{8.8}
\NormalTok{s1 }\OtherTok{=} \FloatTok{9.3}
\NormalTok{s2}\OtherTok{=}\FloatTok{11.5}
\NormalTok{s.pool }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(((n1}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{s1}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ (n2}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{s2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(n1}\SpecialCharTok{+}\NormalTok{n2}\DecValTok{{-}2}\NormalTok{))}
\NormalTok{LCL }\OtherTok{=}\NormalTok{ (x1bar}\SpecialCharTok{{-}}\NormalTok{x2bar)}\SpecialCharTok{+}\FunctionTok{qt}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\AttributeTok{df =}\NormalTok{ n1}\SpecialCharTok{+}\NormalTok{n2}\DecValTok{{-}2}\NormalTok{)}\SpecialCharTok{*}\NormalTok{s.pool}\SpecialCharTok{*}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{n1 }\SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{/}\NormalTok{n2)}
\NormalTok{UCL }\OtherTok{=}\NormalTok{ (x1bar}\SpecialCharTok{{-}}\NormalTok{x2bar)}\SpecialCharTok{+}\FunctionTok{qt}\NormalTok{(}\FloatTok{0.975}\NormalTok{, }\AttributeTok{df =}\NormalTok{ n1}\SpecialCharTok{+}\NormalTok{n2}\DecValTok{{-}2}\NormalTok{)}\SpecialCharTok{*}\NormalTok{s.pool}\SpecialCharTok{*}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{n1 }\SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{/}\NormalTok{n2)}
\NormalTok{CI}\OtherTok{=}\FunctionTok{cbind}\NormalTok{(LCL, UCL)}
\NormalTok{CI }\OtherTok{=} \FunctionTok{round}\NormalTok{(CI, }\DecValTok{2}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(CI)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r}
\hline
LCL & UCL\\
\hline
-12.3 & 4.1\\
\hline
\end{tabular}

Therefore, the 95\% confidence interval of \(\mu_1-\mu_2\) is {[}-12.3, 4.1{]}. Since the 95\% confidence interval does contain 0. We are 95\% confident that there is no significant difference between the two population means.

\textbf{Conclusion Remarks}

\begin{itemize}
\item
  We can discuss the difference between two independent proportions using the same logic in the previous sections.
\item
  In the next module, we will discuss testing hypotheses - another type of inference.
\end{itemize}

\hypertarget{assignment---confidence-intervals}{%
\section{Assignment - Confidence Intervals}\label{assignment---confidence-intervals}}

Please study the examples in the class note and complete the following problems. Please note that you are expected to interpret each of the confidence intervals you obtained. You can modify my code to calculate confidence intervals.

\textbf{Problem 1}

We wish to estimate the mean serum indirect bilirubin level of 4-day-old infants. The mean for a
sample of 16 infants was found to be 5.98 mg/100 cc. Assume that bilirubin levels in 4-day-old infants
are approximately normally distributed with a standard deviation of 3.5 mg/100 cc. Construct a 95\% confidence interval of the mean serum indirect bilirubin level of 4-day-old infants (\(\mu\)) and interpret the interval.

\textbf{Problem 2}

10 obstetrics and gynecology interns participated in a study conducted by researchers at the University of Colorado Health Sciences Center. The researchers wanted to assess competence in performing clinical breast examinations. One of the baseline measurements was the number of such examinations performed. The following data give the number of breast examinations performed for this sample of 10 interns.

30, 40, 8, 20, 26, 35, 35, 20, 25, 20

Construct a 95\% confidence interval for the number of breast examinations and give an interpretation of the confidence interval.

\textbf{Problem 3}

The punctuality of patients in keeping appointments is of interest to a research team. In a study of patient flow through the offices of general practitioners, it was found that a sample of 35 patients was 17.2 minutes late for appointments, on average. Previous research had shown the standard deviation to be about 8 minutes. The population distribution was felt to be non-normal. Find the 90 percent confidence interval for \(\mu\), the true mean amount of time late for appointments and interpret the confidence interval.

\textbf{Problem 4}

The following are the activity values (micromoles per minute per gram of tissue) of a
certain enzyme measured in normal gastric tissue of 35 patients with gastric carcinoma.

\begin{verbatim}
0.360, 1.189, 0.614, 0.788, 0.273, 2.464, 0.571, 1.827, 0.537, 0.374, 0.449, 0.262, 
0.448, 0.971, 0.372, 0.898, 0.411, 0.348, 1.925, 0.550, 0.622, 0.610, 0.319, 0.406, 
0.413, 0.767, 0.385, 0.674, 0.521, 0.603, 0.533, 0.662, 1.177, 0.307, 1.499
\end{verbatim}

We wish to construct a 95 percent confidence interval for the population mean and interpret the confidence interval. It is not necessary to assume that the sampled population of values is normally distributed.

\textbf{Problem 5}

In a study, 136 subjects with syncope or near syncope were studied. Syncope is the temporary loss of consciousness due to a sudden decline in blood flow to the brain. Of these subjects, 75 also reported having cardiovascular disease. Construct a 99 percent confidence interval for the population proportion of subjects with syncope or near syncope who also have cardiovascular disease and interpret the interval.

\textbf{Problem 6}

In a study of factors thought to be responsible for the adverse effects of smoking on human
reproduction, cadmium level determinations (nanograms per gram) were made on the placenta tissue of a sample of 14 mothers who were smokers and an independent random sample of 18 nonsmoking
mothers. The results were as follows:

Nonsmokers: 10.0, 8.4, 12.8, 25.0, 11.8, 9.8, 12.5, 15.4, 23.5, 9.4, 25.1, 19.5, 25.5, 9.8, 7.5, 11.8, 12.2, 15.0

Smokers: 30.0, 30.1, 15.0, 24.1, 30.5, 17.8, 16.8, 14.8, 13.4, 28.5, 17.5, 14.4, 12.5, 20.4

Assume that both smokers and non-smoker populations are normally distributed and have equal variance.

Does it appear likely that the mean cadmium level is higher among smokers than nonsmokers? Why
Do you reach this conclusion? {[}Hint: answer the above questions by constructing a 95\% confidence interval of the difference between population means.{]}

\hypertarget{testing-statistical-hypothesis}{%
\chapter{Testing Statistical Hypothesis}\label{testing-statistical-hypothesis}}

There are two basic statistical inferences: confidence interval and testing hypothesis. In confidence interval inference, we estimate the population parameter(s) by constructing an interval that reveals the information about the precision and accuracy of the estimate and the level of confidence of the estimate to be correct.

The other type of inference is to justify a statement about a population parameter. For example, if someone \textbf{claims} that the average height of students at a university is higher than 70 inches, how to justify the claim? To reach the binary decision of either supporting the claim or rejecting the claim, we need to gather information from the population and then use the sample evidence to make the statistical decision.

The logic of statistical hypothesis testing is similar to the medical diagnostic decision process and jury trial - both are evidence-based decision processes. Analogies between the statistical testing hypothesis and the aforementioned two processes are summarized in the following table.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-HypothesisTest-logic} 

}

\caption{Analogies of statistical hypothesis testing to jury trial and medical diagnostics}\label{fig:unnamed-chunk-107}
\end{figure}

Note that no matter whichever statistical decision is made, there will be two possible errors: Type I and type II errors. We can see from the above analogies that \textbf{the type I error is {more serious than} the type II error}. Because of this relationship between the two types of errors, in practice, we control the type I error and then minimize the type II error - this is the basis of Neymann-Pearson's Lemma for statistical hypothesis testing.

With the above conceptual understanding of the testing hypothesis, we next formally formulate the statistical testing hypothesis so that we can implement it in real-world applications.

\hypertarget{formulation-of-statistical-hypothesis-testing}{%
\section{Formulation of Statistical Hypothesis Testing}\label{formulation-of-statistical-hypothesis-testing}}

The formal statistical testing hypothesis is summarized in the following steps.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-Steps4HypothesisTest} 

}

\caption{Formulation of statistical hypothesis testing}\label{fig:unnamed-chunk-108}
\end{figure}

Unlike the formulation used in the textbook, we separate the \textbf{statistical hypothesis testing procedure} from the general testing problem. Steps 2-5 are actual statistical procedures. The general workflow of conducting a testing hypothesis is summarized in the following few sections.

\hypertarget{null-and-alternative-hypotheses}{%
\subsection{Null and Alternative Hypotheses}\label{null-and-alternative-hypotheses}}

We start with a practical question that involves data and a statement claiming population parameters.

\textbf{Example} The yield of alfalfa from a random sample of six test plots is 1.4, 1.6, 0.9, 1.9, 2.2, and 1.2 tons per acre. Assume that the random sample comes from a normal population. Test at the 0.05 level of significance whether this supports the contention that the average yield for this kind of alfalfa is 1.5 tons per acre.

\begin{itemize}
\tightlist
\item
  \textbf{Data Set}: \texttt{\{1.4,\ 1.6,\ 0.9,\ 1.9,\ 2.2,\ 1.2\}}
\item
  \textbf{Claim}: \texttt{The\ average\ yield\ for\ this\ kind\ of\ alfalfa\ is\ 1.5\ tons\ per\ acre}.
\end{itemize}

If we use the Greek letter \(\mu\) to denote the population mean, then the claim is \(\mu \ne 1.5\) of this particular example. In general, there are six possible claims

\[\mu = 1.5, \mu \ne 1.5, \mu > 1.5, \mu \le 1.5, \mu < 1.5, \mu > 1.5 \]

Three of the six potential claims \textbf{have an equal sign} and the other three potential claims \textbf{do not have an equal sign}.

\hypertarget{relationship-between-the-claim-and-the-statistical-hypotheses}{%
\subsection{Relationship between the claim and the statistical hypotheses}\label{relationship-between-the-claim-and-the-statistical-hypotheses}}

The statistical \textbf{null hypothesis} must contain an \textbf{equal sign} since we assume that \textbf{the null hypothesis is true}. The \textbf{alternative hypothesis} is the opposite of the \textbf{null hypothesis}!

In other words, if the claim has an \textbf{equal sign}, the corresponding \textbf{null hypothesis} is the same as the claim. Otherwise, the \textbf{opposite of the claim will be the null hypothesis}.

The above relationship is summarized in the following table.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-claim-null-hypothesis} 

}

\caption{Formulation of statistical hypothesis testing}\label{fig:unnamed-chunk-109}
\end{figure}

\textbf{Example 1 (Revisited)} Based on the above description, the \textbf{claim} is \(\mu = 1.5\). It contains an \textbf{equal sign}. Therefore, the \textbf{null hypothesis (Ho)} and the \textbf{alternative hypothesis (Ha)} are given by

Ho: \(\mu = 1.5\) v.s. Ha: \(\mu \ne 1.5\).

\hypertarget{test-statistics---statistical-evidence}{%
\subsection{Test Statistics - Statistical Evidence}\label{test-statistics---statistical-evidence}}

In the construction of confidence intervals of population mean and proportion, we used the distribution of \textbf{pivotal quantity} since it contains all the information needed for constructing a confidence interval. \textbf{{The same amount of information is needed for testing the hypothesis}}. We introduced four major types of \textbf{pivotal quantities} in the previous module for constructing the confidence intervals of the single population mean \(\mu\) and proportion \(p\).

In testing hypotheses, we assume the \textbf{null hypothesis} is true. Therefore, we replace the unknown population parameter with the claimed value of the parameter in the \textbf{null hypothesis}. Therefore, the four test statistics are under various assumptions.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-Test-Statistics} 

}

\caption{Test statistics under different assumptions}\label{fig:unnamed-chunk-110}
\end{figure}

The above four statistics are either standard normal or t distribution. \textbf{If the claimed value in the null hypothesis is close to the true population parameter, we would expect the value of the test statistic to be around zero}. However, if the value of the test statistic is \textbf{far away from zero}, we intend to reject the \textbf{null hypothesis} and accept the \textbf{alternative hypothesis}.

A similar question that was asked when we constructed confidence intervals needs to be answered in the testing hypothesis. The \textbf{null hypothesis} is rejected if the test statistic is far ways from zero. \textbf{{How Far Is Far?}}

\hypertarget{types-of-test-and-rejection-region}{%
\subsection{Types of Test and Rejection Region}\label{types-of-test-and-rejection-region}}

The rejection of a test is dependent on its specific type (right-, left-, and two-tailed) and the type I error (also called significance level \(\alpha\), the same \(\alpha\) used in the confidence level \(1-\alpha\)). Next, we assume the significance to be \(\alpha\). The reject region of each of the three types of tests is summarized in the following.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-Test-types} 

}

\caption{Three different types of hypothesis tests}\label{fig:unnamed-chunk-111}
\end{figure}

From the above figure, we can see the location of the rejection region from the form of the alternative test. The rejection region of the left-tailed test is on the left tail of \textbf{the distribution of the test statistic}. The area of the rejection regions is equal to the \textbf{significance level \(\alpha\)}. For a two-tailed test, there are two rejection regions located on both sides of the tails of the sampling distribution of the test statistic. The tail regions on both tails are equal to \(\alpha/2\). We can also similarly interpret the right-tailed test.

\hypertarget{statistical-decision-rule-p-value}{%
\subsection{Statistical Decision Rule: p-value}\label{statistical-decision-rule-p-value}}

In this subsection, we define the \textbf{p-value} based on the statistic and the significance level to determine whether the test statistic is in the rejection region.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-p-values} 

}

\caption{Definitions of p-values for different types of tests}\label{fig:unnamed-chunk-112}
\end{figure}

The definition of the p-value for different types of tests is given in the above figures. The statistical decision rule is summarized in the following.

\begin{itemize}
\item
  If the p-value is less than the significance level \(\alpha\), the \textbf{null hypothesis is rejected} and the alternative is accepted.
\item
  If the p-value is greater than the significance level \(\alpha\), the \textbf{null hypothesis is accepted} and the alternative is rejected.
\end{itemize}

\hypertarget{summary-of-the-test-hypothesis-procedure}{%
\subsection{Summary of the Test Hypothesis Procedure}\label{summary-of-the-test-hypothesis-procedure}}

The above subsection describes the steps for performing a formal statistical hypothesis. When we implement the testing procedure, we need to follow the above-mentioned steps. Particularly, write the original clearly and then based on the description in section 2.1.1 set up the \textbf{null hypothesis (Ho)} and the \textbf{alternative hypothesis (Ha)} correctly.

In the next section, we will present numerical examples with different claims and assumptions about the populations. For the two-sample comparison problems, we only focus on testing the difference between two population means.

Some of the examples will be based on the raw data set(s). The analysis related to your research will be based on raw data you generated from lab experiments or fieldwork.

\hypertarget{case-studies}{%
\section{Case Studies}\label{case-studies}}

One important piece of advice is to draw a density curve of the distribution of the underlying distribution of the test statistics and label all information about the population, samples, and the null hypothesis (rejection regions) on the density curve so you can choose a correct R function from \textbf{pnorm()} or \textbf{pt()} to find the p-value. As a convention, if you are given the significance level \(\alpha\), you are expected to use the default significance level \(\alpha = 0.05\) meaning that your resulting statistical decision only allows less than \(5\%\) of chance to be wrong.

\hypertarget{case-i-normal-population-with-a-known-variance.}{%
\subsection{Case I: normal population with a known variance.}\label{case-i-normal-population-with-a-known-variance.}}

This situation is not common in real-life applications unless you have some prior information about the population variance and you believe that variance remains unchanged.

\textbf{Example 2}: Researchers are interested in the mean age of a certain population. The data available to the researchers are the ages of a simple random \textbf{sample of 10 individuals} drawn from the population of interest with mean \(\bar{x} = 27\). It is assumed that the sample comes from a population whose ages are approximately normally distributed. Let us also assume that the population has a known variance of \(\sigma_0^2 = 20\). The question that researchers want to ask is: Can we conclude that the mean age of this population is different from 30 years? Assuming that the mean age of the population is equal to 30.

\textbf{Solution}: First of all, the \textbf{claim} is the mean age of the research population remains unchanged meaning that \(\mu_0=20\). Since the original claim has an equal sign in it, the null hypothesis is identical to the claim. Therefore, the null and alternative hypotheses are given by

\[
H_o: \mu = 30  \leftrightarrow  H_a: \mu \ne 30.
\]

Since the study population is normal with known variance \(\sigma_0^2=20\). Therefore, the test statistic
\[
TS = \frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}} \to N(0,1)
\]
The calculation of the p-value is given in the following code.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# given conditions}
\NormalTok{xbar }\OtherTok{=} \DecValTok{27}
\NormalTok{sig.sq}\FloatTok{.0} \OtherTok{=} \DecValTok{20}
\NormalTok{mu0 }\OtherTok{=} \DecValTok{30}
\NormalTok{n }\OtherTok{=} \DecValTok{10}
\NormalTok{alpha }\OtherTok{=} \FloatTok{0.05}   \CommentTok{\# not given, use the default}
\CommentTok{\# two{-}tailed normal test}
\NormalTok{TS }\OtherTok{=}\NormalTok{ (xbar }\SpecialCharTok{{-}}\NormalTok{ mu0)}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(sig.sq}\FloatTok{.0}\SpecialCharTok{/}\NormalTok{n))    }\CommentTok{\# test statistics}
\NormalTok{right.tail.area }\OtherTok{=} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(TS)      }\CommentTok{\# TS is standard normal}
\NormalTok{left.tail.area }\OtherTok{=} \FunctionTok{pnorm}\NormalTok{(TS)           }
\NormalTok{p.value }\OtherTok{=} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{min}\NormalTok{(right.tail.area, left.tail.area)   }\CommentTok{\# double the smaller tail area}
\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03389485
\end{verbatim}

p-value = 0.03389 is less than the default significance level of 0.05. We reject the NULL HYPOTHESIS. We conclude the current mean age of the study population is significantly different from 30 years ago.

\textbf{Remarks}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The statistical decision is always made on the \textbf{Null Hypothesis}!
\item
  If we are given a raw data set, we need to find the sample mean and sample size and then use the above code to find the p-value.
\end{enumerate}

\textbf{Example 3}: Refer to \textbf{Example 2}. Suppose, instead of asking if they could conclude that \(\mu_0= 30\), the
researchers had asked: Can we conclude that \(\mu < 30\)?

\textbf{Solution}: To this question, the claim is \(\mu < 30\) that does NOT have an equal sign in it, we need to the opposite the claim as the null hypothesis. The alternative hypothesis will be identical to the claim. That is, we have the following null and alternative hypotheses.

\[
H_o: \mu \ge 30  \leftrightarrow  H_a: \mu < 30.
\]

This is a left-tailed test. The rejection region is on the left tail of the density curve of the test statistic. The p-value is calculated by

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# given conditions}
\NormalTok{xbar }\OtherTok{=} \DecValTok{27}
\NormalTok{sig.sq}\FloatTok{.0} \OtherTok{=} \DecValTok{20}
\NormalTok{mu0 }\OtherTok{=} \DecValTok{30}
\NormalTok{n }\OtherTok{=} \DecValTok{10}
\NormalTok{alpha }\OtherTok{=} \FloatTok{0.05}   \CommentTok{\# not given, use the default}
\CommentTok{\# two{-}tailed normal test}
\NormalTok{TS }\OtherTok{=}\NormalTok{ (xbar }\SpecialCharTok{{-}}\NormalTok{ mu0)}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(sig.sq}\FloatTok{.0}\SpecialCharTok{/}\NormalTok{n))    }\CommentTok{\# test statistics}
\NormalTok{right.tail.area }\OtherTok{=} \FunctionTok{pnorm}\NormalTok{(TS)         }\CommentTok{\# TS is standard normal}
\NormalTok{p.value }\OtherTok{=}\NormalTok{ right.tail.area}
\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.01694743
\end{verbatim}

Since the p-value is less than 0.05. We reject the null hypothesis that \(\mu \ge 30\), and we conclude the actual mean age of the population less than 30.

\hypertarget{case-ii-normal-population-with-an-unknown-variance}{%
\subsection{Case II: Normal population with an unknown variance}\label{case-ii-normal-population-with-an-unknown-variance}}

From theory, if the sample was taken from a normal population with unknown variance, the test statistic is always a random variable that follows t-distribution with degrees of freedom \(df = n-1\).

\[
TS = \frac{\bar{x}-\mu_0}{s/\sqrt{n}} \to t_{n-1}
\]

If the sample is large, the t-distribution is close to the standard normal distribution. In this case, you can use either normal or t-distribution to find the p-value. \textbf{However}, if the sample size small (n \textless{} 30), we \textbf{MUST} use the t -distribution to find the p-value.

\textbf{Example 4}: Nakamura et al.~studied subjects with medial collateral ligament (MCL) and anterior cruciate ligament (ACL) tears. Between February 1995 and December 1997, 17 consecutive patients with combined acute ACL and grade III MCL injuries were treated by the same physician at the research center. One of the variables of interest was the length of time in days between the occurrence of the injury and the first magnetic resonance imaging (MRI). The data are shown in the following.

14, 0, 28, 14, 9, 10, 24, 9, 18, 4, 24, 26, 8, 2, 12, 21, 3

We wish to know if we can conclude that the mean number of days between injury and initial MRI \textbf{is not 15 days} in a population presumed to be represented by these sample data. The original problem did not mention the normal distribution of the population, but we assumed the normality of the population in order to perform a t-test about the population mean.

\textbf{Solution}: The claim about the population mean is \(\mu \ne 15\). This is a two-tailed test.

\[
H_o: \mu = 15  \leftrightarrow  H_a: \mu \ne 15.
\]

Since we have a small sample (n = 17) from an implicitly assumed population with an unknown variance. The test statistic

\[
TS = \frac{\bar{x} - \mu_0}{s/\sqrt{n}} \to t_{n-1}
\]

Since this is a t-test with a given set of raw data values, we can use a convenient R function \textbf{t.test()} was built based on the above formulas and the definition of p-values. You type \texttt{?t.test} in the R console to find the help document of this function and accompanying examples as well.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# defining a data set}
\NormalTok{days }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{14}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{3}\NormalTok{) }
\FunctionTok{t.test}\NormalTok{(days,                         }\CommentTok{\# data set}
       \AttributeTok{mu =} \DecValTok{15}\NormalTok{,                    }\CommentTok{\# claimed value in the null hypothesis}
       \AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{,     }\CommentTok{\# confidence level 0.95 =\textgreater{} significant level 1{-}0.95.}
       \AttributeTok{alternative =}\StringTok{"two.sided"}\NormalTok{)  }\CommentTok{\# alternative hypothesis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  days
## t = -0.79148, df = 16, p-value = 0.4402
## alternative hypothesis: true mean is not equal to 15
## 95 percent confidence interval:
##   8.725081 17.863155
## sample estimates:
## mean of x 
##  13.29412
\end{verbatim}

The p-value is 0.4403 which is greater than 0.05. We \textbf{fail to reject} the null hypothesis. Therefore, the sample DOES NOT have evidence to support the claim that \(\mu \ne 15\).

\textbf{Remark} We use the formula to find the p-value using \textbf{pt()} to find the p-values as shown in the following.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{days }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{14}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{xbar }\OtherTok{=} \FunctionTok{mean}\NormalTok{(days)}
\NormalTok{s }\OtherTok{=} \FunctionTok{sd}\NormalTok{(days)}
\NormalTok{mu0 }\OtherTok{=} \DecValTok{15}
\NormalTok{n }\OtherTok{=} \FunctionTok{length}\NormalTok{(days)}
\NormalTok{alpha }\OtherTok{=} \FloatTok{0.05}    \CommentTok{\# not given, use the default}
\CommentTok{\# two{-}tailed normal test}
\NormalTok{TS }\OtherTok{=}\NormalTok{ (xbar }\SpecialCharTok{{-}}\NormalTok{ mu0)}\SpecialCharTok{/}\NormalTok{(s}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n))    }\CommentTok{\# test statistics}
\NormalTok{right.tail.area }\OtherTok{=} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pt}\NormalTok{(TS, }\AttributeTok{df =}\NormalTok{ n}\DecValTok{{-}1}\NormalTok{)         }\CommentTok{\# TS is standard normal}
\NormalTok{leftt.tail.area }\OtherTok{=} \FunctionTok{pt}\NormalTok{(TS, }\AttributeTok{df =}\NormalTok{ n}\DecValTok{{-}1}\NormalTok{) }
\NormalTok{p.value }\OtherTok{=} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{min}\NormalTok{(right.tail.area,leftt.tail.area)}
\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4402394
\end{verbatim}

We can see the p-values generated from the two methods are identical.

\hypertarget{unspecified-population-with-an-unknown-variance---clt}{%
\section{Unspecified population with an unknown variance - CLT}\label{unspecified-population-with-an-unknown-variance---clt}}

This is a direct application of the CLT. The key is that the sample size MUST be large!

\textbf{Example 5}: The goal of a study by Klingler et al.~was to determine how symptom recognition and perception influence clinical presentation as a function of race. They characterized symptoms and care-seeking behavior in African-American patients with chest pain seen in the emergency department. One of the presenting vital signs was systolic blood
pressure. Among 157 African-American men, the mean systolic blood pressure was 146mm Hg with a standard deviation of 27. We wish to know if, on the basis of these data, we may conclude that the mean systolic blood pressure for a population of African-American men \textbf{is greater than} 140.

\textbf{Solution}: The claim is that the mean systolic blood pressure for a population of African-American men \textbf{is greater than} 140. That is \(\mu > 140\). Since the claim does not have an equal sign in it, its opposite will be the null hypothesis.

\[
H_o: \mu \le 140  \leftrightarrow  H_a: \mu > 140.
\]

We are also given the sample size \(n = 157 > 30\), by the CLT, the test statistic

\[
TS = \frac{\bar{x} - \mu_0}{s/\sqrt{n}} \to N(0,1)
\]

The following code calculates the p-value

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# sample information}
\NormalTok{n }\OtherTok{=} \DecValTok{157}
\NormalTok{xbar }\OtherTok{=} \DecValTok{146}
\NormalTok{s }\OtherTok{=} \DecValTok{27}
\NormalTok{mu0 }\OtherTok{=} \DecValTok{140}
\DocumentationTok{\#\#\#}
\NormalTok{TS }\OtherTok{=}\NormalTok{ (xbar }\SpecialCharTok{{-}}\NormalTok{ mu0)}\SpecialCharTok{/}\NormalTok{(s}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{157}\NormalTok{))}
\CommentTok{\# This is a right{-}tailed test. We need the right tail area using 1{-} pnorm(TS)}
\NormalTok{p.value }\OtherTok{=} \DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{pnorm}\NormalTok{(TS)}
\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.002681041
\end{verbatim}

\hypertarget{testing-population-proportion}{%
\section{Testing Population Proportion}\label{testing-population-proportion}}

The most important step is to check whether conditions \(np > 5\) and \(n(1-p) > 5\) before claiming that
\[
TS = \frac{\hat{p}-p_0}{\sqrt{\hat{p}(1-\hat{p})/n}} \to N(0,1)
\]
R has a function \textbf{prop.test()} for testing the proportion and equality of two proportions. Of course, we can also translate the above formula and appropriate distribution of the test statistics to calculate the p-value.

\textbf{Example 6}: Wagenknecht et al.~collected data on a sample of 301 Hispanic women living in San Antonio, Texas. One variable of interest was the percentage of subjects with impaired fasting glucose (IFG). IFG refers to a metabolic stage intermediate between normal glucose homeostasis and diabetes. In the study, 24 women were classified in the IFG stage. The article cites population estimates for IFG among Hispanic women in Texas as 6.3 percent. Is there sufficient evidence to indicate that the population of Hispanic women in San Antonio has a prevalence of IFG higher than 6.3 percent?

\textbf{Solution}: Note that the claim is that the population of Hispanic women in San Antonio has a prevalence of IFG higher than 6.3 percent. That is, \(p > 0.063\). The null and alternative hypotheses are given by

\[
H_o: p \le 0.063  \leftrightarrow  H_a: p > 0.063.
\]

Since \(\hat{p} = 24/301\) \(n\hat{p} = 301\times\hat{p} = 24 > 5\), \(n(1-\hat{p}) = 301\times(1-24/301) > 5\). We can claim that the test statistic is approximately normally distributed.

\[
TS_0 = \frac{\hat{p}-p_0}{\sqrt{\hat{p}(1-\hat{p})/n}} \to N(0,1)
\]

R also has a built-in function \textbf{prop.test()} that can be used for testing a single proportion or the equality of two proportions. Here we use the \textbf{prop.test()} to conduct the above right-tailed test. The test used in the R function has the following form

\[
TS_1 = \frac{\hat{p}-p_0}{\sqrt{p_0(1-p_0)/n}} \to N(0,1)
\]

The p-value reported from \textbf{prop.test()} is slightly different from the formula in which the denominator uses \(\hat{p}\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\AttributeTok{x=}\DecValTok{24}\NormalTok{,                     }\CommentTok{\# number of successes}
          \AttributeTok{n =} \DecValTok{301}\NormalTok{,                  }\CommentTok{\# sample size}
          \AttributeTok{p =} \FloatTok{0.063}\NormalTok{,                }\CommentTok{\# claimed probability in the null hypothesis}
          \AttributeTok{alternative =} \StringTok{"greater"}\NormalTok{,  }\CommentTok{\# right{-}tailed test}
          \AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)        }\CommentTok{\# significance level = 1 {-} confidence level}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  1-sample proportions test with continuity correction
## 
## data:  24 out of 301, null probability 0.063
## X-squared = 1.1585, df = 1, p-value = 0.1409
## alternative hypothesis: true p is greater than 0.063
## 95 percent confidence interval:
##  0.05623225 1.00000000
## sample estimates:
##          p 
## 0.07973422
\end{verbatim}

The p-value is 0.1409 which is greater than 0.05. We fail to reject the null hypothesis that \(p \le 0.063\). Therefore, the sample does not support the claim that \(p > 0.063\).

\textbf{Remark} Both \(TS_0\) and \(TS_1\) are valid statistics. They are derived using different methods. The following code shows that the two statistics result in similar p-values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TS1 }\OtherTok{=}\NormalTok{ (}\DecValTok{24}\SpecialCharTok{/}\DecValTok{301}\FloatTok{{-}0.063}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{((}\DecValTok{24}\SpecialCharTok{/}\DecValTok{301}\NormalTok{)}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1{-}24}\SpecialCharTok{/}\DecValTok{301}\NormalTok{)}\SpecialCharTok{/}\DecValTok{301}\NormalTok{))}
\NormalTok{TS2 }\OtherTok{=}\NormalTok{ (}\DecValTok{24}\SpecialCharTok{/}\DecValTok{301}\FloatTok{{-}0.063}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{((}\FloatTok{0.063}\NormalTok{)}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\FloatTok{{-}0.063}\NormalTok{)}\SpecialCharTok{/}\DecValTok{301}\NormalTok{))}
\NormalTok{p.value1 }\OtherTok{=} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(TS1)}
\NormalTok{p.value2 }\OtherTok{=} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(TS2)}
\FunctionTok{cbind}\NormalTok{(p.value1, p.value2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       p.value1  p.value2
## [1,] 0.1419072 0.1160539
\end{verbatim}

\hypertarget{testing-the-difference-between-two-population-means}{%
\section{Testing the difference between two population means}\label{testing-the-difference-between-two-population-means}}

Comparing two population means is common in practice. In this subsection, we introduce two special procedures for testing two population means under different conditions.

\hypertarget{both-populations-are-unspecified-and-sample-sizes-are-large.}{%
\subsection{Both populations are unspecified and sample sizes are large.}\label{both-populations-are-unspecified-and-sample-sizes-are-large.}}

Assume that \(\{ x_1, x_2, \cdots, x_{n_1}\}\) and \(\{ y_1, y_2, \cdots, y_{n_2}\}\) are taken from two independent populations with means \(\mu_1\) and \(\mu_2\), respectively. Assume \(n_1 > 30\) and \(n_2 > 30\). By CLT, we have

\[
\bar{x}-\bar{y} \to N\left(\mu_1-\mu_2, \sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}\right).
\]

Then the test statistic for testing \(\mu_1-\mu_2\) is defined to be

\[
TS = \frac{(\bar{x}-\bar{y}) -(\mu_1-\mu_2)}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}} \to N(0, 1)
\]

\textbf{Example 7}: To identify the role of various disease states and additional risk factors in the development of thrombosis. One focus of the study was to determine if there were differing levels of the anticardiolipin antibody IgG in subjects with and without thrombosis. The following table summarizes the researchers' findings in a study

\begin{verbatim}
Sample statistics:

                     mean   size   stdev
Thrombosis (x)      59.01    53    44.89
No thrombosis (y)   46.61    54    34.85
\end{verbatim}

We wish to know if we may conclude, on the basis of these results, that, in general, persons with thrombosis have, on average, higher IgG levels than persons without thrombosis.

\textbf{Solution}: The \textbf{claim} is that persons with thrombosis have, on average, higher IgG levels than persons without thrombosis, that is, \(\mu_1 - \mu_2 > 0\). The opposite of the claim will be the null hypothesis.

\[
H_o: \mu_1 - \mu_2 \le 0 \leftrightarrow  H_a: \mu_1 - \mu_2 > 0 .
\]

This is a right-tailed test. The test statistic is

\[
TS = \frac{(\bar{x}-\bar{y}) -(\mu_1-\mu_2)}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}} =
 \frac{(59.01-46.61) -(0)}{\sqrt{\frac{44.89^2}{53}+\frac{34.85^2}{54}}}
\]

We use the following code to find the p-value.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#                      mean   size   stdev}
\CommentTok{\# Thrombosis (x)      59.01    53    44.89}
\CommentTok{\# No thrombosis (y)   46.61    54    34.85}
\NormalTok{xbar }\OtherTok{=} \FloatTok{59.01}
\NormalTok{ybar }\OtherTok{=} \FloatTok{46.61}
\NormalTok{s1 }\OtherTok{=} \FloatTok{44.89}
\NormalTok{s2 }\OtherTok{=} \FloatTok{34.85}
\NormalTok{n1 }\OtherTok{=} \DecValTok{53}
\NormalTok{n2 }\OtherTok{=} \DecValTok{54}
\DocumentationTok{\#\#}
\NormalTok{TS }\OtherTok{=}\NormalTok{ ((xbar }\SpecialCharTok{{-}}\NormalTok{ ybar) }\SpecialCharTok{{-}} \DecValTok{0}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(s1}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{n1 }\SpecialCharTok{+}\NormalTok{ s2}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{n2)}
\DocumentationTok{\#\# right{-}tailed test}
\NormalTok{p.value }\OtherTok{=} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(TS)}
\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05546304
\end{verbatim}

Since p-value = 0.0555 \textgreater{} 0.05, we fail to reject the \textbf{null hypothesis} at level 0.05. We conclude that the \(\mu_1 - \mu_2 > 0\) meaning that persons with thrombosis have, on average, higher IgG levels than persons without thrombosis.

\textbf{Remark}: If we are given two raw data sets, we use \textbf{mean()} and \textbf{var()} and \textbf{length()} to calculate the means, variances, and sample sizes.

\hypertarget{both-populations-are-normal-with-unknown-but-equal-variances}{%
\subsection{Both populations are normal with unknown but equal variances}\label{both-populations-are-normal-with-unknown-but-equal-variances}}

Assume that \(\{ x_1, x_2, \cdots, x_{n_1}\}\) and \(\{ y_1, y_2, \cdots, y_{n_2}\}\) are taken from two independent \textbf{normal} populations with means \(\mu_1\) and \(\mu_2\), respectively. Let \(s_1^2\) and \(s_2^2\) be the corresponding sample variances. Since we assume that the two population variances are equal.

\[
s_{pool} = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 -2}}
\]

Then the test statistic for testing \(\mu_1 - \mu_2\) is defined to be

\[
TS = \frac{(\bar{x}-\bar{y}) -(\mu_1-\mu_2)}{\sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 -2}}} \to N(0, 1)
\]

We can use the above formulas to find the p-value based on the type of test. In R, \textbf{t.test()} can also be used to generate the result directly if we are given the raw data.

\textbf{Example 8}: To investigate wheelchair maneuvering in individuals with lower-level spinal cord injury (SCI) and healthy controls (C). Subjects used a modified wheelchair to incorporate a rigid seat surface to facilitate the specified experimental measurements. Interface pressure measurement was recorded by using a high-resolution pressure-sensitive mat with a spatial resolution of four sensors per square centimeter taped on the rigid seat support. During static sitting conditions, average pressures were recorded under the ischial tuberosities (the bottom part of the pelvic bones). The data for measurements of the left ischial tuberosity (in mm Hg) for the SCI and control groups are shown in the following

\begin{verbatim}
Control: 131 115 124 131 122 117 88 114 150 169
SCI:     60 150 130 180 163 130 121 119 130 148
\end{verbatim}

We wish to know if we may conclude, on the basis of these data, that, in general, healthy subjects exhibit lower pressure than SCI subjects.

\textbf{Solution}: The \textbf{claim} is healthy subjects exhibit lower pressure than SCI subjects, \(\mu_c < \mu_s\). Since the claim \(\mu_c - \mu_s < 0\) has no equal sign in it,

\[
H_o: \mu_c - \mu_s \ge 0 \leftrightarrow  H_a: \mu_1 - \mu_2 < 0 .
\]

This is a left-tailed test. We use the following code to test the above hypothesis.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# R is case{-}sensitive, we encourage to use of all lowercase letters to name variables}
\NormalTok{control }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{131}\NormalTok{, }\DecValTok{115}\NormalTok{, }\DecValTok{124}\NormalTok{, }\DecValTok{131}\NormalTok{, }\DecValTok{122}\NormalTok{, }\DecValTok{117}\NormalTok{, }\DecValTok{88}\NormalTok{, }\DecValTok{114}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{169}\NormalTok{)}
\NormalTok{sci}\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{60}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{130}\NormalTok{, }\DecValTok{180}\NormalTok{, }\DecValTok{163}\NormalTok{, }\DecValTok{130}\NormalTok{, }\DecValTok{121}\NormalTok{, }\DecValTok{119}\NormalTok{, }\DecValTok{130}\NormalTok{, }\DecValTok{148}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{t.test}\NormalTok{(control, sci, }
       \AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{, }
       \AttributeTok{alternative =} \StringTok{"less"}\NormalTok{, }
       \AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  control and sci
## t = -0.56936, df = 18, p-value = 0.2881
## alternative hypothesis: true difference in means is less than 0
## 95 percent confidence interval:
##      -Inf 14.31935
## sample estimates:
## mean of x mean of y 
##     126.1     133.1
\end{verbatim}

We can also use the above formulas to find the p-value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{control }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{131}\NormalTok{, }\DecValTok{115}\NormalTok{, }\DecValTok{124}\NormalTok{, }\DecValTok{131}\NormalTok{, }\DecValTok{122}\NormalTok{, }\DecValTok{117}\NormalTok{, }\DecValTok{88}\NormalTok{, }\DecValTok{114}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{169}\NormalTok{)}
\NormalTok{sci}\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{60}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{130}\NormalTok{, }\DecValTok{180}\NormalTok{, }\DecValTok{163}\NormalTok{, }\DecValTok{130}\NormalTok{, }\DecValTok{121}\NormalTok{, }\DecValTok{119}\NormalTok{, }\DecValTok{130}\NormalTok{, }\DecValTok{148}\NormalTok{)}
\NormalTok{xbar.c }\OtherTok{=} \FunctionTok{mean}\NormalTok{(control)}
\NormalTok{s.sq.c }\OtherTok{=} \FunctionTok{var}\NormalTok{(control)}
\NormalTok{ybar.s }\OtherTok{=} \FunctionTok{mean}\NormalTok{(sci)}
\NormalTok{s.sq.s }\OtherTok{=} \FunctionTok{var}\NormalTok{(sci)}
\NormalTok{n.c }\OtherTok{=} \FunctionTok{length}\NormalTok{(control)}
\NormalTok{n.s }\OtherTok{=} \FunctionTok{length}\NormalTok{(sci)}
\DocumentationTok{\#\#}
\NormalTok{s.pool }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(((n.c}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{s.sq.c}\SpecialCharTok{+}\NormalTok{(n.s}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{s.sq.s)}\SpecialCharTok{/}\NormalTok{(n.c}\SpecialCharTok{+}\NormalTok{n.s}\DecValTok{{-}2}\NormalTok{))}
\DocumentationTok{\#\# }
\NormalTok{TS }\OtherTok{=}\NormalTok{ (xbar.c }\SpecialCharTok{{-}}\NormalTok{ ybar.s)}\SpecialCharTok{/}\NormalTok{(s.pool}\SpecialCharTok{*}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{n.c }\SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{/}\NormalTok{n.s))}
\DocumentationTok{\#\# left{-}tailed test}
\NormalTok{p.value }\OtherTok{=} \FunctionTok{pt}\NormalTok{(TS, }\AttributeTok{df =}\NormalTok{ n.c}\SpecialCharTok{+}\NormalTok{n.s}\DecValTok{{-}2}\NormalTok{)}
\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2880734
\end{verbatim}

With p-value = 0.288 \textgreater{} 0.05, we fail to reject the \textbf{null hypothesis} and reject the alternative hypothesis. We don't have the sample evidence to support the claim that healthy subjects exhibit lower pressure than SCI subjects.

\textbf{Remark}: \textbf{t.test()} can conduct two-sample test with unequal variances.

\hypertarget{paired-t-test}{%
\subsection{Paired t-test}\label{paired-t-test}}

The paired t-test is widely used in clinical studies. For example, an investigator wants to assess the effect of an intervention in reducing systolic blood pressure (SBP) in a pre-post design. Here, for each patient, there would be two observations of SBP, that is, before and after. Here instead of individual observations, the difference between pairs of observations would be of interest and the problem reduces to the one-sample situation where the null hypothesis would be to test the mean difference in SBP equal to zero against the alternate hypothesis of mean SBP being not equal to zero. The underlying assumption for using paired t-test is that under the null hypothesis \textbf{the population of difference is normally distributed} and this can be judged using the sample values.

\textbf{t.test()} can do the paired test. We can also convert the two-sample problem to a single-sample problem. Let \(\{x_1, x_2, \cdots, x_n \}\) be the before sample and \(\{y_1, y_2, \cdots, y_n \}\) be the after sample. We can take the difference of the corresponding before-after sample values to create a single sample \(\{y_1-x_1, y_2-x_2, \cdots, y_n-x_n \}\). We can use the one-sample procedure to test the difference between before and after means.

\textbf{Example 9}: To study the effects of reminiscence therapy for older women with depression. She studied 15 women 60 years or older residing for 3 months or longer in an assisted living long-term care facility. For this study, depression was measured by the Geriatric Depression Scale (GDS). Higher scores indicate more severe depression symptoms. The participants received reminiscence therapy for long-term care, which uses family photographs, scrapbooks, and personal memorabilia to stimulate memory and conversation among group members. Pre-treatment and post-treatment
depression scores are given in the following table. Can we conclude, based on these data, that subjects who participate in reminiscence therapy experience, on average, a decline in GDS depression scores? Let \(\alpha = 01\).

\begin{verbatim}
PreGDS:  12 10 16 2 12 18 11 16 16 10 14 21 9 19 20
PostGDS: 11 10 11 3  9 13  8 14 16 10 12 22 9 16 18
\end{verbatim}

\textbf{Solution}: The \textbf{claim} is that subjects who participate in reminiscence therapy experience, on average, a decline in GDS depression scores. \(\mu_{after} - \mu_{before} < 0\). This is a left-tailed test.

\[
H_o: \mu_{after} - \mu_{before} \ge 0 \leftrightarrow  H_a: \mu_{after} - \mu_{before} < 0 .
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pre.GDS }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\NormalTok{post.GDS }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{3}\NormalTok{,  }\DecValTok{9}\NormalTok{, }\DecValTok{13}\NormalTok{,  }\DecValTok{8}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{18}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{t.test}\NormalTok{(post.GDS, pre.GDS,  }
       \AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{, }
       \AttributeTok{alternative =} \StringTok{"less"}\NormalTok{, }
       \AttributeTok{paired =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Paired t-test
## 
## data:  post.GDS and pre.GDS
## t = -3.167, df = 14, p-value = 0.003428
## alternative hypothesis: true mean difference is less than 0
## 95 percent confidence interval:
##        -Inf -0.7101668
## sample estimates:
## mean difference 
##            -1.6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pre.GDS }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\NormalTok{post.GDS }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{3}\NormalTok{,  }\DecValTok{9}\NormalTok{, }\DecValTok{13}\NormalTok{,  }\DecValTok{8}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{18}\NormalTok{)}
\NormalTok{dif.GDS }\OtherTok{=}\NormalTok{ post.GDS }\SpecialCharTok{{-}}\NormalTok{ pre.GDS}
\FunctionTok{t.test}\NormalTok{(dif.GDS,}
       \AttributeTok{mu =} \DecValTok{0}\NormalTok{,}
       \AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{,}
       \AttributeTok{alternative =} \StringTok{"less"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  dif.GDS
## t = -3.167, df = 14, p-value = 0.003428
## alternative hypothesis: true mean is less than 0
## 95 percent confidence interval:
##        -Inf -0.7101668
## sample estimates:
## mean of x 
##      -1.6
\end{verbatim}

\hypertarget{concluding-remarks}{%
\subsection{Concluding Remarks}\label{concluding-remarks}}

The R built-in function \textbf{t.test(x,y)} is a black-box method that is convenient to perform the t-tests. For a two-sample test, the difference in the hypothesis testing is defined to be \(\mu_x - \mu_y\). \textbf{Reversing the order will result in a wrong answer}.

\textbf{prop.test()} can also be used to test the equality of two proportions. We also need to pay attention to the order of the two proportions.

Using built-in functions \textbf{t.test()} and \textbf{prop.test()} is convenient, but we have to know how these functions were set up to avoid unnecessary mistakes. Using formulas to perform test hypotheses can help enhance the understanding of the concept of these testing procedures.

\hypertarget{practice-problems}{%
\section{Practice Problems}\label{practice-problems}}

This is an open book and open note exam. The level of detail in your solution should be similar to that in the examples in the class notes. Please keep in mind that interpretation of results is as important as generation of the results. You can use either the built-in R functions or the formulas given in the class notes to complete the exams.

For confidence interval and testing hypothesis problems, you need to \textbf{justify} the sampling distributions and interpret the results. The default confidence level is 0.95 and the default significance level is 0.05.

\textbf{Problem 1}

In a study of the physical endurance levels of male college freshmen, the following composite endurance scores based on several exercise routines were collected.

\begin{verbatim}
254, 281, 192, 260, 212, 179, 225, 179, 181, 149, 
182, 210, 235, 239, 258, 166, 159, 223, 186, 190,
180, 188, 135, 233, 220, 204, 219, 211, 245, 151, 
198, 190, 151, 157, 204, 238, 205, 229, 191, 200,
222, 187, 134, 193, 264, 312, 214, 227, 190, 212, 
165, 194, 206, 193, 218, 198, 241, 149, 164, 225,
265, 222, 264, 249, 175, 205, 252, 210, 178, 159, 
220, 201, 203, 172, 234, 198, 173, 187, 189, 237,
272, 195, 227, 230, 168, 232, 217, 249, 196, 223,
232, 191, 175, 236, 152, 258, 155, 215, 197, 210,
214, 278, 252, 283, 205, 184, 172, 228, 193, 130,
218, 213, 172, 159, 203, 212, 117, 197, 206, 198,
169, 187, 204, 180, 261, 236, 217, 205, 212, 218,
191, 124, 199, 235, 139, 231, 116, 182, 243, 217,
251, 206, 173, 236, 215, 228, 183, 204, 186, 134,
188, 195, 240, 163, 208
\end{verbatim}

Use the above data to construct a frequency table and a histogram. Describe your findings from the histogram.

\textbf{Problem 2}

Iron deficiency anemia is an important nutritional health problem in the United States. A dietary assessment was performed on 51 boys 9 to 11 years of age whose families were below the poverty level. The mean daily iron intake among these boys was found to be 12.50 mg with a standard deviation of 4.75 mg. Suppose the mean daily iron intake among a large population of 9- to 11-year-old boys from all income strata is 14.44 mg. We want to test whether the mean iron intake among the low-income group is different from that of the general population. Carry out the hypothesis test using the critical-value method with an \(\alpha\) level of .05. State the hypotheses that we can use to consider this question and summarize your findings.

\textbf{Problem 3}

A topic of recent clinical interest is the possibility of using drugs to reduce infarct size in patients who have had a myocardial infarction within the past 24 hours. Suppose we know that in untreated patients the mean infarct size is 25 (ck-g-EQ/m2). Furthermore, in 8 patients treated with a drug, the mean infarct size is 16 with a standard deviation of 10. Is the drug effective in \textbf{reducing} infarct size? Assuming that the infarct size is normally distributed.

\textbf{Problem 4}

Drug A was prescribed for a random sample of 12 patients complaining of insomnia. An independent random sample of 16 patients with the same complaint received drug B. The number of hours of sleep experienced during the second night after treatment began was as follows.

\begin{verbatim}
A: 3.5, 5.7, 3.4, 6.9, 17.8, 3.8, 3.0, 6.4, 6.8, 3.6, 6.9, 5.7
B: 4.5, 11.7, 10.8, 4.5, 6.3, 3.8, 6.2, 6.6, 7.1, 6.4, 4.5, 5.1, 3.2, 4.7, 4.5, 3.0
\end{verbatim}

Construct a 95 percent confidence interval for the difference between the population means. Assume that the populations are normal and variances are unknown but equal.

\textbf{Problem 5}

Can we conclude that, on average, lymphocytes and tumor cells differ in size? The following are the cell diameters (\(\mu m\)) of 40 lymphocytes and 50 tumor cells obtained from biopsies of tissue from patients with melanoma.

\begin{verbatim}
## Lymphocytes
9.0,  9.4,  4.7,   4.8,  8.9,  4.9,  8.4,  5.9,  6.3,  5.7,  
5.0,  3.5,  7.8,  10.4,  8.0,  8.0,  8.6,  7.0,  6.8,  7.1,   
5.7,  7.6,  6.2,   7.1,  7.4,  8.7,  4.9,  7.4,  6.4,  7.1,  
6.3,  8.8,  8.8,  5.2,  7.1,  5.3,   4.7,  8.4,  6.4,  8.3

## Tumor Cells
12.6, 14.6, 16.2, 23.9, 23.3, 17.1, 20.0, 21.0, 19.1, 19.4,
16.7, 15.9, 15.8, 16.0, 17.9, 13.4, 19.1, 16.6, 18.9, 18.7,
20.0, 17.8, 13.9, 22.1, 13.9, 18.3, 22.8, 13.0, 17.9, 15.2,
17.7, 15.1, 16.9, 16.4, 22.8, 19.4, 19.6, 18.4, 18.2, 20.7,
16.3, 17.7, 18.1, 24.3, 11.2, 19.5, 18.6, 16.4, 16.1, 21.5
\end{verbatim}

What statistical method you are going to use to conduct the analysis? What are the assumptions for the method? Interpret your result appropriately.

\textbf{Problem 6}

To evaluate the analgesic effectiveness of a daily dose of oral methadone in patients with chronic neuropathic pain syndromes. The researchers used a visual analog scale (0--100 mm, a higher number indicates higher pain) ratings for maximum pain intensity over the course of the day. Each subject took either 20 mg of methadone or a placebo each day for 5 days. Subjects did not know which treatment they were taking. The following table gives the mean maximum pain intensity scores for the 5 days on methadone and the 5 days on placebo.

\begin{verbatim}
subject ID:    1    2    3     4    5    6   7    8   9    10   11
methadone:   29.8 73.0 98.6 58.8 60.6 57.2 57.2 89.2 97.0 49.8 37.0
placebo:     57.2 69.8 98.2 62.4 67.2 70.6 67.8 95.6 98.4 63.2 63.6
\end{verbatim}

Do these data provide sufficient evidence, at the .05 level of significance, to indicate that, in general, the maximum pain intensity is lower on days when methadone is taken? Perform a formal inferential procedure and interpret the result.

\textbf{Problem 7}

A study was conducted on genetic and environmental influences on cholesterol levels. The data set used for the study was obtained from a twin registry in Sweden. Specifically, four populations of adult twins were studied: (1) monozygotic (MZ) twins reared apart, (2) MZ twins reared together, (3) dizygotic (DZ) twins reared apart, and (4) DZ twins reared together. One issue is whether it is necessary to correct \textbf{potential differences} in sex before performing more complex genetic analyses. The data in the following table were presented for total cholesterol levels for MZ twins reared apart, by sex.

\begin{verbatim}
           Men      Women
Mean:     253.3     271.0
sd:        44.1      44.1
size(n):   44        48
\end{verbatim}

If we assume (a) serum cholesterol is normally distributed, (b) the samples are independent, and (c) the standard deviations for men and women are the same.

Using a two-sided test. State the hypotheses being tested, and implement the method. Report a p-value and interpret the result.

\hypertarget{analysis-of-variance-anova}{%
\chapter{Analysis of Variance (ANOVA)}\label{analysis-of-variance-anova}}

In the previous module, we introduced the test on equality of two population variances. A natural question is how to compare three, four, and more population means.

\hypertarget{the-question-of-one-way-anova}{%
\section{The Question of One-way ANOVA}\label{the-question-of-one-way-anova}}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img08/w08-mytilus_edulis} 

}

\caption{ Mussule Mytilus trossulus}\label{fig:unnamed-chunk-126}
\end{figure}

Consider here are some data on a shell measurement in the mussel Mytilus trossulus from five locations: Tillamook, Oregon; Newport, Oregon; Petersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, taken from a much larger data set.

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Tillamook & Newport & Petersburg & Magadan & Tvarminne \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
(\(\mu_1\)) & (\(\mu_2\)) & (\(\mu_3\)) & (\(\mu_4\)) & (\(\mu_5\)) \\
0.0571 & 0.0873 & 0.0974 & 0.1033 & 0.0703 \\
0.0813 & 0.0662 & 0.1352 & 0.0915 & 0.1026 \\
0.0831 & 0.0672 & 0.0817 & 0.0781 & 0.0956 \\
0.0976 & 0.0819 & 0.1016 & 0.0685 & 0.0973 \\
0.0817 & 0.0749 & 0.0968 & 0.0677 & 0.1039 \\
0.0859 & 0.0649 & 0.1064 & 0.0697 & 0.1045 \\
0.0735 & 0.0835 & 0.105 & 0.0764 & \\
0.0659 & 0.0725 & & 0.0689 & \\
0.0923 & & & & \\
0.0836 & & & & \\
\end{longtable}

The statistical \textbf{null hypothesis} is that the mean lengths of mussel shell \emph{are the same across the five locations}. That is, the null hypothesis has the following form.

\(H_0:\) \(\mu_1 = \mu_2 = \mu_3 = \mu_4 = \mu5.\)

The \textbf{alternative hypothesis} is that the mean lengths of the mussel shells of the five locations are \emph{not all equal}.

\(H_a:\) at least one of the means is different from the other.

\hypertarget{what-is-anova}{%
\subsection{What is ANOVA}\label{what-is-anova}}

ANOVA is a statistical technique that assesses potential differences in a continuous dependent variable by a categorical variable having 2 or more categories. For example, an ANOVA can examine potential differences in the starting salaries of undergraduate students majoring in Biology, Mathematics, and Psychology. We have learned the two-sample t-test to compare the difference between the two population means. To compare three or more population means, we need a new procedure - analysis of variance (ANOVA).

The use of ANOVA depends on the research designs (see the class note of week \#2). One-way and k-way ANOVAs are commonly used in practice.

\begin{itemize}
\item
  \textbf{One-way ANOVA} involves a single factor variable that has several categories. The one-way ANOVA addresses whether the means across categories are equal. If the means are not equal, the ANOVA does not tell which specific difference. Separate procedures are needed to perform pair-wise comparisons to detect the specific difference.
\item
  \textbf{Two-way ANOVA} involves two-factor variables. Each factor variable has several categories. The null hypotheses could be different from situation to situation. This is not the main to cover in this module. We will use multiple linear regression approaches to this problem.
\end{itemize}

\hypertarget{assumptions-of-anova}{%
\subsection{Assumptions of ANOVA}\label{assumptions-of-anova}}

There are two basic assumptions for the \textbf{classical ANOVA}.

\begin{itemize}
\item
  The response variable has a normal distribution with potentially different means at different factor levels.
\item
  The variance of the response variable has constant variance (the variances of different categories are equal).
\end{itemize}

The ANOVA procedures are sensitive to the normality assumption of the response variable. In practice, we need to perform diagnostic analysis and make sure the assumptions are satisfied. If the assumptions are violated, the resulting p-values might not be correct.

If the constant variance assumption is not satisfied, the distribution of the variance ratio in the ANOVA table is NOT the specified F distribution. One way to handle multiple comparisons with this unequal variance data is the well-known Welch ANOVA. We will use it in the data analysis of the case study.

\hypertarget{steps-of-anova}{%
\section{Steps of ANOVA}\label{steps-of-anova}}

In this section, we outline the steps for the analysis of variance.

\hypertarget{anova-tables}{%
\subsection{ANOVA Tables}\label{anova-tables}}

All computer programs that perform ANOVA generate the following ANOVA table.

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Source & SS & DF & MS & F \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Between & \(SS_B\) & \(K-1\) & \(MS_B = SS_B/(K-1)\) & \(F=MS_B/MSE_W\) \\
Within & \(SS_W\) & \(N-K\) & \(MS_W = SS_W/(N-K)\) & \\
Total & \(SS_T\) & \(N-1\) & & \\
\end{longtable}

I will use the mussel length data as an example to calculate the quantities in the above table.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2400}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Tillamook
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Newport
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Petersburg
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Magadan
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Tvarminne
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
(\(\mu_1\)) & (\(\mu_2\)) & (\(\mu_3\)) & (\(\mu_4\)) & (\(\mu_5\)) \\
0.0571 & 0.0873 & 0.0974 & 0.1033 & 0.0703 \\
0.0813 & 0.0662 & 0.1352 & 0.0915 & 0.1026 \\
0.0831 & 0.0672 & 0.0817 & 0.0781 & 0.0956 \\
0.0976 & 0.0819 & 0.1016 & 0.0685 & 0.0973 \\
0.0817 & 0.0749 & 0.0968 & 0.0677 & 0.1039 \\
0.0859 & 0.0649 & 0.1064 & 0.0697 & 0.1045 \\
0.0735 & 0.0835 & 0.105 & 0.0764 & \\
0.0659 & 0.0725 & & 0.0689 & \\
0.0923 & & & & \\
0.0836 & & & & \\
\(n_1, \bar{x}_1, s_1\) & \(n_2, \bar{x}_2, s_2\) & \(n_3, \bar{x}_3, s_3\) & \(n_4, \bar{x}_4, s_4\) & \(n_5, \bar{x}_5, s_5\) \\
\end{longtable}

In the last row of the table, we calculated the sample mean (\(\displaystyle\bar{x}_i\)), sample standard deviation (\(\displaystyle s_i\)), and sample size (\(\displaystyle n_i\)) of each of the \(K=5\) locations. Let \(\displaystyle \bar{x}\) is the mean of the combined sample mean and \(N=n_1 + n_2 + n_3 + n_4 + n_5.\)

\begin{itemize}
\item
  The sum of squared deviations between groups.
  \[\displaystyle SS_{B} = \sum n_i(\bar{x}_i-\bar{x})^2.\]
\item
  The sum of squared deviation within groups
  \[\displaystyle SS_{W} = \sum (n_i-1)s_i^2.\]
\item
  The sum of the total error
  \[\displaystyle SS_T = SS_B + SS_W.\]
\end{itemize}

\textbf{Important Result}: If the ANOVA assumptions are satisfied, \(F \approx F_{K-1, N-K}\).

If the p-value of the \textbf{F test} is less than the given significance level, we reject the null hypothesis.

\(H_0:\) \(\mu_1 = \mu_2 = \mu_3 = \mu_4 = \mu5.\)

This implies that at least one of the means is different from the other.

\hypertarget{post-hoc-pairwise-comparison}{%
\subsection{Post hoc Pairwise Comparison}\label{post-hoc-pairwise-comparison}}

If the null hypothesis of the ANOVA is rejected, we need to perform a pairwise comparison the identify the significant difference between the means. There are different types of comparisons: pairwise comparison and various simultaneous comparison procedures are available in R.

\hypertarget{how-to-summarize-anova-results}{%
\subsection{How to Summarize ANOVA Results}\label{how-to-summarize-anova-results}}

\begin{itemize}
\item
  \textbf{Describe the test type you used and the purpose of the test} - An ANOVA is appropriate for multiple test subjects. In the above example, the measurements of the same shell were taken from 5 locations.
\item
  \textbf{Write whether or not a significant difference existed between the means of each test group}. Write ``There was'' or ``There was not a significant effect of the factor.''
\item
  \textbf{Write the results of the F test}. Write the F test, followed by a parenthesis, then the two sets of degrees of freedom values separated by a comma, followed by an equal sign and the F value. The statement is something like: ``F (two sets of degrees of freedom) = F value, p = p-value.''
\item
  \textbf{Write ``Post hoc test comparison'' if a significant result is present}. Write the post hoc test you used.
\item
  \textbf{Recap the results in an easy-to-understand sentence or two}. We could write ``A significant difference existed between locations.
\end{itemize}

\hypertarget{welchs-anova}{%
\section{Welch's ANOVA}\label{welchs-anova}}

The classical ANOVA assumes that the population is normal and variance is constant (i.e., group variances are equal). This is the base to define the F test. If the assumptions are violated, the variance ratio (F test statistic) does not follow an F distribution.

One method commonly used in practice is the Welch ANOVA in which the degrees of freedom are modified to approximate the F distribution. The ``Games-Howell'' test is commonly used for the post-hoc multiple comparisons for the Welch ANOVA. The Games-Howell post-hoc test is another nonparametric approach to compare combinations of groups or treatments.

In R, \texttt{oneway.test()} in \textbf{stats} performs Welch ANOVA. It is routine to use both classical ANOVA and Welch's ANOVA. If there are no violations of the model, both ANOVAs yield similar results. If the two results are different, Welch ANOVA is more appropriate. The classical ANOVA is easy to interpret and understandable. If there are no violations, we always report the classical ANOVA.

\hypertarget{case-study-mussel-length-example---solution}{%
\section{Case Study: Mussel Length Example - Solution}\label{case-study-mussel-length-example---solution}}

We will use R to conduct ANOVA and post hoc comparison on the mussel length example.

\hypertarget{creating-an-anova-table}{%
\subsection{Creating An ANOVA Table}\label{creating-an-anova-table}}

Next, we create an R data set (data frame) based on the given data table using the following R code to perform the ANOVA procedure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0571}\NormalTok{,}\FloatTok{0.0813}\NormalTok{, }\FloatTok{0.0831}\NormalTok{, }\FloatTok{0.0976}\NormalTok{, }\FloatTok{0.0817}\NormalTok{, }\FloatTok{0.0859}\NormalTok{, }\FloatTok{0.0735}\NormalTok{, }\FloatTok{0.0659}\NormalTok{, }\FloatTok{0.0923}\NormalTok{, }\FloatTok{0.0836}\NormalTok{) }
\NormalTok{x2 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0873}\NormalTok{,}\FloatTok{0.0662}\NormalTok{, }\FloatTok{0.0672}\NormalTok{, }\FloatTok{0.0819}\NormalTok{, }\FloatTok{0.0749}\NormalTok{, }\FloatTok{0.0649}\NormalTok{, }\FloatTok{0.0835}\NormalTok{, }\FloatTok{0.0725}\NormalTok{)}
\NormalTok{x3 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0974}\NormalTok{,}\FloatTok{0.1352}\NormalTok{, }\FloatTok{0.0817}\NormalTok{, }\FloatTok{0.1016}\NormalTok{, }\FloatTok{0.0968}\NormalTok{, }\FloatTok{0.1064}\NormalTok{, }\FloatTok{0.1050}\NormalTok{)}
\NormalTok{x4 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1033}\NormalTok{,}\FloatTok{0.0915}\NormalTok{, }\FloatTok{0.0781}\NormalTok{, }\FloatTok{0.0685}\NormalTok{, }\FloatTok{0.0677}\NormalTok{, }\FloatTok{0.0697}\NormalTok{, }\FloatTok{0.0764}\NormalTok{, }\FloatTok{0.0689}\NormalTok{)}
\NormalTok{x5 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0703}\NormalTok{,}\FloatTok{0.1026}\NormalTok{, }\FloatTok{0.0956}\NormalTok{, }\FloatTok{0.0973}\NormalTok{, }\FloatTok{0.1039}\NormalTok{, }\FloatTok{0.1045}\NormalTok{)}
\NormalTok{mussel.len }\OtherTok{=} \FunctionTok{c}\NormalTok{(x1, x2, x3, x4, x5)      }\CommentTok{\# pool all sub{-}samples of lengths}
\NormalTok{location }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Tillamook"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x1)), }
             \FunctionTok{rep}\NormalTok{(}\StringTok{"Newport"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x2)),}
             \FunctionTok{rep}\NormalTok{(}\StringTok{"Petersburg"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x3)),}
             \FunctionTok{rep}\NormalTok{(}\StringTok{"Magadan"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x4)),}
             \FunctionTok{rep}\NormalTok{(}\StringTok{"Tvarminne"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x5)))  }\CommentTok{\# location vector matches the lengths}
\NormalTok{data.matrix }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{len =}\NormalTok{ mussel.len, }\AttributeTok{location =}\NormalTok{ location)   }\CommentTok{\# data a data table}
\NormalTok{musseldata }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(data.matrix)        }\CommentTok{\# data frame}
\NormalTok{model01 }\OtherTok{=} \FunctionTok{lm}\NormalTok{(len }\SpecialCharTok{\textasciitilde{}}\NormalTok{ location, }\AttributeTok{data =}\NormalTok{ musseldata)  }\CommentTok{\# a model for extracting ANOVA}
\NormalTok{anova.model }\OtherTok{=} \FunctionTok{anova}\NormalTok{(model01)   }\CommentTok{\# creating the ANOVA table}
\FunctionTok{kable}\NormalTok{(anova.model, }\AttributeTok{caption =} \StringTok{"Analysis of variance table"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-127}Analysis of variance table}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
  & Df & Sum Sq & Mean Sq & F value & Pr(>F)\\
\hline
location & 4 & 0.0045197 & 0.0011299 & 7.12102 & 0.0002812\\
\hline
Residuals & 34 & 0.0053949 & 0.0001587 & NA & NA\\
\hline
\end{tabular}
\end{table}

The ANOVA test indicates that not all means are equal (\(F(4,34) = 7.12, p = 0.00028\)). The follow-up pair-wise comparisons of the group means are given in the next subsection.

\hypertarget{diagnostic-analysis}{%
\subsection{Diagnostic Analysis}\label{diagnostic-analysis}}

Since the ANOVA is sensitive to the normality assumption of the population and the constant variance. We need to check whether there are violations of the assumption. The following diagnostic plots are used to check the appropriateness of the ANOVA test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aov.model }\OtherTok{=} \FunctionTok{aov}\NormalTok{(len}\SpecialCharTok{\textasciitilde{}}\NormalTok{location, }\AttributeTok{data=}\NormalTok{musseldata)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(aov.model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-128-1} \end{center}

\textbf{Constant variance}: If all points on the top-left graph are evenly spread around the horizontal axis (the red curve overlaps with the horizontal axis), then the constant variance assumption is satisfied.

\textbf{Normal population}: If all points on the top right graph are all close to the off-diagonal line, then the normality assumption is satisfied.

The above plot showed that there are no significant violations of the assumptions of the ANOVA test. Next, we perform pair-wise comparisons.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{welch }\OtherTok{=} \FunctionTok{oneway.test}\NormalTok{(mussel.len }\SpecialCharTok{\textasciitilde{}}\NormalTok{ location)}
\NormalTok{F.stats }\OtherTok{=} \FunctionTok{as.vector}\NormalTok{(welch}\SpecialCharTok{$}\NormalTok{statistic)}
\NormalTok{num.df }\OtherTok{=} \FunctionTok{as.vector}\NormalTok{(welch}\SpecialCharTok{$}\NormalTok{parameter[}\DecValTok{1}\NormalTok{])}
\NormalTok{denom.df }\OtherTok{=} \FunctionTok{as.vector}\NormalTok{(welch}\SpecialCharTok{$}\NormalTok{parameter[}\DecValTok{2}\NormalTok{])}
\NormalTok{p.value }\OtherTok{=} \FunctionTok{as.vector}\NormalTok{(welch}\SpecialCharTok{$}\NormalTok{p.value)}
\NormalTok{cap.text }\OtherTok{=}\NormalTok{ welch}\SpecialCharTok{$}\NormalTok{method}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{F.stats =}\NormalTok{ F.stats, }\AttributeTok{num.df =}\NormalTok{ num.df,}
            \AttributeTok{denom.df=}\NormalTok{denom.df, }\AttributeTok{p.value =}\NormalTok{ p.value), }\AttributeTok{caption =}\NormalTok{ cap.text)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-129}One-way analysis of means (not assuming equal variances)}
\centering
\begin{tabular}[t]{r|r|r|r}
\hline
F.stats & num.df & denom.df & p.value\\
\hline
5.664479 & 4 & 15.69546 & 0.0050796\\
\hline
\end{tabular}
\end{table}

\hypertarget{welch-anova}{%
\subsection{Welch ANOVA}\label{welch-anova}}

Although no significant violations were observed in the above residual plots, we still perform the Welch ANOVA for illustrative purposes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/ref/anova{-}posthoc{-}test.txt"}\NormalTok{)}
\CommentTok{\# posthoc.tgh \textless{}{-} function(y, x, method=c("games{-}howell", "tukey"), digits=2)}
\NormalTok{posthoc }\OtherTok{=} \FunctionTok{posthoc.tgh}\NormalTok{(mussel.len, location, }\AttributeTok{method=}\StringTok{"tukey"}\NormalTok{,}\AttributeTok{digits=}\DecValTok{2}\NormalTok{ )}
\NormalTok{descriptives.stats }\OtherTok{=}\NormalTok{ posthoc}\SpecialCharTok{$}\NormalTok{intermediate}\SpecialCharTok{$}\NormalTok{descriptives}
\FunctionTok{kable}\NormalTok{(descriptives.stats, }\AttributeTok{caption =} \StringTok{"Descriptive statistics of sub{-}populations"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-130}Descriptive statistics of sub-populations}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
  & n & means & variances\\
\hline
Magadan & 8 & 0.0780125 & 0.0001676\\
\hline
Newport & 8 & 0.0748000 & 0.0000739\\
\hline
Petersburg & 7 & 0.1034429 & 0.0002627\\
\hline
Tillamook & 10 & 0.0802000 & 0.0001431\\
\hline
Tvarminne & 6 & 0.0957000 & 0.0001680\\
\hline
\end{tabular}
\end{table}

We can see that both classical and Welch ANOVA yield the same result (both p-values are \textless{} 0.01). Therefore, we report the results of classical ANOVA and Tukey's HSD procedure.

\hypertarget{post-hoc-multiple-comparisons}{%
\subsection{Post-hoc Multiple Comparisons}\label{post-hoc-multiple-comparisons}}

Since the null hypothesis was rejected, it is necessary to quantify the differences between groups in order to determine which groups significantly differ from each other.

There are several multiple comparison tests with implementations in various R libraries. We will use Tukey's Honest Significant Differences (HSD). The Tukey HSD procedure will run a pairwise comparison of all possible combinations of groups and test these pairs for significant differences between their means, all while adjusting the p-value. If the assumption of constant variance is violated, the Welch ANOVA will be carried out. The Games-Howell test will be used if the null hypothesis in the Welch ANOVA is rejected.

The following R code performs the pair-wise comparisons between the group means.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aov.model }\OtherTok{=} \FunctionTok{aov}\NormalTok{(len}\SpecialCharTok{\textasciitilde{}}\NormalTok{location, }\AttributeTok{data=}\NormalTok{musseldata)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{TukeyHSD}\NormalTok{(aov.model)}\SpecialCharTok{$}\NormalTok{location, }\AttributeTok{caption =} \StringTok{"Tukey multiple comparisons of means"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-131}Tukey multiple comparisons of means}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & diff & lwr & upr & p adj\\
\hline
Newport-Magadan & -0.0032125 & -0.0213487 & 0.0149237 & 0.9857956\\
\hline
Petersburg-Magadan & 0.0254304 & 0.0066576 & 0.0442031 & 0.0036924\\
\hline
Tillamook-Magadan & 0.0021875 & -0.0150180 & 0.0193930 & 0.9959794\\
\hline
Tvarminne-Magadan & 0.0176875 & -0.0019019 & 0.0372769 & 0.0928839\\
\hline
Petersburg-Newport & 0.0286429 & 0.0098701 & 0.0474156 & 0.0009253\\
\hline
Tillamook-Newport & 0.0054000 & -0.0118055 & 0.0226055 & 0.8934665\\
\hline
Tvarminne-Newport & 0.0209000 & 0.0013106 & 0.0404894 & 0.0317354\\
\hline
Tillamook-Petersburg & -0.0232429 & -0.0411181 & -0.0053676 & 0.0056547\\
\hline
Tvarminne-Petersburg & -0.0077429 & -0.0279230 & 0.0124373 & 0.8028001\\
\hline
Tvarminne-Tillamook & 0.0155000 & -0.0032310 & 0.0342310 & 0.1446987\\
\hline
\end{tabular}
\end{table}

The multiple comparisons show that four differences are significant at level 0.05. These differences will be reported in the summary.

\textbf{For illustrative purposes}, we carry the Games-Howell test for multiple comparisons in the following.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Games.howell }\OtherTok{=}\NormalTok{ posthoc}\SpecialCharTok{$}\NormalTok{output}\SpecialCharTok{$}\NormalTok{games.howell}
\FunctionTok{kable}\NormalTok{(Games.howell, }\AttributeTok{caption=}\StringTok{"Games{-}Howell multiple comparisons"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-132}Games-Howell multiple comparisons}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
  & t & df & p\\
\hline
Magadan:Newport & 0.5847249 & 12.169510 & 0.9748911\\
\hline
Magadan:Petersburg & 3.3254180 & 11.496217 & 0.0412241\\
\hline
Magadan:Tillamook & 0.3684022 & 14.550533 & 0.9956174\\
\hline
Magadan:Tvarminne & 2.5281745 & 10.915427 & 0.1539440\\
\hline
Newport:Petersburg & 4.1880670 & 8.857240 & 0.0156494\\
\hline
Newport:Tillamook & 1.1127299 & 15.868239 & 0.7976041\\
\hline
Newport:Tvarminne & 3.4248678 & 8.205773 & 0.0506123\\
\hline
Petersburg:Tillamook & 3.2279514 & 10.436333 & 0.0530007\\
\hline
Petersburg:Tvarminne & 0.9564490 & 10.967062 & 0.8686901\\
\hline
Tillamook:Tvarminne & 2.3828489 & 9.970454 & 0.1972836\\
\hline
\end{tabular}
\end{table}

The results are slightly different from Tukey's HSD test. Next, we present a visual presentation before summarizing the ANOVA result and post-hoc comparisons.

\hypertarget{visual-comparison---boxplots}{%
\subsection{Visual Comparison - Boxplots}\label{visual-comparison---boxplots}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Box{-}plot of mussel length by locations}
\FunctionTok{boxplot}\NormalTok{(mussel.len }\SpecialCharTok{\textasciitilde{}}\NormalTok{ location,         }
        \AttributeTok{main=}\StringTok{"Mussel Length Data"}\NormalTok{, }\CommentTok{\# plot title}
        \AttributeTok{xlab=}\StringTok{"Mussel Length"}\NormalTok{,      }\CommentTok{\# label of x{-}axis}
        \AttributeTok{ylab=}\StringTok{"Locations"}\NormalTok{,          }\CommentTok{\# label y{-}axis}
        \AttributeTok{border =} \StringTok{"navy"}\NormalTok{,           }\CommentTok{\# border color box{-}plot}
        \AttributeTok{col=}\StringTok{"skyblue"}\NormalTok{)             }\CommentTok{\# box color}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-133-1} \end{center}

The box plot also confirms that the group means are not identical.

\hypertarget{anova-reporting}{%
\subsection{ANOVA Reporting}\label{anova-reporting}}

We conducted a one-way ANOVA test on the equality of the mean lengths of mussel shells in the five locations and found a statistical significance with F(4, 34) = 7.12, p-value = 0.00028. This implies that the means at different locations are not identical.

After we conducted pairwise comparisons of the means of the lengths of mussel shells across five locations using the HSD test, we found that, among the 10 pairwise comparisons, four of them are significant with an adjusted p-value less than 0.05: Petersburg - Magadan = 0.0254304 (p = 0.0037), Petersburg - Newport = 0.0286429 (p = 0.0009), Tvarminne - Newport = 0.0209 (p = 0.032), and Tillamook - Petersburg = -0.0232429 (p = 0.0057). The group box plot also showed the difference between the locations.

\hfill\break

\hypertarget{practice-problems-1}{%
\section{Practice Problems}\label{practice-problems-1}}

The effects of thermal pollution on Asiatic clams at three different geographical
locations were analyzed by researchers. Sample data on clamshell length, width, and
height are displayed in the following table.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img08/w08-clamshell-data-table} 

}

\caption{Clam shell data table}\label{fig:unnamed-chunk-134}
\end{figure}

The objective is to determine if there is a significant difference in mean length, height, or width (measured in mm) of the clamshell at the three different locations by performing three analyses.

To save you some time, I created three data sets for length, width, and height respectively in the following code chunk. \textbf{You only choose ONE of the three sets} to complete this week's assignment.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# length}
\NormalTok{Len.loc}\FloatTok{.1} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{7.20}\NormalTok{, }\FloatTok{7.50}\NormalTok{, }\FloatTok{6.89}\NormalTok{, }\FloatTok{6.95}\NormalTok{, }\FloatTok{6.73}\NormalTok{, }\FloatTok{7.25}\NormalTok{, }\FloatTok{7.20}\NormalTok{, }\FloatTok{6.85}\NormalTok{, }\FloatTok{7.52}\NormalTok{, }\FloatTok{7.01}\NormalTok{, }\FloatTok{6.65}\NormalTok{, }
              \FloatTok{7.55}\NormalTok{, }\FloatTok{7.14}\NormalTok{, }\FloatTok{7.45}\NormalTok{, }\FloatTok{7.24}\NormalTok{, }\FloatTok{7.75}\NormalTok{, }\FloatTok{6.85}\NormalTok{, }\FloatTok{6.50}\NormalTok{, }\FloatTok{6.64}\NormalTok{, }\FloatTok{7.19}\NormalTok{, }\FloatTok{7.15}\NormalTok{, }\FloatTok{7.21}\NormalTok{, }
              \FloatTok{7.15}\NormalTok{, }\FloatTok{7.30}\NormalTok{, }\FloatTok{6.35}\NormalTok{)}
\NormalTok{Len.loc}\FloatTok{.2} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{7.25}\NormalTok{, }\FloatTok{7.23}\NormalTok{, }\FloatTok{6.85}\NormalTok{, }\FloatTok{7.07}\NormalTok{, }\FloatTok{6.55}\NormalTok{, }\FloatTok{7.43}\NormalTok{, }\FloatTok{7.30}\NormalTok{, }\FloatTok{6.90}\NormalTok{, }\FloatTok{7.10}\NormalTok{, }\FloatTok{6.95}\NormalTok{, }\FloatTok{7.39}\NormalTok{, }
              \FloatTok{6.54}\NormalTok{, }\FloatTok{6.39}\NormalTok{, }\FloatTok{6.08}\NormalTok{, }\FloatTok{6.30}\NormalTok{, }\FloatTok{6.35}\NormalTok{, }\FloatTok{7.34}\NormalTok{, }\FloatTok{6.70}\NormalTok{, }\FloatTok{7.08}\NormalTok{, }\FloatTok{7.09}\NormalTok{, }\FloatTok{7.40}\NormalTok{, }\FloatTok{6.00}\NormalTok{, }
              \FloatTok{6.94}\NormalTok{)}
\NormalTok{Len.loc}\FloatTok{.3} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{5.95}\NormalTok{, }\FloatTok{7.60}\NormalTok{, }\FloatTok{6.15}\NormalTok{, }\FloatTok{7.00}\NormalTok{, }\FloatTok{6.81}\NormalTok{, }\FloatTok{7.10}\NormalTok{, }\FloatTok{6.85}\NormalTok{, }\FloatTok{6.68}\NormalTok{, }\FloatTok{5.51}\NormalTok{, }\FloatTok{6.85}\NormalTok{, }\FloatTok{7.10}\NormalTok{, }
              \FloatTok{6.81}\NormalTok{, }\FloatTok{7.30}\NormalTok{, }\FloatTok{7.05}\NormalTok{, }\FloatTok{6.75}\NormalTok{, }\FloatTok{6.75}\NormalTok{, }\FloatTok{7.35}\NormalTok{, }\FloatTok{6.22}\NormalTok{, }\FloatTok{6.80}\NormalTok{, }\FloatTok{6.29}\NormalTok{, }\FloatTok{7.55}\NormalTok{, }\FloatTok{7.45}\NormalTok{, }
              \FloatTok{6.70}\NormalTok{, }\FloatTok{7.51}\NormalTok{, }\FloatTok{6.95}\NormalTok{, }\FloatTok{7.50}\NormalTok{)}

\CommentTok{\# Width}
\NormalTok{Width.loc}\FloatTok{.1} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{6.10}\NormalTok{, }\FloatTok{5.90}\NormalTok{, }\FloatTok{5.45}\NormalTok{, }\FloatTok{5.76}\NormalTok{, }\FloatTok{5.36}\NormalTok{, }\FloatTok{5.84}\NormalTok{, }\FloatTok{5.83}\NormalTok{, }\FloatTok{5.75}\NormalTok{, }\FloatTok{6.27}\NormalTok{, }\FloatTok{5.65}\NormalTok{, }\FloatTok{5.55}\NormalTok{, }
                \FloatTok{6.25}\NormalTok{, }\FloatTok{5.65}\NormalTok{, }\FloatTok{6.05}\NormalTok{, }\FloatTok{5.73}\NormalTok{, }\FloatTok{6.35}\NormalTok{, }\FloatTok{6.05}\NormalTok{, }\FloatTok{5.30}\NormalTok{, }\FloatTok{5.36}\NormalTok{, }\FloatTok{5.85}\NormalTok{, }\FloatTok{6.30}\NormalTok{, }\FloatTok{6.12}\NormalTok{, }
                \FloatTok{6.20}\NormalTok{, }\FloatTok{6.15}\NormalTok{,}\FloatTok{5.25}\NormalTok{)}
\NormalTok{Width.loc}\FloatTok{.2} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{6.25}\NormalTok{, }\FloatTok{5.99}\NormalTok{, }\FloatTok{5.61}\NormalTok{, }\FloatTok{5.91}\NormalTok{, }\FloatTok{5.30}\NormalTok{, }\FloatTok{6.10}\NormalTok{, }\FloatTok{5.95}\NormalTok{, }\FloatTok{5.80}\NormalTok{, }\FloatTok{5.81}\NormalTok{, }\FloatTok{5.65}\NormalTok{, }\FloatTok{6.04}\NormalTok{, }
                \FloatTok{5.89}\NormalTok{, }\FloatTok{5.00}\NormalTok{, }\FloatTok{4.80}\NormalTok{, }\FloatTok{5.05}\NormalTok{, }\FloatTok{5.10}\NormalTok{, }\FloatTok{6.45}\NormalTok{, }\FloatTok{5.51}\NormalTok{, }\FloatTok{5.81}\NormalTok{, }\FloatTok{5.95}\NormalTok{, }\FloatTok{6.25}\NormalTok{, }\FloatTok{4.75}\NormalTok{, }
                \FloatTok{5.63}\NormalTok{)}
\NormalTok{Width.loc}\FloatTok{.3} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{4.75}\NormalTok{, }\FloatTok{6.45}\NormalTok{, }\FloatTok{5.05}\NormalTok{, }\FloatTok{5.80}\NormalTok{, }\FloatTok{5.61}\NormalTok{, }\FloatTok{5.75}\NormalTok{, }\FloatTok{5.55}\NormalTok{, }\FloatTok{5.50}\NormalTok{, }\FloatTok{4.52}\NormalTok{, }\FloatTok{5.53}\NormalTok{, }\FloatTok{5.80}\NormalTok{, }
                \FloatTok{5.45}\NormalTok{, }\FloatTok{6.00}\NormalTok{, }\FloatTok{6.25}\NormalTok{, }\FloatTok{5.65}\NormalTok{, }\FloatTok{5.57}\NormalTok{, }\FloatTok{6.21}\NormalTok{, }\FloatTok{5.11}\NormalTok{, }\FloatTok{5.81}\NormalTok{, }\FloatTok{4.95}\NormalTok{, }\FloatTok{5.93}\NormalTok{, }\FloatTok{6.19}\NormalTok{, }
                \FloatTok{5.55}\NormalTok{, }\FloatTok{6.20}\NormalTok{, }\FloatTok{5.69}\NormalTok{, }\FloatTok{6.20}\NormalTok{)}

\DocumentationTok{\#\# Height}
\NormalTok{Height.loc}\FloatTok{.1} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{4.45}\NormalTok{, }\FloatTok{4.65}\NormalTok{, }\FloatTok{4.00}\NormalTok{, }\FloatTok{4.02}\NormalTok{, }\FloatTok{3.90}\NormalTok{, }\FloatTok{4.40}\NormalTok{, }\FloatTok{4.19}\NormalTok{, }\FloatTok{3.95}\NormalTok{, }\FloatTok{4.60}\NormalTok{, }\FloatTok{4.20}\NormalTok{, }\FloatTok{4.10}\NormalTok{, }
                 \FloatTok{4.72}\NormalTok{, }\FloatTok{4.26}\NormalTok{, }\FloatTok{4.85}\NormalTok{, }\FloatTok{4.29}\NormalTok{, }\FloatTok{4.85}\NormalTok{, }\FloatTok{4.50}\NormalTok{, }\FloatTok{3.73}\NormalTok{, }\FloatTok{3.99}\NormalTok{, }\FloatTok{4.05}\NormalTok{, }\FloatTok{4.55}\NormalTok{, }\FloatTok{4.37}\NormalTok{, }
                 \FloatTok{4.36}\NormalTok{, }\FloatTok{4.65}\NormalTok{, }\FloatTok{3.75}\NormalTok{)}
\NormalTok{Height.loc}\FloatTok{.2} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{4.65}\NormalTok{, }\FloatTok{4.20}\NormalTok{, }\FloatTok{4.01}\NormalTok{, }\FloatTok{4.31}\NormalTok{, }\FloatTok{3.95}\NormalTok{, }\FloatTok{4.60}\NormalTok{, }\FloatTok{4.29}\NormalTok{, }\FloatTok{4.33}\NormalTok{, }\FloatTok{4.26}\NormalTok{, }\FloatTok{4.31}\NormalTok{, }\FloatTok{4.50}\NormalTok{, }
                 \FloatTok{3.65}\NormalTok{, }\FloatTok{3.72}\NormalTok{, }\FloatTok{3.51}\NormalTok{, }\FloatTok{3.69}\NormalTok{, }\FloatTok{3.73}\NormalTok{, }\FloatTok{4.55}\NormalTok{, }\FloatTok{3.89}\NormalTok{, }\FloatTok{4.34}\NormalTok{, }\FloatTok{4.39}\NormalTok{, }\FloatTok{4.85}\NormalTok{, }\FloatTok{3.37}\NormalTok{, }
                 \FloatTok{4.09}\NormalTok{)}
\NormalTok{Height.loc}\FloatTok{.3} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{3.20}\NormalTok{, }\FloatTok{4.56}\NormalTok{, }\FloatTok{3.50}\NormalTok{, }\FloatTok{4.30}\NormalTok{, }\FloatTok{4.22}\NormalTok{, }\FloatTok{4.10}\NormalTok{, }\FloatTok{3.89}\NormalTok{, }\FloatTok{3.90}\NormalTok{, }\FloatTok{2.70}\NormalTok{, }\FloatTok{4.00}\NormalTok{, }\FloatTok{4.45}\NormalTok{, }
                 \FloatTok{3.51}\NormalTok{, }\FloatTok{4.31}\NormalTok{, }\FloatTok{4.71}\NormalTok{, }\FloatTok{4.00}\NormalTok{, }\FloatTok{4.06}\NormalTok{, }\FloatTok{4.29}\NormalTok{, }\FloatTok{3.35}\NormalTok{, }\FloatTok{4.50}\NormalTok{, }\FloatTok{3.69}\NormalTok{, }\FloatTok{4.55}\NormalTok{, }\FloatTok{4.70}\NormalTok{, }
                 \FloatTok{4.00}\NormalTok{, }\FloatTok{4.74}\NormalTok{, }\FloatTok{4.29}\NormalTok{, }\FloatTok{4.65}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The following are the steps for completing the assignment:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Pick \textbf{ONE} of the three data sets (length, width, and height) from the above code chunk.
\item
  Modify the code in 4.1 to create an R data set and perform the classical ANOVA analysis. Interpret the ANOVA results as I did in the class note.
\item
  Perform a diagnostic analysis using the residual plot as shown in Section 4.2. Explain whether you observe any violations of the model assumptions.
\item
  If the null hypothesis is rejected, carry out multiple comparisons between the three locations using either Tukey's HSD or Games-Howell procedures.
\item
  Report the ANOVA analysis results similar to what was reported in the note.
\end{enumerate}

\hypertarget{correlation-and-simple-linear-regression}{%
\chapter{Correlation and Simple Linear Regression}\label{correlation-and-simple-linear-regression}}

In the previous module, we discussed the relationship between a continuous variable (the length of mussel shells) and a categorical variable (location, also called factor variable). If there is no association between the two variables, the means of all populations are equal. If there is an association between the continuous variable and the factor variable, then the means of the populations are not identical.

The continuous variable is assumed to normal distribution. The continuous variable is also called the response variable (dependent variable) and the factor variable is called the predictor variable (or explanatory variable).

A natural question is how to characterize the relationship between two continuous variables.

\hypertarget{the-question-and-the-data}{%
\section{The question and the data}\label{the-question-and-the-data}}

\textbf{Example}: Amyotrophic lateral sclerosis (ALS) is characterized by a progressive decline of motor function. The degenerative process affects the respiratory system. To investigate the longitudinal impact of nocturnal noninvasive positive-pressure ventilation on patients with ALS. Prior to treatment, they measured the partial pressure of arterial oxygen (Pao2) and partial pressure of arterial carbon dioxide (Paco2) in patients with the disease. The results were as follows:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-als} 

}

\caption{ Length of clamshells}\label{fig:unnamed-chunk-137}
\end{figure}

Source: M. Butz, K. H.Wollinsky, U.Widemuth-Catrinescu, A. Sperfeld, S. Winter, H. H. Mehrkens, A. C. Ludolph, and H. Schreiber, ``Longitudinal Effects of Noninvasive Positive-Pressure Ventilation in Patients with Amyotrophic Lateral Sclerosis,'' \emph{American Journal of Medical Rehabilitation}, 82 (2003) 597--604.

The layout of the data table is shown below. Each subject (patient ID) has one \textbf{record} with two pieces of information Paco2 and Pao2.

\hypertarget{visual-inspection-for-association}{%
\section{Visual Inspection for Association}\label{visual-inspection-for-association}}

We define two vectors to store the sample values of the partial pressure of arterial oxygen (Pao2) and the partial pressure of arterial carbon dioxide (Paco2). Before conducting the analysis, we make a scatter plot to visualize the relationship between the two continuous variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# define the data sets based on the given data table.}
\NormalTok{Paco2 }\OtherTok{=}\FunctionTok{c}\NormalTok{(}\FloatTok{40.0}\NormalTok{, }\FloatTok{47.0}\NormalTok{, }\FloatTok{34.0}\NormalTok{, }\FloatTok{42.0}\NormalTok{, }\FloatTok{54.0}\NormalTok{, }\FloatTok{48.0}\NormalTok{, }\FloatTok{53.6}\NormalTok{, }\FloatTok{56.9}\NormalTok{, }\FloatTok{58.0}\NormalTok{, }\FloatTok{45.0}\NormalTok{, }\FloatTok{54.5}\NormalTok{, }\FloatTok{54.0}\NormalTok{, }
         \FloatTok{43.0}\NormalTok{, }\FloatTok{44.3}\NormalTok{, }\FloatTok{53.9}\NormalTok{, }\FloatTok{41.8}\NormalTok{, }\FloatTok{33.0}\NormalTok{, }\FloatTok{43.1}\NormalTok{, }\FloatTok{52.4}\NormalTok{, }\FloatTok{37.9}\NormalTok{, }\FloatTok{34.5}\NormalTok{, }\FloatTok{40.1}\NormalTok{, }\FloatTok{33.0}\NormalTok{, }\FloatTok{59.9}\NormalTok{, }
         \FloatTok{62.6}\NormalTok{, }\FloatTok{54.1}\NormalTok{, }\FloatTok{45.7}\NormalTok{, }\FloatTok{40.6}\NormalTok{, }\FloatTok{56.6}\NormalTok{, }\FloatTok{59.0}\NormalTok{)}
\NormalTok{Pao2 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{101.0}\NormalTok{, }\FloatTok{69.0}\NormalTok{, }\FloatTok{132.0}\NormalTok{, }\FloatTok{65.0}\NormalTok{, }\FloatTok{72.0}\NormalTok{, }\FloatTok{76.0}\NormalTok{, }\FloatTok{67.2}\NormalTok{, }\FloatTok{70.9}\NormalTok{, }\FloatTok{73.0}\NormalTok{, }\FloatTok{66.0}\NormalTok{, }\FloatTok{80.0}\NormalTok{, }
         \FloatTok{72.0}\NormalTok{, }\FloatTok{105.0}\NormalTok{, }\FloatTok{113.0}\NormalTok{, }\FloatTok{69.2}\NormalTok{, }\FloatTok{66.7}\NormalTok{, }\FloatTok{67.0}\NormalTok{, }\FloatTok{77.5}\NormalTok{, }\FloatTok{65.1}\NormalTok{, }\FloatTok{71.0}\NormalTok{, }\FloatTok{86.5}\NormalTok{, }\FloatTok{74.7}\NormalTok{, }
         \FloatTok{94.0}\NormalTok{, }\FloatTok{60.4}\NormalTok{, }\FloatTok{52.5}\NormalTok{, }\FloatTok{76.9}\NormalTok{, }\FloatTok{65.3}\NormalTok{, }\FloatTok{80.3}\NormalTok{, }\FloatTok{53.2}\NormalTok{, }\FloatTok{71.9}\NormalTok{)}
\DocumentationTok{\#\# scatter plot}
\FunctionTok{plot}\NormalTok{(Paco2, Pao2, }
     \AttributeTok{pch =} \DecValTok{20}\NormalTok{,}
     \AttributeTok{col =} \StringTok{"navy"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Relationship between Paco2 and Pao2"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Paco2"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Pao2"}
\NormalTok{)}
\DocumentationTok{\#\# The following two lines are not required when you make this scatter plot}
\FunctionTok{segments}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{65}\NormalTok{, }\DecValTok{55}\NormalTok{, }\DecValTok{50}\NormalTok{, }\AttributeTok{lty =} \DecValTok{3}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\FunctionTok{segments}\NormalTok{(}\DecValTok{33}\NormalTok{, }\DecValTok{140}\NormalTok{, }\DecValTok{70}\NormalTok{, }\DecValTok{60}\NormalTok{, }\AttributeTok{lty =} \DecValTok{3}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Pao2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Paco2), }\AttributeTok{col =} \StringTok{"purple"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-138-1} \end{center}

We can see from the above scatter plot that there is a negative association between Paco2 and Pao2 since Pao2 decreases as Paco2 increases. We can also see that \textbf{the variance} of Pao2 is also decreasing as Paco2 increases.

How to quantify the above association?

\hypertarget{coefficient-of-correlation}{%
\section{Coefficient of Correlation}\label{coefficient-of-correlation}}

The strength of linear correlation can be measured by the linear correlation coefficient. We will introduce one such measure - the Pearson correlation coefficient.

\hypertarget{definition-of-pearson-correlation-coefficient}{%
\subsection{Definition of Pearson correlation coefficient}\label{definition-of-pearson-correlation-coefficient}}

One well-known quantity for measuring the \textbf{linear association} between two numerical variables is the Pearson correlation coefficient. The sample Pearson correlation coefficient is defined as

\[
r=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}
\]

\hypertarget{interpretation-of-correlation-coefficient}{%
\subsection{Interpretation of correlation coefficient}\label{interpretation-of-correlation-coefficient}}

The interpretation of the Pearson correlation coefficient is customarily given in the following

\begin{itemize}
\item
  if \(r > 0\), then \(x\) and \(y\) are positively correlated; if \(r < 0\), then \(x\) and \(y\) are negatively correlated;
\item
  if \(r = 0\), there is \textbf{no} linear correlation between \(x\) and \(y\).
\item
  if \(|r| < 0.3\), there is a \textbf{weak} linear correlation between \(x\) and \(y\).
\item
  if \(0.3< |r| < 0.7\), there is a \textbf{moderate} linear correlation between \(x\) and \(y\).
\item
  if \(0.7 < |r| < 1.0\), there is a \textbf{strong} linear correlation between \(x\) and \(y\).
\item
  if \(r = 1\), there is a \textbf{perfect} linear correlation between \(x\) and \(y\).
\end{itemize}

In R, we use the command \textbf{cor()} to calculate the above Pearson correlation coefficient.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Pearson.correlation }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{r=}\FunctionTok{cor}\NormalTok{(Paco2, Pao2))}
\FunctionTok{kable}\NormalTok{(Pearson.correlation, }\AttributeTok{caption =} \StringTok{"Pearson correlation coefficient"}\NormalTok{,}
      \AttributeTok{align=}\StringTok{"c"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-139}Pearson correlation coefficient}
\centering
\begin{tabular}[t]{c}
\hline
r\\
\hline
-0.5307874\\
\hline
\end{tabular}
\end{table}

Therefore, there is a weak negative linear correlation between the partial pressure of arterial oxygen (Pao2) and the partial pressure of arterial carbon dioxide (Paco2).

\hypertarget{least-square-regression-structure-diagnostics-and-applications}{%
\section{Least square regression: structure, diagnostics, and applications}\label{least-square-regression-structure-diagnostics-and-applications}}

For illustration purposes, we make the following plot based on an artificial data set to introduce several important concepts of the least square regression model.

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-140-1} \end{center}

\hypertarget{definitions}{%
\subsection{Definitions}\label{definitions}}

The following concepts are annotated in the above figure.

\begin{itemize}
\item
  The variable associated with the vertical variable is called the \textbf{response} variable. The *response** variable is always placed on the left-hand side of the model formula.
\item
  The variable that impacts the value of the response variable is called the explanatory variable (also called predictor, independent variables).
\item
  The points in the figure plotted based on the data set are called observed data points.
\item
  The line in the figure is called the \textbf{fitted regression line}.
\item
  The points on the fitted regression line are called \textbf{fitted data points}.
\item
  The difference between coordinates observed and fitted points is called ** the residual** of the observed data point. The residual is, in fact, called the fitted error. It reflects the goodness of the fitted regression line. \(e_i\) \((i = 1 ,2 ,\cdots, n)\) are the estimated residual errors.
\item
  The \textbf{best} regression line is obtained by minimizing the sum of the squared errors, \(e_1^2 +e_2^2+\cdots + e_2^2\). This is the reason why we call the \textbf{best} regression line the \textbf{least square regression line} .
\item
  The intercept and slope completely determine a straight line. The intercept and slope of the \textbf{least square} regression line are obtained by minimizing the sum of the squared residual errors.
\item
  The statistical term \emph{linear model} is the equation of the fitted regression line.
\item
  There are two possible applications of a linear regression model.

  \begin{itemize}
  \item
    \textbf{Association analysis} - basic describes how the change of explanatory variable impacts the value of the response variable.
  \item
    \textbf{Prediction analysis} - predict the values of the response variable based on the corresponding \textbf{new values} of the explanatory variable.
  \end{itemize}
\end{itemize}

In this module, we only discuss the least square regression with ONE explanatory variable. In the next module, we will generalize this model to multiple explanatory variables.

\hypertarget{assumptions-of-least-square-regression}{%
\subsection{Assumptions of Least Square Regression}\label{assumptions-of-least-square-regression}}

The assumptions of the least square linear regression are identical to the ones of the ANOVA.

\begin{itemize}
\item
  The response variable is a \textbf{normal random variable}. Its mean is dependent on the \textbf{non-random} explanatory variable.
\item
  The variance of the response variable is constant (i.e., its variance is NOT dependent on the \textbf{non-random} explanatory variable).
\item
  The relationship between the response and explanatory variables is assumed to be correctly specified.
\end{itemize}

\hypertarget{model-building-and-diagnostics}{%
\subsection{Model Building and Diagnostics}\label{model-building-and-diagnostics}}

We will use \textbf{lm()} and the artificial data used in the above plot to find the least square estimate of \textbf{parameters}: intercept and slope.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{height }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{176}\NormalTok{, }\DecValTok{154}\NormalTok{, }\DecValTok{138}\NormalTok{, }\DecValTok{132}\NormalTok{, }\DecValTok{176}\NormalTok{, }\DecValTok{181}\NormalTok{, }\DecValTok{150}\NormalTok{)}
\NormalTok{bodymass }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{82}\NormalTok{, }\DecValTok{49}\NormalTok{, }\DecValTok{53}\NormalTok{, }\DecValTok{47}\NormalTok{, }\DecValTok{69}\NormalTok{, }\DecValTok{77}\NormalTok{, }\DecValTok{62}\NormalTok{)}
\NormalTok{ls.reg }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(height }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bodymass)}
\NormalTok{parameter.estimates }\OtherTok{\textless{}{-}}\NormalTok{ ls.reg}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{kable}\NormalTok{(parameter.estimates, }
      \AttributeTok{caption =} \StringTok{"Least square estimate of the intercept and slope"}\NormalTok{, }
      \AttributeTok{align=}\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-141}Least square estimate of the intercept and slope}
\centering
\begin{tabular}[t]{l|c}
\hline
  & x\\
\hline
(Intercept) & 78.627588\\
\hline
bodymass & 1.267897\\
\hline
\end{tabular}
\end{table}

The least-square estimated intercept and slope are approximately equal to 78.63 and 1.27. Therefore, the fitted least square regression line is \(\widehat{height}_i = 78.63 + 1.27\times bodymass_i\). The residual error \(e_i=height_i - \widehat{height}_i\), for \(i=1, 2, \cdots, n.\)

Since the explanatory variable is implicitly assumed to be a non-random variable, we can see the relationship between the response variable and the residual in the following general representation.

\[
response.variable = \alpha + \beta \times predictor.variable + \epsilon
\]

The assumption that the response variable is normal with a constant variance is equivalent to that \(\epsilon\) is a normal random variable with mean 0 and constant variance \(\sigma_0^2\).

The residual \(\epsilon\) is estimated by the errors \(e_i\). Therefore, we can look at the distribution of \(\{e_1, e_2, \cdots, e_n\}\) to see potential violations of the model assumptions.

\hypertarget{residual-diagnostics-and-remedies}{%
\subsection{Residual Diagnostics and Remedies}\label{residual-diagnostics-and-remedies}}

We make four default residual plots from R function \textbf{lm()} in R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))  }\CommentTok{\# par =\textgreater{} graphic parameter}
                     \CommentTok{\# mfrow =\textgreater{} splits the graphic page into panels}
\CommentTok{\#}
\FunctionTok{plot}\NormalTok{(ls.reg}\SpecialCharTok{$}\NormalTok{fitted.values, ls.reg}\SpecialCharTok{$}\NormalTok{residuals,    }
     \AttributeTok{main=}\StringTok{"Residuals vs Fitted"}\NormalTok{,       }\CommentTok{\# title of the plot}
     \AttributeTok{xlab =} \StringTok{"Fitted Values"}\NormalTok{,           }\CommentTok{\# label of X{-}axis }
     \AttributeTok{ylab =} \StringTok{"Residuals"}                \CommentTok{\# label of y{-}axis}
\NormalTok{     )}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h=}\DecValTok{0}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{qqnorm}\NormalTok{(ls.reg}\SpecialCharTok{$}\NormalTok{fitted.values, }\AttributeTok{main =} \StringTok{"Normal Q{-}Q"}\NormalTok{)}
\FunctionTok{qqline}\NormalTok{(ls.reg}\SpecialCharTok{$}\NormalTok{fitted.values, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-142-1} \end{center}

We can see that there seems to be a minor violation of the assumption of constant variance from the residual plot in the top left figure. In statistics, we have a transformation to stabilize the variable. We will introduce the well-known Box-Cox transformation in this module. It is a generic transformation and was designed to identify the optimal power transformation to stabilize the variance and maintain the normality. Sometimes it works really well but not always. It is always worth a try in case we observe the pattern of non-constant variance.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{boxcox}\NormalTok{(height }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bodymass, }
       \AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{12}\NormalTok{, }\AttributeTok{length =} \DecValTok{100}\NormalTok{), }
       \AttributeTok{xlab=}\StringTok{"lambda"}\NormalTok{)}
\DocumentationTok{\#\# }
\FunctionTok{title}\NormalTok{(}\AttributeTok{main =} \StringTok{"Box{-}Cox Transformation: 95\% CI of lambda"}\NormalTok{,}
      \AttributeTok{col.main =} \StringTok{"navy"}\NormalTok{, }\AttributeTok{cex.main =} \FloatTok{0.9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-143-1} \end{center}

The above plot shows the 95\% confidence interval of the power (\(\lambda\)) on the potential power transformation of the response variable \(height\).

In practice, we choose the \textbf{most convenient} number in the interval as the power to transform the response variable. Since 1 is in the interval, it is necessary to perform a power transformation. We can also choose the logarithmic transformation of height since \(\lambda = 0\) is also in the interval.

\textbf{Note}: A special power transformation is the logarithmic transformation of the response if we choose \(\lambda = 0\).

\hypertarget{final-model-with-applications}{%
\subsection{Final Model with Applications}\label{final-model-with-applications}}

Once the final model is identified, we can use it in two different ways: association and prediction.

In the \textbf{association analysis}, we interpret the regression coefficient associated with the significant explanatory variable.

In this toy example, the summarized statistics are extracted and summarized in the following

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(ls.reg)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{caption =} \StringTok{"Summary of regression model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-144}Summary of regression model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 78.627588 & 18.7505570 & 4.193347 & 0.0085442\\
\hline
bodymass & 1.267897 & 0.2929519 & 4.328005 & 0.0075135\\
\hline
\end{tabular}
\end{table}

The p-value of testing the null hypothesis that the slope parameter is zero is 0.0075. We reject the null hypothesis \(H_0: slope = 0\). This implies that \textbf{bodymass} impacts \textbf{height}. To be more specific, as the value of \textbf{bodymass} increases by one unit, the response variable \textbf{height} will increase by 1.27 cm.

In the \textbf{prediction analysis}, we can predict the \textbf{height} with any given new \textbf{bodymass} that is not in the data set. For example, assume \textbf{bodymass = 75}, we can then use the R function \textbf{predict()} to predict the \textbf{height}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.height }\OtherTok{=} \FunctionTok{predict}\NormalTok{(ls.reg, }\AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{bodymass=}\DecValTok{75}\NormalTok{), }
                      \AttributeTok{interval =} \StringTok{"prediction"}\NormalTok{, }\AttributeTok{level=}\FloatTok{0.05}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(pred.height, }\AttributeTok{caption=}\StringTok{"The predicted height with body mass: 75"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-145}The predicted height with body mass: 75}
\centering
\begin{tabular}[t]{r|r|r}
\hline
fit & lwr & upr\\
\hline
173.7199 & 172.9821 & 174.4577\\
\hline
\end{tabular}
\end{table}

For a given person with a body mass of 75 units, the predicted height of that person is 173.7cm with a 95\% predictive interval {[}173.0, 174.5{]}.

\textbf{In summary}: It is dependent on the objectives of your data analysis,

\begin{itemize}
\item
  if the objective is association analysis, the summary will focus on the interpretation of the regression coefficient with a p-value \textless{} 0.05.
\item
  if the objective is prediction, the summary will focus on the predicted value and its predictive interval.
\item
  if the objectives are both association analysis and prediction, then summarize both results as shown above.
\end{itemize}

\hypertarget{case-study-amyotrophic-lateral-sclerosis-analysis-revisited}{%
\section{Case Study: Amyotrophic lateral sclerosis analysis revisited}\label{case-study-amyotrophic-lateral-sclerosis-analysis-revisited}}

We only perform the least square regression analysis about the relationship between the partial
pressure of arterial oxygen (Pao2) and partial pressure of arterial carbon dioxide (Paco2) in patients with the disease.

The scatter plot of Paco2 versus Pao2 in section 1 indicates a negative association between them. Next, we assume the linear relationship between Paco2 and Pao2 and build the least square regression in the following steps.

\hypertarget{objectives}{%
\subsection{Objectives}\label{objectives}}

The objective of this analysis is to build a linear regression model and then use this model to

\begin{itemize}
\item
  assess how Pao2 impacts Paco2.
\item
  predict the value of Paco2 for given Pao2
\end{itemize}

\hypertarget{model-fitting}{%
\subsection{Model Fitting}\label{model-fitting}}

We fit a least square regression to the data first. Based on the objective of the analysis, Paco2 will be the response variable and Pao2 will be the explanatory variable. We first fit the following model

\[
Paco2 = \alpha + \beta \times Pao2 + \epsilon
\]
\(\alpha\), \(\beta\), and \(\epsilon\) are called intercept, slope, and residuals, respectively. Then carry out the diagnostics of the above model and find the potential remedy.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Paco2 }\OtherTok{=}\FunctionTok{c}\NormalTok{(}\FloatTok{40.0}\NormalTok{, }\FloatTok{47.0}\NormalTok{, }\FloatTok{34.0}\NormalTok{, }\FloatTok{42.0}\NormalTok{, }\FloatTok{54.0}\NormalTok{, }\FloatTok{48.0}\NormalTok{, }\FloatTok{53.6}\NormalTok{, }\FloatTok{56.9}\NormalTok{, }\FloatTok{58.0}\NormalTok{, }\FloatTok{45.0}\NormalTok{, }\FloatTok{54.5}\NormalTok{, }\FloatTok{54.0}\NormalTok{, }
         \FloatTok{43.0}\NormalTok{, }\FloatTok{44.3}\NormalTok{, }\FloatTok{53.9}\NormalTok{, }\FloatTok{41.8}\NormalTok{, }\FloatTok{33.0}\NormalTok{, }\FloatTok{43.1}\NormalTok{, }\FloatTok{52.4}\NormalTok{, }\FloatTok{37.9}\NormalTok{, }\FloatTok{34.5}\NormalTok{, }\FloatTok{40.1}\NormalTok{, }\FloatTok{33.0}\NormalTok{, }\FloatTok{59.9}\NormalTok{, }
         \FloatTok{62.6}\NormalTok{, }\FloatTok{54.1}\NormalTok{, }\FloatTok{45.7}\NormalTok{, }\FloatTok{40.6}\NormalTok{, }\FloatTok{56.6}\NormalTok{, }\FloatTok{59.0}\NormalTok{)}
\NormalTok{Pao2 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{101.0}\NormalTok{, }\FloatTok{69.0}\NormalTok{, }\FloatTok{132.0}\NormalTok{, }\FloatTok{65.0}\NormalTok{, }\FloatTok{72.0}\NormalTok{, }\FloatTok{76.0}\NormalTok{, }\FloatTok{67.2}\NormalTok{, }\FloatTok{70.9}\NormalTok{, }\FloatTok{73.0}\NormalTok{, }\FloatTok{66.0}\NormalTok{, }\FloatTok{80.0}\NormalTok{, }\FloatTok{72.0}\NormalTok{,}
         \FloatTok{105.0}\NormalTok{, }\FloatTok{113.0}\NormalTok{, }\FloatTok{69.2}\NormalTok{, }\FloatTok{66.7}\NormalTok{, }\FloatTok{67.0}\NormalTok{, }\FloatTok{77.5}\NormalTok{, }\FloatTok{65.1}\NormalTok{, }\FloatTok{71.0}\NormalTok{, }\FloatTok{86.5}\NormalTok{, }\FloatTok{74.7}\NormalTok{, }\FloatTok{94.0}\NormalTok{, }\FloatTok{60.4}\NormalTok{,}
         \FloatTok{52.5}\NormalTok{, }\FloatTok{76.9}\NormalTok{, }\FloatTok{65.3}\NormalTok{, }\FloatTok{80.3}\NormalTok{, }\FloatTok{53.2}\NormalTok{, }\FloatTok{71.9}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{ls.reg0 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Paco2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Pao2)   }\CommentTok{\# fitting a least square regression}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))   }\CommentTok{\# split the graphic page into 4 panels}
\FunctionTok{plot}\NormalTok{(ls.reg0)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-146-1} \end{center}

The residual diagnostic plots show that there are violations of the model assumptions. The Q-Q plot does not support the normality assumption of the residuals. The top-left residual plot does not support the constant variance assumption since the variance of the residual increases as the fitted value increases.

Next, we perform the Box-Cox transformation.

\hypertarget{box-cox-transformation}{%
\subsection{Box-Cox Transformation}\label{box-cox-transformation}}

In the Box-cox transformation, the range of potential \(\lambda\) is selected by trial and error so that the figure should contain the 95\% confidence interval. We will use a function in the library \textbf{\{MASS\}}. If you don't have the library on your computer, you need to install it and then load it to the workspace.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{boxcox}\NormalTok{(Paco2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Pao2, }\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{length =} \DecValTok{10}\NormalTok{), }
       \AttributeTok{xlab=}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(lambda)))}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main =} \StringTok{"Box{-}Cox Transformation: 95\% CI of lambda"}\NormalTok{,}
      \AttributeTok{col.main =} \StringTok{"navy"}\NormalTok{, }\AttributeTok{cex.main =} \FloatTok{0.9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-147-1.pdf}

The above Box-Cox procedure indicates that the power transformation will not improve the residual plots. I will not try other transformations in this course and simply use the above model as the final working model for prediction and perform association analysis.

\hypertarget{model-applications}{%
\subsection{Model Applications}\label{model-applications}}

Recall that we will use the final working model to assess the association between the Paco2 and Pao2 and predict the value of Paco2 with the new Pao2 as well.

We first present summary statistics of the least square regression model in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ls.reg.final }\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(Paco2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Pao2)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(ls.reg.final)}\SpecialCharTok{$}\NormalTok{coef, }
      \AttributeTok{caption =}\StringTok{"Summary of the final least square regression model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-148}Summary of the final least square regression model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 67.9716010 & 6.3535426 & 10.698221 & 0.0000000\\
\hline
Pao2 & -0.2687739 & 0.0811017 & -3.314037 & 0.0025473\\
\hline
\end{tabular}
\end{table}

We can see that Pao2 significantly impacts Paco2 with a p-value = 0.0025. To be more specific, as Pao2 increases by a unit, the Paco2 \textbf{decreases} by about 0.27. The negative sign of the estimated slope indicates the negative linear association between Paco2 and Pao2.

Now, let's assume that there are two new patients who Pao2 levels 63 and 75, respectively. Note that these two Pao2 are \textbf{within the range of Pao2}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pander)}
\DocumentationTok{\#\# put the new observations in the form of the data frame.}
\NormalTok{new.pao2 }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Pao2 =} \FunctionTok{c}\NormalTok{(}\DecValTok{63}\NormalTok{,}\DecValTok{75}\NormalTok{))}
\DocumentationTok{\#\#}
\NormalTok{pred.new }\OtherTok{=} \FunctionTok{predict}\NormalTok{(ls.reg.final, }\AttributeTok{newdata =}\NormalTok{ new.pao2, }
                   \AttributeTok{interval =} \StringTok{"prediction"}\NormalTok{,}
                   \AttributeTok{level =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{pred.new.cbind }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{Pao2.new=}\FunctionTok{c}\NormalTok{(}\DecValTok{63}\NormalTok{,}\DecValTok{75}\NormalTok{), pred.new)}
\FunctionTok{pander}\NormalTok{(pred.new.cbind, }\AttributeTok{caption =} \StringTok{"95\% predictive intervals of Paco2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1528}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1111}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1111}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1111}}@{}}
\caption{95\% predictive intervals of Paco2}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Pao2.new
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
fit
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
lwr
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
upr
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Pao2.new
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
fit
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
lwr
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
upr
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
63 & 51.04 & 50.55 & 51.53 \\
75 & 47.81 & 47.33 & 48.3 \\
\end{longtable}

The above predictive table indicates that the predicted value of Paco2 with Pao2 = 63 is about 51.04 with a 95\% predictive interval {[}50.55, 51.23{]}. The predicted Paco2 is 47.81 with a 95\% predictive interval {[}47.23, 48.30{]} for Pao2 = 73.

\hypertarget{multiple-linear-regression}{%
\chapter{Multiple Linear Regression}\label{multiple-linear-regression}}

We discussed the relationship between variables in the previous two modules. The continuous variable with a normal distribution is called the response (dependent) variable and the other variable is called the explanatory (predictor, independent, or risk) variable. If the predictor variable is a factor variable, the model is called the ANOVA model which focuses on comparing the means across all factor levels. If the predictor variable is \textbf{continuous}, the model is called simple linear regression (SLR). Note that all predictor variables are assumed to be non-random.

\hypertarget{the-practical-question}{%
\section{The Practical Question}\label{the-practical-question}}

Maximum mouth opening (MMO) is also an important diagnostic reference for dental clinicians as a preliminary evaluation. Establishing a normal range for MMO could allow dental clinicians to objectively evaluate the treatment effects and set therapeutic goals for patients performing mandibular functional exercises.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img10/w10-MMO} 

}

\caption{MMO, ML, and RA}\label{fig:unnamed-chunk-151}
\end{figure}

To study the relationship between maximum mouth opening and measurements of the lower jaw (mandible). A researcher randomly selected a sample of 35 subjects and measured the dependent variable, maximum mouth opening (MMO, measured in mm), as well as predictor variables, mandibular length (ML, measured in mm), and angle of rotation of the mandible (RA, measured in degrees) of each of the 35 subjects.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img10/w10-DentalDataTable} 

}

\caption{Dental Data for the multiple linear regression model (MLR)}\label{fig:unnamed-chunk-152}
\end{figure}

The question is whether the maximum mouth opening (MMO) is determined by \textbf{two variables simultaneously}. We want to assess how these two variables (ML and RA) impact MMO \textbf{simultaneously}.

If we pick one predictor variable at a time, ML, to build a simple linear regression model and ignore the other predictor variable (RA), you only get the marginal relationship between MMO and ML since you implicitly assume that the relationship between MMO and ML will not be impacted by RA. This implicit assumption is, in general, incorrect. We need to consider all predictor variables at the same time. This is the motivation for studying multiple linear regression (MLR).

\hypertarget{the-process-of-building-a-multiple-linear-regression-model}{%
\section{The Process of Building A Multiple Linear Regression Model}\label{the-process-of-building-a-multiple-linear-regression-model}}

The previous motivation example involves two continuous predictor variables. In real-world applications, it is common to have many predictor variables. Predictor variables are also assumed to be non-random. They could be categorical, continuous, or discrete. In a specific application, you may have a set of categorical, continuous, and discrete predictor variables in one data set.

\hypertarget{assumptions-of-mlr}{%
\subsection{Assumptions of MLR}\label{assumptions-of-mlr}}

There are several assumptions of multiple linear regression models.

\begin{itemize}
\item
  The response variable is a normal random variable and its mean is influenced by explanatory variables but not the variance.
\item
  The explanatory variables are assumed to be non-random.
\item
  The explanatory variables are assumed to be uncorrelated to each other.
\item
  The functional form of the explanatory variables in the regression model is correctly specified.
\item
  The data is a random sample taken independently from the study population with a specified distribution.
\end{itemize}

Some of these assumptions will be used directly to define model diagnostic measures. The idea is to assume all conditions are met (at least temporarily) and then fit the model to the data set.

\hypertarget{the-structure-of-mlr}{%
\subsection{The Structure of MLR}\label{the-structure-of-mlr}}

Assume that there are \(p\) predictor variables \(\{x_1, x_2, \cdots, x_p \}\), the first-order linear regression is defined in the following form

\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon
\]

\(\beta_0\) is the intercept, \(\beta_1, \beta_2, \cdots, \beta_p\) are called slope parameters. if \(\beta_i=0\), the associated predictor variable \(x_i\) is uncorrelated with response vararible \(y\). If \(\beta_i > 0\), then \(y\) and \(x_i\) are positively correlated. In fact, \(\beta_1\) is the increment of \(y\) as \(x_i\) increases one unit and other predictors remain unchanged.

The response variable is assumed to be a normal random variable with constant variance. If the first-order linear regression function is correct, then

\[y \to N(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p, \sigma^2).\]
This also implies that \(\epsilon \to N(0,1)\). The residual of each data point can be estimated from the data with an assumed linear regression model.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img10/w10-RegressionPlane} 

}

\caption{Illustrative regression plane: MMO vs ML and RA}\label{fig:unnamed-chunk-153}
\end{figure}

For ease of illustration, let's consider the case of the MLR with two predictor variables in the motivation example.

\[MMO = \beta_0 + \beta_1 ML + \beta_2 RA + \epsilon\]

is the first-order linear regression model. The following figure gives the graphical annotations of the fundamental concepts in linear regression models. This is a generalization of the regression line (see the analogous figure in the previous module for the simple linear regression model).

Since \(MMO\) is a normal random variable with constant variance, \(MMO \to N(\beta_0+\beta_1ML +\beta_2 RA, \sigma^2)\), or equivalently, \(\epsilon \to N(0, \sigma^2)\). The residuals are defined to be the directional vertical distances between the observed points and the regression plane.

In some practical applications, we may need \textbf{the second-order} linear regression model to reflect the actual relationship between predictor variables and the response variable. For example, \[MMO = \alpha_0 + \alpha_1 ML + \alpha_2 RA + \alpha_3 ML^2 + \alpha_4 RA^2 + \alpha_5 ML\times RA + \epsilon\] is called (the second-order) linear regression model. With the second-order terms in the regression function, we obtain the regression surface as shown in Figure.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img10/w10-RegressionSurface} 

}

\caption{Illustrative regression surface: MMO vs ML and RA}\label{fig:unnamed-chunk-154}
\end{figure}

If the second-order linear regression is appropriate, then \(\epsilon \to N(0, \sigma^2)\) and \(E[MMO] = \alpha_0 + \alpha_1 ML + \alpha_2 RA + \alpha_3 ML^2 + \alpha_4 RA^2 + \alpha_5 ML\times RA\). The residuals of the second-order linear regression model are defined to be the directional distance between the observed points and the regression surface.

\hypertarget{more-on-model-specifications}{%
\subsection{More on Model Specifications}\label{more-on-model-specifications}}

In the above section, we introduced both first- and second-order polynomial regression models. In general, it is not common to use high-order polynomial regression models in real-world applications.

\begin{itemize}
\item
  \textbf{Interaction effect} - It is common to include interaction terms (i.e., the cross product of two or more predictor variables) in the multiple linear regression models when the effect of one variable on the response variable is dependent on the other predictor variable. In other words, the interaction terms capture the \textbf{joint effect} of predictor variables. \textbf{It is rare to have third-order or higher-order interaction terms in a regression model}.
\item
  \textbf{Dummy variables} - All categorical predictor variables are automatically converted into dummy variables (binary indicator variables). If categorical variables in the data are numerically coded, we have to turn these numerically coded variables into factor variables in the regression model.
\item
  \textbf{Discretization and Regrouping} - Discretizing numerical predictor variables and regrouping categorical or discrete predictor variables are two basic pre-process procedures that are actually very common in many practical applications.

  \begin{itemize}
  \item
    Sometimes these two procedures are required to satisfy certain model assumptions. For example, if a categorical variable has a few categories that have less than 5 observations, the resulting p-values based on certain hypothesis tests will be invalid. In this case, We have to regroup some of the categories in \textbf{meaningful ways} to resolve the \textbf{sparsity} issues in order to obtain valid results.
  \item
    In many other applications, we want the model to be easy to interpret. Discretizing numerical variables is common. For example, we can see grouped ages and salary ranges in different applications.
  \end{itemize}
\end{itemize}

\hypertarget{estimation-of-regression-coefficients}{%
\subsection{Estimation of Regression Coefficients}\label{estimation-of-regression-coefficients}}

A simple and straightforward method for estimating the coefficients of linear regression models is to minimize the sum of the squared residuals - least square estimation (LSE). To find the LSE of the regression coefficients, we need to

\begin{itemize}
\item
  choose the (first-order, second-order, or even high-order) regression function (see 3D hyper-plane or hyper-surface in the above two figures as examples).
\item
  find the distances between the observed points and the hyper-plane (or hyper-surface). These distances are the residuals of the regression - which is dependent on the regression coefficients.
\item
  calculate the sum of squared residuals. This sum of the residuals is still dependent on the regression coefficients.
\item
  find the values for the regression coefficients that minimize the sum of the squared residuals. These values are called the least square estimates (LSEs) of the corresponding regression coefficients.
\end{itemize}

R function \textbf{lm()} implements the above the LSE algorithm to find the regression coefficients. We have used this function in ANOVA and simple linear regression models.

\hypertarget{model-diagnostics}{%
\subsection{Model Diagnostics}\label{model-diagnostics}}

Unlike simple linear regression models, the primary assumptions of the regression model focus on the normal distribution of the response variable and the correct regression function. For multiple linear regression models, we need to impose a couple of assumptions in addition to those in the simple linear regression models

\begin{itemize}
\tightlist
\item
  \textbf{Residual Diagnostics}
\end{itemize}

One of the fundamental assumptions of linear regression modeling is that the response variable is normally distributed with a constant variance. This implies \(\epsilon \to N(0, \sigma^2)\).

After obtaining LSE of the regression coefficients, we can estimate the residuals and use these estimated residuals to detect the potential violations of the normality assumption of the response variable. To be more specific, we consider the first-order polynomial regression, the estimated residual of \(i\)-th observation is defined to be \(e_i = MMO - \hat{\beta}_0 + \hat{\beta}_1 ML + \hat{\beta}_2 RA\)

If there is no violation of the normality assumption, we would expect the following residual plot and Q-Q plot.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img10/w10-GoodResidualPlots} 

}

\caption{Good residual plot and normal Q-Q plot}\label{fig:unnamed-chunk-155}
\end{figure}

Some of the commonly seen poor residual plots represent different violations of various assumptions. We can try to use various transformations (such as Box-Cox power transformation) of the response variable to correct the issue.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img10/w10-BadResidualPlots} 

}

\caption{Poor residual plots representing various violations of the model assumptions}\label{fig:unnamed-chunk-156}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Multicollinearity}
\end{itemize}

Some of the predictor variables are linearly correlated. The consequence of multi-collinearity causes to unstable LSE of the regression coefficients (i.e., the LSEs of the regression coefficients are sensitive to a small change in the model). It also reduces the precision of the estimate coefficients and, hence, the p-values are not reliable.

Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If our primary goal is to make predictions, we don't need to understand the role of each independent variable and we don't need to reduce severe multicollinearity.

If the primary goal is to perform association analysis, we need to reduce collinearity since both LSE and p-values are the keys to association analysis.

To detect multicollinearity, we can use the variance inflation factor (VIF) to inspect the multicollinearity of the individual predictor variable. There are some different methods to reduce multicollinearity. Centering predictor variables is one of them and works well sometimes. Some other advanced modeling-based methods are covered in more advanced courses.

\hypertarget{goodness-of-fit-and-variable-selection}{%
\subsection{Goodness-of-fit and Variable Selection}\label{goodness-of-fit-and-variable-selection}}

Several different goodness-of-fit measures are available for the linear regression model due to the assumption of the normality assumption of the response variable.

\begin{itemize}
\tightlist
\item
  \textbf{Coefficient of Determination}
\end{itemize}

We only introduce \textbf{the coefficient of determination \(R^2\)} which measures the percentage of variability within the -values that can be explained by the regression model. In simple linear regression models, \textbf{the coefficient of determination \(R^2\)} is simply the square of the sample Pearson correlation coefficient.

\begin{itemize}
\tightlist
\item
  \textbf{Statistical Significance and Practical Importance}
\end{itemize}

A small p-value of the significant test for a predictor variable indicates the variable is statistically significant but may not be practically important. On the other hand, some practically important predictor variables may not achieve statistical significance due to the limited sample size. In the practical applications, \textbf{we may want to include some of the practically important predictor variables in the final model regardless of their statistical significance}.

\begin{itemize}
\tightlist
\item
  \textbf{Model Selection}
\end{itemize}

One of the criteria for assessing the goodness-of-fit is the parsimony of the model. A parsimonious model is a model that accomplishes the desired level of explanation or prediction with as few predictor variables as possible. There are generally two ways of evaluating a model: Based on predictions and based on goodness of fit on the current data such as \(R^2\) and some likelihood-based measures.

R has an automatic variable selection procedure, \textbf{step()}, which uses the goodness-of-fit measure AIC (Akaike Information Criterion) which is not formally introduced in this class due to the level of mathematics needed in the definition, but we can still use it to perform the automatic variable selection. \href{http://rstudio-pubs-static.s3.amazonaws.com/2899_a9129debf6bd47d2a0501de9c0dc583d.html}{This tutorial gives detailed examples on how to use \textbf{step()} (link)}.

\hypertarget{case-study-1}{%
\section{Case Study 1}\label{case-study-1}}

We use the dental data in the motivation example for the case study.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MMO}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\FloatTok{52.34}\NormalTok{, }\FloatTok{51.90}\NormalTok{, }\FloatTok{52.80}\NormalTok{, }\FloatTok{50.29}\NormalTok{, }\FloatTok{57.79}\NormalTok{, }\FloatTok{49.41}\NormalTok{, }\FloatTok{53.28}\NormalTok{, }\FloatTok{59.71}\NormalTok{, }\FloatTok{53.32}\NormalTok{, }\FloatTok{48.53}\NormalTok{, }
      \FloatTok{51.59}\NormalTok{, }\FloatTok{58.52}\NormalTok{, }\FloatTok{62.93}\NormalTok{, }\FloatTok{57.62}\NormalTok{, }\FloatTok{65.64}\NormalTok{, }\FloatTok{52.85}\NormalTok{, }\FloatTok{64.43}\NormalTok{, }\FloatTok{57.25}\NormalTok{, }\FloatTok{50.82}\NormalTok{, }\FloatTok{40.48}\NormalTok{, }
      \FloatTok{59.68}\NormalTok{, }\FloatTok{54.35}\NormalTok{, }\FloatTok{47.00}\NormalTok{, }\FloatTok{47.23}\NormalTok{, }\FloatTok{41.19}\NormalTok{, }\FloatTok{42.76}\NormalTok{, }\FloatTok{51.88}\NormalTok{, }\FloatTok{42.77}\NormalTok{, }\FloatTok{52.34}\NormalTok{, }\FloatTok{50.45}\NormalTok{, }
      \FloatTok{43.18}\NormalTok{, }\FloatTok{41.99}\NormalTok{, }\FloatTok{39.45}\NormalTok{, }\FloatTok{38.91}\NormalTok{, }\FloatTok{49.10}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{ML}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\FloatTok{100.85}\NormalTok{, }\FloatTok{93.08}\NormalTok{, }\FloatTok{98.43}\NormalTok{, }\FloatTok{102.95}\NormalTok{, }\FloatTok{108.24}\NormalTok{, }\FloatTok{98.34}\NormalTok{, }\FloatTok{95.57}\NormalTok{, }\FloatTok{98.85}\NormalTok{,}\FloatTok{98.32}\NormalTok{, }\FloatTok{92.70}\NormalTok{, }
     \FloatTok{88.89}\NormalTok{, }\FloatTok{104.06}\NormalTok{, }\FloatTok{98.18}\NormalTok{, }\FloatTok{91.01}\NormalTok{, }\FloatTok{96.98}\NormalTok{, }\FloatTok{97.85}\NormalTok{, }\FloatTok{96.89}\NormalTok{, }\FloatTok{98.35}\NormalTok{, }\FloatTok{90.65}\NormalTok{, }\FloatTok{92.99}\NormalTok{, }
     \FloatTok{108.97}\NormalTok{, }\FloatTok{91.85}\NormalTok{, }\FloatTok{104.30}\NormalTok{, }\FloatTok{93.16}\NormalTok{, }\FloatTok{94.18}\NormalTok{, }\FloatTok{89.56}\NormalTok{, }\FloatTok{105.85}\NormalTok{, }\FloatTok{89.29}\NormalTok{, }\FloatTok{92.58}\NormalTok{, }\FloatTok{98.64}\NormalTok{, }
     \FloatTok{83.70}\NormalTok{, }\FloatTok{88.46}\NormalTok{, }\FloatTok{94.93}\NormalTok{, }\FloatTok{96.81}\NormalTok{, }\FloatTok{93.13}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{RA }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{32.08}\NormalTok{, }\FloatTok{39.21}\NormalTok{, }\FloatTok{33.74}\NormalTok{, }\FloatTok{34.19}\NormalTok{, }\FloatTok{35.13}\NormalTok{, }\FloatTok{30.92}\NormalTok{, }\FloatTok{37.71}\NormalTok{, }\FloatTok{44.71}\NormalTok{, }\FloatTok{33.17}\NormalTok{, }\FloatTok{31.74}\NormalTok{, }
       \FloatTok{37.07}\NormalTok{, }\FloatTok{38.71}\NormalTok{, }\FloatTok{43.89}\NormalTok{, }\FloatTok{41.06}\NormalTok{, }\FloatTok{41.92}\NormalTok{, }\FloatTok{35.25}\NormalTok{, }\FloatTok{45.11}\NormalTok{, }\FloatTok{39.44}\NormalTok{, }\FloatTok{38.33}\NormalTok{, }\FloatTok{25.93}\NormalTok{, }
       \FloatTok{36.78}\NormalTok{, }\FloatTok{42.02}\NormalTok{, }\FloatTok{27.20}\NormalTok{, }\FloatTok{31.37}\NormalTok{, }\FloatTok{27.87}\NormalTok{, }\FloatTok{28.69}\NormalTok{, }\FloatTok{31.04}\NormalTok{, }\FloatTok{32.78}\NormalTok{, }\FloatTok{37.82}\NormalTok{, }\FloatTok{33.36}\NormalTok{, }
       \FloatTok{31.93}\NormalTok{, }\FloatTok{28.32}\NormalTok{, }\FloatTok{24.82}\NormalTok{, }\FloatTok{23.88}\NormalTok{, }\FloatTok{36.17}\NormalTok{)}
\NormalTok{DentalData }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{MMO =}\NormalTok{ MMO, }\AttributeTok{ML =}\NormalTok{ ML, }\AttributeTok{RA =}\NormalTok{ RA))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Pair-wise Scatter Plot}
\end{itemize}

This pairwise scatter plot tells whether there are significant correlations between \textbf{numerical predictor variables}.

\begin{figure}

{\centering \includegraphics{STA501EB_files/figure-latex/unnamed-chunk-158-1} 

}

\caption{Pair-wise scatter plot}\label{fig:unnamed-chunk-158}
\end{figure}

We can see the following patterns from the above pair-wise scatter plot.

(1). Both ML and RA are linearly correlated with the response variable MMO. This is what we expected.

(2). ML and RA are not linearly correlated. This indicates that there is no collinearity issue.

(3). We also don't see any special patterns such as outliers and extremely skewed distribution. There is no need to perform discretization and regrouping procedures on the predictor variables.

(4). In this data set, there is no categorical variables or categorical variable with a numerical coding system in this data set. There is no need to create dummy variables.

\begin{itemize}
\tightlist
\item
  \textbf{Initial model}
\end{itemize}

The following initial model includes all predictor variables and then performs the residual diagnostics immediately afterward.

The residual plots indicate that

(1). One of the observations seems to be an outlier (observation 15);

(2). There is a minor violation of the assumption of constant variance.

(3). There is also a minor violation of the assumption of normality of the distribution of the residuals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ini.model }\OtherTok{=} \FunctionTok{lm}\NormalTok{(MMO }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ML }\SpecialCharTok{+}\NormalTok{ RA, }\AttributeTok{data =}\NormalTok{ DentalData)   }\CommentTok{\#  interaction effect}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(ini.model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-159-1} \end{center}

Next, we will carry the Box-Cox transformation to identify a potential power transformation of the response variable MMO.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{boxcox}\NormalTok{(MMO }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ML }\SpecialCharTok{+}\NormalTok{ RA, }
       \AttributeTok{data =}\NormalTok{ DentalData, }
       \AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\AttributeTok{length =} \DecValTok{10}\NormalTok{), }
       \AttributeTok{xlab=}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(lambda)))}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main =} \StringTok{"Box{-}Cox Transformation: 95\% CI of lambda"}\NormalTok{,}
      \AttributeTok{col.main =} \StringTok{"navy"}\NormalTok{, }\AttributeTok{cex.main =} \FloatTok{0.9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-160-1} \end{center}

Since both 0 and 1 are in the \(95\%\) confidence interval of \(\lambda\), technically speaking, there is no need to perform the power transformation. By the optimal \(\lambda\) is closer to 0, we try to perform the log transformation (corresponding to \(\lambda =0\)) to see whether there will be some improvement of the initial model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{transform.model }\OtherTok{=} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(MMO) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ML }\SpecialCharTok{*}\NormalTok{ RA, }\AttributeTok{data  =}\NormalTok{ DentalData)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(transform.model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-161-1} \end{center}

The above residual plots indicate an improvement in model fit. We will use the transformed response to build the final model.

\begin{itemize}
\tightlist
\item
  \textbf{Final Model}
\end{itemize}

The model based on the log-transformed response is summarized in the following.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(transform.model)}\SpecialCharTok{$}\NormalTok{coef, }
\AttributeTok{caption =} \StringTok{"Summarized statistics of the regression }
\StringTok{      coefficients of the model with log{-}transformed response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-162}Summarized statistics of the regression 
      coefficients of the model with log-transformed response}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 1.9957108 & 1.0023242 & 1.9910831 & 0.0553479\\
\hline
ML & 0.0124400 & 0.0104503 & 1.1903999 & 0.2429256\\
\hline
RA & 0.0296960 & 0.0293716 & 1.0110477 & 0.3198204\\
\hline
ML:RA & -0.0000884 & 0.0003059 & -0.2890324 & 0.7744807\\
\hline
\end{tabular}
\end{table}

we can see that the interaction effect is insignificant in the model. We drop the highest term in the regression model either manually or automatically. In the next code chunk, we use the automatic variable selection method to find the final model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{transform.model }\OtherTok{=} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(MMO)}\SpecialCharTok{\textasciitilde{}}\NormalTok{ML}\SpecialCharTok{*}\NormalTok{RA, }\AttributeTok{data =}\NormalTok{ DentalData)}
\DocumentationTok{\#\# I will use automatic variable selection function to search the final model}
\NormalTok{final.model }\OtherTok{=}  \FunctionTok{step}\NormalTok{(transform.model, }\AttributeTok{direction =} \StringTok{"backward"}\NormalTok{, }\AttributeTok{trace =} \DecValTok{0}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(final.model)}\SpecialCharTok{$}\NormalTok{coef, }
\AttributeTok{caption =} \StringTok{"Summarized statistics of the regression }
\StringTok{      coefficients of the final model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-163}Summarized statistics of the regression 
      coefficients of the final model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 2.2833535 & 0.1176375 & 19.410076 & 0\\
\hline
ML & 0.0094391 & 0.0011705 & 8.064482 & 0\\
\hline
RA & 0.0212140 & 0.0012017 & 17.653748 & 0\\
\hline
\end{tabular}
\end{table}

Now we have three candidate models to select from. We extract the coefficient of determination (\(R^2\)) of each of the three candidate models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r.ini.model }\OtherTok{=} \FunctionTok{summary}\NormalTok{(ini.model)}\SpecialCharTok{$}\NormalTok{r.squared}
\NormalTok{r.transfd.model }\OtherTok{=} \FunctionTok{summary}\NormalTok{(transform.model)}\SpecialCharTok{$}\NormalTok{r.squared}
\NormalTok{r.final.model }\OtherTok{=} \FunctionTok{summary}\NormalTok{(final.model)}\SpecialCharTok{$}\NormalTok{r.squared}
\DocumentationTok{\#\#}
\NormalTok{Rsquare }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{ini.model =}\NormalTok{ r.ini.model, }\AttributeTok{transfd.model =}\NormalTok{ r.transfd.model, }
                \AttributeTok{final.model =}\NormalTok{ r.final.model)}
\FunctionTok{kable}\NormalTok{(Rsquare, }\AttributeTok{caption=}\StringTok{"Coefficients of correlation of the three candidate models"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-164}Coefficients of correlation of the three candidate models}
\centering
\begin{tabular}[t]{r|r|r}
\hline
ini.model & transfd.model & final.model\\
\hline
0.9204481 & 0.9257218 & 0.9255216\\
\hline
\end{tabular}
\end{table}

The second and the third models have almost the same \(R^2\), \(92.56\%\) and \(92.57\%\). Both models are based on the log-transformed MMO. The interpretations of these two models are not straightforward. The initial model has a slightly lower \(92.0\%\). Since the initial model has a simple structure and is easy to interpret, we chose the initial model as the final model to report. The summarized statistic is given in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary.ini.model }\OtherTok{=} \FunctionTok{summary}\NormalTok{(ini.model)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{kable}\NormalTok{(summary.ini.model, }\AttributeTok{caption =} \StringTok{"Summary of the final working model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-165}Summary of the final working model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & -31.4247984 & 6.1474668 & -5.111829 & 0.0000144\\
\hline
ML & 0.4731743 & 0.0611653 & 7.735992 & 0.0000000\\
\hline
RA & 1.0711725 & 0.0627967 & 17.057792 & 0.0000000\\
\hline
\end{tabular}
\end{table}

In summary, both ML and RA are statistically significant (p-value \(\approx 0\)) and both are positively correlated to MMO. Further, for a given angle of rotation of the mandible (RA), when mandibular length (ML) increases by 1mm, the maximum mouth opening (MMO) increases by 0.473 mm. However, for holding ML, a 1-degree increase in RA will result in a 1.071 mm increase in MMO.

\hypertarget{case-study-2}{%
\section{Case Study 2}\label{case-study-2}}

We discussed the ANOVA model in module 8. In fact, the ANOVA model is a special linear regression model. The location is a factor variable. We now build a linear regression using mussel shell length as the response and the location as the predictor variable in the following (code is copied from module 8).

Since predictor variable location is a categorical factor variable, R function \textbf{lm()} will automatically define four dummy variables for each category except for the baseline category is, by default, the smallest character values (alphabetical order). In our example, the value \textbf{Magadan} is the smallest. Other categories will be compared with the baseline category through the corresponding dummy variable.

To be more specific, the four dummy variables associated with the four categories will be defined by

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  locationNewport = 1 if the location is Newport, 0 otherwise;
\item
  locationPetersburg = 1 if the location is Petersburg, 0 otherwise;
\item
  locationTillamook = 1 if the location is Tillamook, 0 otherwise;
\item
  locationTvarminne = 1 if the location is Tvarminne, 0 otherwise.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0571}\NormalTok{,}\FloatTok{0.0813}\NormalTok{, }\FloatTok{0.0831}\NormalTok{, }\FloatTok{0.0976}\NormalTok{, }\FloatTok{0.0817}\NormalTok{, }\FloatTok{0.0859}\NormalTok{, }\FloatTok{0.0735}\NormalTok{, }\FloatTok{0.0659}\NormalTok{, }
       \FloatTok{0.0923}\NormalTok{, }\FloatTok{0.0836}\NormalTok{) }
\NormalTok{x2 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0873}\NormalTok{,}\FloatTok{0.0662}\NormalTok{, }\FloatTok{0.0672}\NormalTok{, }\FloatTok{0.0819}\NormalTok{, }\FloatTok{0.0749}\NormalTok{, }\FloatTok{0.0649}\NormalTok{, }\FloatTok{0.0835}\NormalTok{, }\FloatTok{0.0725}\NormalTok{)}
\NormalTok{x3 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0974}\NormalTok{,}\FloatTok{0.1352}\NormalTok{, }\FloatTok{0.0817}\NormalTok{, }\FloatTok{0.1016}\NormalTok{, }\FloatTok{0.0968}\NormalTok{, }\FloatTok{0.1064}\NormalTok{, }\FloatTok{0.1050}\NormalTok{)}
\NormalTok{x4 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1033}\NormalTok{,}\FloatTok{0.0915}\NormalTok{, }\FloatTok{0.0781}\NormalTok{, }\FloatTok{0.0685}\NormalTok{, }\FloatTok{0.0677}\NormalTok{, }\FloatTok{0.0697}\NormalTok{, }\FloatTok{0.0764}\NormalTok{, }\FloatTok{0.0689}\NormalTok{)}
\NormalTok{x5 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0703}\NormalTok{,}\FloatTok{0.1026}\NormalTok{, }\FloatTok{0.0956}\NormalTok{, }\FloatTok{0.0973}\NormalTok{, }\FloatTok{0.1039}\NormalTok{, }\FloatTok{0.1045}\NormalTok{)}
\NormalTok{len  }\OtherTok{=} \FunctionTok{c}\NormalTok{(x1, x2, x3, x4, x5)      }\CommentTok{\# pool all sub{-}samples of lengths}
\NormalTok{location }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Tillamook"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x1)), }
             \FunctionTok{rep}\NormalTok{(}\StringTok{"Newport"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x2)),}
             \FunctionTok{rep}\NormalTok{(}\StringTok{"Petersburg"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x3)),}
             \FunctionTok{rep}\NormalTok{(}\StringTok{"Magadan"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x4)),}
             \FunctionTok{rep}\NormalTok{(}\StringTok{"Tvarminne"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x5))) }\CommentTok{\# location vector matches the lengths}
\NormalTok{data.matrix }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{len =}\NormalTok{  len, }\AttributeTok{location =}\NormalTok{ location)   }\CommentTok{\# data a data table}
\NormalTok{musseldata }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(data.matrix)        }\CommentTok{\# data frame}
\DocumentationTok{\#\# end of data set creation}
\DocumentationTok{\#\#}
\DocumentationTok{\#\# starting building ANOVA model}
\NormalTok{anova.model}\FloatTok{.01} \OtherTok{=} \FunctionTok{lm}\NormalTok{(len }\SpecialCharTok{\textasciitilde{}}\NormalTok{ location, }\AttributeTok{data =}\NormalTok{ musseldata)  }\CommentTok{\# define a model for generating the ANOVA}
\DocumentationTok{\#\#}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(anova.model}\FloatTok{.01}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-166-1} \end{center}

The above residual plots indicate no serious violation of the model assumption. The model that generates the above residual plot will be used as the final working model. The inference of the regression coefficients is summarized in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.stats }\OtherTok{=} \FunctionTok{summary}\NormalTok{(anova.model}\FloatTok{.01}\NormalTok{)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{kable}\NormalTok{(sum.stats, }\AttributeTok{caption =} \StringTok{"Summary of the ANOVA model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-167}Summary of the ANOVA model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 0.0780125 & 0.0044536 & 17.5168782 & 0.0000000\\
\hline
locationNewport & -0.0032125 & 0.0062983 & -0.5100593 & 0.6133053\\
\hline
locationPetersburg & 0.0254304 & 0.0065193 & 3.9007522 & 0.0004300\\
\hline
locationTillamook & 0.0021875 & 0.0059751 & 0.3661039 & 0.7165558\\
\hline
locationTvarminne & 0.0176875 & 0.0068029 & 2.5999834 & 0.0136962\\
\hline
\end{tabular}
\end{table}

From the above summary tale, we can see that P-values associated with location dummy variables locationNewport, locationTillamook are bigger than 0.05 meaning the means associated with \textbf{Newport}, \textbf{Tillamook}, and the baseline \textbf{Magadan} (not appearing in the summary table). The p-values associated with \textbf{Petersburg} and \textbf{Tvarminn} are less the 0.05 which implies that the mean length of these two locations is significantly different from that of the baseline location \textbf{Magadan}. Further, the coefficient associated with dummy variable \textbf{locationPetersburg} indicates that the mean length of mussel shell in \textbf{Petersburg} is 0.0543 units longer than that in the baseline location \textbf{Magadan}. We can also interpret the coefficients associated with \textbf{locationTvarminne}.\\

\hypertarget{practice-problems-2}{%
\section{Practice Problems}\label{practice-problems-2}}

Family caregiving of older adults is more common in Korea than in the United States. A research team studied 100 caregivers of older adults with dementia in Seoul, South Korea. The dependent variable was caregiver burden as measured by the Korean Burden Inventory (KBI). Scores ranged from 28 to 140, with higher scores indicating a higher burden. Explanatory variables were indexes that measured the following:

\textbf{ADL}: total activities of daily living (low scores indicate that the elderly perform activities
independently).

\textbf{MEM}: memory and behavioral problems (higher scores indicate more problems).

\textbf{COG}: cognitive impairment (lower scores indicate a greater degree of cognitive impairment).

The reported data are given in the following code chunk.

\hfill\break

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{28}\NormalTok{, }\DecValTok{68}\NormalTok{, }\DecValTok{59}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{70}\NormalTok{, }\DecValTok{38}\NormalTok{, }\DecValTok{46}\NormalTok{, }\DecValTok{57}\NormalTok{, }\DecValTok{89}\NormalTok{, }\DecValTok{48}\NormalTok{, }\DecValTok{74}\NormalTok{, }\DecValTok{78}\NormalTok{, }\DecValTok{43}\NormalTok{, }\DecValTok{76}\NormalTok{, }\DecValTok{72}\NormalTok{, }\DecValTok{61}\NormalTok{, }\DecValTok{63}\NormalTok{, }\DecValTok{77}\NormalTok{, }
      \DecValTok{85}\NormalTok{, }\DecValTok{31}\NormalTok{, }\DecValTok{79}\NormalTok{, }\DecValTok{92}\NormalTok{, }\DecValTok{76}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{78}\NormalTok{, }\DecValTok{103}\NormalTok{, }\DecValTok{99}\NormalTok{, }\DecValTok{73}\NormalTok{, }\DecValTok{88}\NormalTok{, }\DecValTok{64}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{71}\NormalTok{, }\DecValTok{41}\NormalTok{, }\DecValTok{85}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{68}\NormalTok{, }
      \DecValTok{57}\NormalTok{, }\DecValTok{84}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{83}\NormalTok{, }\DecValTok{73}\NormalTok{, }\DecValTok{57}\NormalTok{, }\DecValTok{69}\NormalTok{, }\DecValTok{81}\NormalTok{, }\DecValTok{71}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{48}\NormalTok{, }\DecValTok{94}\NormalTok{, }\DecValTok{57}\NormalTok{, }\DecValTok{49}\NormalTok{, }\DecValTok{88}\NormalTok{, }\DecValTok{54}\NormalTok{, }\DecValTok{73}\NormalTok{, }
      \DecValTok{87}\NormalTok{, }\DecValTok{47}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{65}\NormalTok{, }\DecValTok{57}\NormalTok{, }\DecValTok{85}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{49}\NormalTok{, }\DecValTok{57}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{49}\NormalTok{, }\DecValTok{63}\NormalTok{, }\DecValTok{89}\NormalTok{, }
      \DecValTok{67}\NormalTok{, }\DecValTok{43}\NormalTok{, }\DecValTok{47}\NormalTok{, }\DecValTok{70}\NormalTok{, }\DecValTok{99}\NormalTok{, }\DecValTok{53}\NormalTok{, }\DecValTok{78}\NormalTok{, }\DecValTok{112}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{68}\NormalTok{, }\DecValTok{63}\NormalTok{, }\DecValTok{49}\NormalTok{, }\DecValTok{42}\NormalTok{, }
      \DecValTok{56}\NormalTok{, }\DecValTok{46}\NormalTok{, }\DecValTok{72}\NormalTok{, }\DecValTok{95}\NormalTok{, }\DecValTok{57}\NormalTok{, }\DecValTok{88}\NormalTok{, }\DecValTok{81}\NormalTok{, }\DecValTok{104}\NormalTok{, }\DecValTok{88}\NormalTok{, }\DecValTok{115}\NormalTok{, }\DecValTok{66}\NormalTok{, }\DecValTok{92}\NormalTok{, }\DecValTok{97}\NormalTok{, }\DecValTok{69}\NormalTok{, }\DecValTok{112}\NormalTok{, }\DecValTok{88}\NormalTok{)}

\NormalTok{X1 }\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{39}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{89}\NormalTok{, }\DecValTok{57}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{88}\NormalTok{, }\DecValTok{90}\NormalTok{, }\DecValTok{38}\NormalTok{, }\DecValTok{83}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{47}\NormalTok{, }\DecValTok{90}\NormalTok{, }\DecValTok{63}\NormalTok{, }\DecValTok{34}\NormalTok{, }
      \DecValTok{76}\NormalTok{, }\DecValTok{26}\NormalTok{,}\DecValTok{68}\NormalTok{,  }\DecValTok{85}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{82}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{81}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{72}\NormalTok{, }\DecValTok{46}\NormalTok{, }\DecValTok{63}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{77}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{60}\NormalTok{, }
      \DecValTok{33}\NormalTok{, }\DecValTok{49}\NormalTok{, }\DecValTok{89}\NormalTok{, }\DecValTok{72}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{73}\NormalTok{, }\DecValTok{58}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{90}\NormalTok{, }\DecValTok{48}\NormalTok{, }\DecValTok{47}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{63}\NormalTok{, }\DecValTok{76}\NormalTok{, }\DecValTok{79}\NormalTok{, }\DecValTok{48}\NormalTok{, }\DecValTok{90}\NormalTok{, }
      \DecValTok{55}\NormalTok{, }\DecValTok{83}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{44}\NormalTok{, }\DecValTok{79}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{55}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{46}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{47}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{61}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{68}\NormalTok{, }\DecValTok{80}\NormalTok{, }
      \DecValTok{43}\NormalTok{, }\DecValTok{53}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{63}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{82}\NormalTok{, }\DecValTok{88}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{69}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{59}\NormalTok{, }\DecValTok{53}\NormalTok{, }\DecValTok{65}\NormalTok{, }\DecValTok{90}\NormalTok{, }\DecValTok{88}\NormalTok{, }
      \DecValTok{66}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{48}\NormalTok{, }\DecValTok{82}\NormalTok{, }\DecValTok{88}\NormalTok{, }\DecValTok{63}\NormalTok{, }\DecValTok{79}\NormalTok{, }\DecValTok{71}\NormalTok{, }\DecValTok{66}\NormalTok{, }\DecValTok{81}\NormalTok{)}

\NormalTok{X2 }\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{31}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{41}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{41}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{33}\NormalTok{, }
      \DecValTok{13}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{28}\NormalTok{,}\DecValTok{12}\NormalTok{, }\DecValTok{57}\NormalTok{, }\DecValTok{51}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{57}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{30}\NormalTok{, }
      \DecValTok{64}\NormalTok{, }\DecValTok{31}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{44}\NormalTok{, }\DecValTok{57}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{24}\NormalTok{, }
      \DecValTok{21}\NormalTok{, }\DecValTok{31}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{31}\NormalTok{, }\DecValTok{65}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{30}\NormalTok{, }
      \DecValTok{22}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{49}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{38}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{56}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{14}\NormalTok{, }
      \DecValTok{41}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{49}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{38}\NormalTok{, }\DecValTok{48}\NormalTok{, }\DecValTok{66}\NormalTok{)}

\NormalTok{X3 }\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{18}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{23}\NormalTok{, }
      \DecValTok{18}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{15}\NormalTok{, }
      \DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{18}\NormalTok{, }
      \DecValTok{20}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{27}\NormalTok{, }
      \DecValTok{14}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }
      \DecValTok{17}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hfill\break

In this assignment, you are expected to replicate the analysis in case study \#1 in the weekly note. To be more specific, you can proceed with the analysis with the following steps.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Perform exploratory data analysis: pair-wise scatter plot. Describe the patterns you observed from the pair-wise plot.
\item
  Build an initial model that includes all variables and perform the residual analysis. Inspect the residual plot and describe potential abnormal patterns you observed from the residual plots.
\item
  Explore potential power transformation using Box-cox transformation. Use trial and error to identify an appropriate range so you can view the 95\% confidence interval for \(\lambda\). Please keep in mind that \(\lambda = 0\) implies log transformation.
\item
  Use the automatic variable selection to identify the \textbf{best} model.
\item
  Use the coefficient of determination to select the final model from the candidate models (initial, the \textbf{best} model from step 3.)
\item
  Interpret the final working model.
\end{enumerate}

\hypertarget{logistic-regression-models}{%
\chapter{Logistic Regression Models}\label{logistic-regression-models}}

Linear regression models are used to assess the association between the continuous response variable and other predictor variables. If the response variable is a binary categorical variable, the linear regression model is not appropriate. We need a new model, the logistic regression model, to assess the association between the binary response variable and other predictor variables.

This module focuses on the regression model with a binary response.

\hypertarget{motivational-example-and-practical-question}{%
\section{Motivational Example and Practical Question}\label{motivational-example-and-practical-question}}

\textbf{Example }: Longevity in male fruit flies is positively associated with adult size. However, reproduction has a high physiological cost that could impact longevity.

\begin{center}\includegraphics[width=0.8\linewidth]{img11/w11-FruitFliesImage} \end{center}

The original study looks at the association between longevity and adult size in male fruit flies kept under one of two conditions. One group is kept with sexually active females over the male's life span. The other group is cared for in the same way but kept with females who are not sexually active.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img11/w11-FruitFlies} 

}

\caption{Fruit Flies Data Table}\label{fig:unnamed-chunk-171}
\end{figure}

The above table gives the longevity in days for the male fruit flies allowed to reproduce (IndReprod = 0) and for those deprived of the opportunity (IndReprod = 1).

The data was collected from a case-control study design. The original study association analysis using the multiple linear regression model in which Longevity was a response variable and Thorax and IndReprod were used as predictor variables. In this example, we build a logistic regression to assess the association between longevity and reduction. Due to the case-control study design, the resulting logistic regression cannot be used as a predictive model.

\begin{figure}

{\centering \includegraphics{STA501EB_files/figure-latex/unnamed-chunk-173-1} 

}

\caption{The scatter plots of a binary response v.s. a continuous predictor variable}\label{fig:unnamed-chunk-173}
\end{figure}

Since the response variable is binary (i.e., it can only take on two distinct values, 0 and 1 in this example), the linear regression line is a bad choice since (1) the response variable is not continuous, (2) the fitted regression line can take on any values between negative infinity and positive infinity. The response variable takes on only either 0 or 1 (see the dark red straight line).

A meaningful approach to assess the association between the binary response variable and the predictor variable by looking at how the predictor variables impact the probability of observing the \textbf{bigger} value (i.e., the one that has a higher alphabetical order) of the response variable. The above \textbf{S} curve describes the relationship between the values of the predictor variable(s) and the probability of observing the \textbf{bigger} value of the response variable.

\hypertarget{logistic-regression-models-and-applications}{%
\section{Logistic Regression Models and Applications}\label{logistic-regression-models-and-applications}}

Logistic regression is a member of the generalized linear regression (GLM) family which includes linear regression models. The modeling-building process is the same as that in linear regression modeling.

\hypertarget{the-structure-of-logistic-regression-model}{%
\subsection{The Structure of Logistic Regression Model}\label{the-structure-of-logistic-regression-model}}

The actual probability function of observing the \textbf{bigger} value of the response variable for giving the predictor variable is given by

\[
P(Y=1) = \frac{\exp(\beta_0 + \beta_1 \text{Longevity})}{1 + \exp(\beta_0 + \beta_1 \text{Longevity})}
\]

If \(\beta_1\) (also called slope parameter) is equal to zero, the longevity and the reproduction status are NOT associated. Otherwise, there is an association between the response and the predictor variables. The sign of the slope parameter reflects the positive or negative association.

\hypertarget{assumptions-and-diagnostics}{%
\subsection{Assumptions and Diagnostics}\label{assumptions-and-diagnostics}}

There are assumptions for the binary logistic regression model.

\begin{itemize}
\item
  The response variable must be binary (taking on only two distinct values).
\item
  Predictor variables are assumed to be uncorrelated.
\item
  The functional form of the predictor variables is correctly specified.
\end{itemize}

The model diagnostics for logistic regression are much more complicated than in the linear regression models. We will not discuss this topic in this course.

\hypertarget{coefficient-estimation-and-interpretation}{%
\subsection{Coefficient Estimation and Interpretation}\label{coefficient-estimation-and-interpretation}}

The estimation of the logistic regression coefficients is not as intuitive as we saw in the linear regression model (regression lines and regression plane or surface). An advanced mathematical method needs to be used to estimate the regression coefficient. The R function \textbf{glm()} can be used to find the estimate of regression coefficients.

The interpretation of the coefficients of the logistic regression model is also not straightforward. In the motivational example, the value of \(\beta_1\) is the change of log odds of observing reproduction status to be 1. As usual, we will not make an inference of the intercept. In case-control data, the intercept is inestimable.

The output of \textbf{glm()} contains information similar to what has been seen in the output of \textbf{lm()} in the linear regression model.

\hypertarget{use-of-glm-and-annotations}{%
\subsection{\texorpdfstring{Use of \textbf{glm()} and Annotations}{Use of glm() and Annotations}}\label{use-of-glm-and-annotations}}

We use the motivational example to illustrate the setup of \textbf{glm()} and the interpretation of the output.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Fit the logistic regression}
\NormalTok{mymodel }\OtherTok{=}\FunctionTok{glm}\NormalTok{(IndReprod }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Longevity,   }\CommentTok{\# model formula (response on the left).}
          \AttributeTok{family=}\NormalTok{binomial,            }\CommentTok{\# must be binomial for the binary response!}
          \AttributeTok{data=}\NormalTok{fruitflies)            }\CommentTok{\# data frame name}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img11/w11-glm-output} 

}

\caption{The complete output of glm()}\label{fig:unnamed-chunk-175}
\end{figure}

The output has four major pieces of information: the model formula, the five-number-summary of the deviance residuals, significant test results of the predictor variables, and goodness-of-fit measures. In this course, we only focus on the significance tests.

\hypertarget{applications-of-logistic-regression-models}{%
\subsection{Applications of Logistic Regression Models}\label{applications-of-logistic-regression-models}}

Like other regression models, the logistic regression models have two basic applications: association analysis and prediction analysis.

The association analysis focuses on the interpretation of the regression coefficients that have information about whether predictor variables are associated with the response variable through the probability of the \textbf{bigger} value of the response variable.

Since the logistic regression builds the relationship between the probability of observing the \textbf{bigger} value of the response and the predictor variable, predicting the \textbf{value of the response variable} requires a cut-off probability to assign a value of 1 or 0 to the response variable. The prediction process of a logistic regression model is depicted in the following figure.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img11/w11-LogisticPrediction} 

}

\caption{Prediction process of the logistic regression models}\label{fig:unnamed-chunk-176}
\end{figure}

\hypertarget{case-studies-1}{%
\section{Case Studies}\label{case-studies-1}}

We present two examples in this section.

\hypertarget{the-simple-logistic-regression-model}{%
\subsection{The simple logistic regression model}\label{the-simple-logistic-regression-model}}

Suzuki et al.~(2006) measured sand grain size on 28 beaches in Japan and observed the presence or absence of the burrowing wolf spider Lycosa ishikariana on each beach. Sand grain size is a measurement variable, and spider presence or absence is a nominal variable. Spider presence or absence is the dependent variable; if there is a relationship between the two variables, it would be sand grain size affecting spiders, not the presence of spiders affecting the sand.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img11/w11-SpiderDataTable} 

}

\caption{Spider Data Table}\label{fig:unnamed-chunk-177}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grainsize}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.245}\NormalTok{, }\FloatTok{0.247}\NormalTok{, }\FloatTok{0.285}\NormalTok{, }\FloatTok{0.299}\NormalTok{, }\FloatTok{0.327}\NormalTok{, }\FloatTok{0.347}\NormalTok{, }\FloatTok{0.356}\NormalTok{, }\FloatTok{0.360}\NormalTok{, }\FloatTok{0.363}\NormalTok{, }\FloatTok{0.364}\NormalTok{, }
            \FloatTok{0.398}\NormalTok{, }\FloatTok{0.400}\NormalTok{, }\FloatTok{0.409}\NormalTok{, }\FloatTok{0.421}\NormalTok{, }\FloatTok{0.432}\NormalTok{, }\FloatTok{0.473}\NormalTok{, }\FloatTok{0.509}\NormalTok{, }\FloatTok{0.529}\NormalTok{, }\FloatTok{0.561}\NormalTok{, }\FloatTok{0.569}\NormalTok{, }
            \FloatTok{0.594}\NormalTok{, }\FloatTok{0.638}\NormalTok{, }\FloatTok{0.656}\NormalTok{, }\FloatTok{0.816}\NormalTok{, }\FloatTok{0.853}\NormalTok{, }\FloatTok{0.938}\NormalTok{, }\FloatTok{1.036}\NormalTok{, }\FloatTok{1.045}\NormalTok{)}
\NormalTok{status}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }
         \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{spider }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{grainsize =}\NormalTok{ grainsize, }\AttributeTok{status =}\NormalTok{ status))}
\end{Highlighting}
\end{Shaded}

\textbf{Fitting a Simple Logistic Regression Model}

Since there is only one predictor variable in this study, simply choose the simple linear regression model to this data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spider.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(status }\SpecialCharTok{\textasciitilde{}}\NormalTok{ grainsize, }
                   \AttributeTok{family =}\NormalTok{ binomial,}
                   \AttributeTok{data =}\NormalTok{ spider)}
\NormalTok{significant.tests }\OtherTok{=} \FunctionTok{summary}\NormalTok{(spider.model)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{pander}\NormalTok{(significant.tests, }\AttributeTok{caption =} \StringTok{"Summary of the significant tests of }
\StringTok{      the logistic regression model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2500}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1528}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1806}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1389}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1528}}@{}}
\caption{Summary of the significant tests of
the logistic regression model}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Std. Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
z value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pr(\textgreater\textbar z\textbar)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Std. Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
z value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pr(\textgreater\textbar z\textbar)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{(Intercept)} & -1.648 & 1.354 & -1.217 & 0.2237 \\
\textbf{grainsize} & 5.122 & 3.006 & 1.704 & 0.08844 \\
\end{longtable}

\textbf{Association Analysis}

The above significant tests indicate that the grain size does not achieve significance (p-value = 0.08844) at level 0.05. Note that the p-value is calculated based on the sample, it is also a random variable. Moreover, the sample size in this study is relatively small. We will claim the association between the two variables. As the grain size increases by one unit, the log odds of observing the wolf spider burrowing increase by 5.121553. In other words, the grain size and the presence of spiders are positively associated.

\textbf{Prediction Analysis}

As an example, we choose two new grain sizes 0.33 and 0.57, and want to predict the presence of the wide spiders on the beaches with the given grain sizes. We used the R function \textbf{predict()} in linear regression, we used the same function to predict the logistic regression model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spider.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(status }\SpecialCharTok{\textasciitilde{}}\NormalTok{ grainsize, }
                   \AttributeTok{family =}\NormalTok{ binomial,}
                   \AttributeTok{data =}\NormalTok{ spider)}
\DocumentationTok{\#\# }
\NormalTok{mynewdata }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{grainsize=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.275}\NormalTok{, }\FloatTok{0.57}\NormalTok{))}
\NormalTok{pred.prob }\OtherTok{=} \FunctionTok{predict}\NormalTok{(spider.model, }\AttributeTok{newdata =}\NormalTok{ mynewdata, }
        \AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\DocumentationTok{\#\# threshold probability}
\NormalTok{cut.off.prob }\OtherTok{=} \FloatTok{0.5}
\NormalTok{pred.response }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(pred.prob }\SpecialCharTok{\textgreater{}}\NormalTok{ cut.off.prob, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# predicts the response}
\NormalTok{pred.table }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{new.grain.size =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.275}\NormalTok{, }\FloatTok{0.57}\NormalTok{),}
                   \AttributeTok{pred.response =}\NormalTok{ pred.response)}
\FunctionTok{pander}\NormalTok{(pred.table, }\AttributeTok{caption =} \StringTok{"Predicted Value of response variable }
\StringTok{      with the given cut{-}off probability"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2361}}
  >{\centering\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2361}}@{}}
\caption{Predicted Value of response variable
with the given cut-off probability}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
new.grain.size
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
pred.response
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
new.grain.size
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
pred.response
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.275 & 0 \\
0.57 & 1 \\
\end{longtable}

\hypertarget{multiple-logistic-regression-model}{%
\subsection{Multiple Logistic Regression Model}\label{multiple-logistic-regression-model}}

In this case study, we used a published study on bird introduction in New Zealand. The objective is to predict the success of avian introduction to New Zealand. Detailed information about the study can be found in the following article. \url{https://stat501.s3.amazonaws.com/w11-Correlates-of-introduction-success-in-exotic.pdf}. The data is included in the article. A text format data file was created and can be downloaded or read directly from the following URL: \url{https://stat501.s3.amazonaws.com/w11-birds-data.txt}.

\begin{center}\includegraphics[width=0.8\linewidth]{img11/w11-BirdImage} \end{center}

The response variable: Status - status of success
Predictor variables:

\begin{itemize}
\tightlist
\item
  length - female body length (mm)
\item
  mass = female body mass (g)
\item
  range = geographic range (\% area of Australia)
\item
  migr = migration score: 1= sedentary, 2 = sedentary and migratory, 3 = migratory
\item
  insect = the number of months in a year with insects in the diet
\item
  diet = diet score: 1 = herbivorous; 2 = omnivorous, 3 = carnivorous.
\item
  clutch = clutch size
\item
  broods = number of broods per season
\item
  wood = use as woodland scored as frequent(1) or infrequent(2)
\item
  upland = use of the upland as frequent(1) or infrequent(2)
\item
  water = use of water scored as frequent(1) or infrequent(2)
\item
  release = minimum number of release events
\item
  indiv = minimum of the number of individuals introduced.
\end{itemize}

We next read the data from the given URL directly to R. Since there are some records with missing values. We drop those records with at least one missing value.

There are several categorical variables with numerical codings. Among them, \textbf{migr} and \textbf{diet} have three categories and the rest of the categorical variables have two categories. In practice, a \textbf{categorical variable with more than two categories must be specified as factor variables} so R can define dummy variables to capture the difference across the difference.

We conducted an exploratory analysis on \textbf{nigr} and \textbf{diet} and found a flat discrepancy across the effect.We simply treat them as a discrete numerical variable using the numerical coding as the values of the variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{birds }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/Data/w11{-}birds{-}data.txt"}
\NormalTok{NZbirds}\OtherTok{=}\FunctionTok{read.table}\NormalTok{(birds, }\AttributeTok{header=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{birds }\OtherTok{=} \FunctionTok{na.omit}\NormalTok{(NZbirds)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Build an Initial Model}
\end{itemize}

We first build a logistic regression model that contains all predictor variables in the data set. This model is usually called the full model. Note that the response variable is the success status (1 = success, 0 = failure). Species is a kind of ID, it should not be included in the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(status }\SpecialCharTok{\textasciitilde{}}\NormalTok{ length }\SpecialCharTok{+}\NormalTok{ mass }\SpecialCharTok{+}\NormalTok{ range }\SpecialCharTok{+}\NormalTok{ migr }\SpecialCharTok{+}\NormalTok{ insect }\SpecialCharTok{+}\NormalTok{ diet }\SpecialCharTok{+} 
\NormalTok{                      clutch }\SpecialCharTok{+}\NormalTok{ broods }\SpecialCharTok{+}\NormalTok{ wood }\SpecialCharTok{+}\NormalTok{ upland }\SpecialCharTok{+}\NormalTok{ water }\SpecialCharTok{+}\NormalTok{ release }\SpecialCharTok{+}\NormalTok{ indiv, }
                    \AttributeTok{family =}\NormalTok{ binomial, }\AttributeTok{data =}\NormalTok{ birds)}
\NormalTok{coefficient.table }\OtherTok{=} \FunctionTok{summary}\NormalTok{(initial.model)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{pander}\NormalTok{(coefficient.table, }\AttributeTok{caption =} \StringTok{"Significance tests of logistic regression model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2500}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1667}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1806}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1528}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1528}}@{}}
\caption{Significance tests of logistic regression model}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Std. Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
z value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pr(\textgreater\textbar z\textbar)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Std. Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
z value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pr(\textgreater\textbar z\textbar)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{(Intercept)} & -6.338 & 5.717 & -1.109 & 0.2676 \\
\textbf{length} & -0.002815 & 0.005317 & -0.5294 & 0.5965 \\
\textbf{mass} & 0.002668 & 0.001674 & 1.594 & 0.111 \\
\textbf{range} & -0.1316 & 0.3502 & -0.3758 & 0.7071 \\
\textbf{migr} & -2.044 & 1.116 & -1.831 & 0.06704 \\
\textbf{insect} & 0.148 & 0.2124 & 0.6969 & 0.4859 \\
\textbf{diet} & 2.029 & 1.883 & 1.077 & 0.2814 \\
\textbf{clutch} & 0.07938 & 0.2683 & 0.2959 & 0.7673 \\
\textbf{broods} & 0.02177 & 0.9283 & 0.02345 & 0.9813 \\
\textbf{wood} & 2.49 & 1.642 & 1.517 & 0.1293 \\
\textbf{upland} & -4.713 & 2.865 & -1.645 & 0.09991 \\
\textbf{water} & 0.2349 & 2.67 & 0.08799 & 0.9299 \\
\textbf{release} & -0.01292 & 0.1932 & -0.06685 & 0.9467 \\
\textbf{indiv} & 0.01593 & 0.008324 & 1.913 & 0.05571 \\
\end{longtable}

The p-values in the above significance test table are all bigger than 0.5. We next search for the best model by dropping some of the insignificant predictor variables. Since there are so many different ways to drop variables, next we use an automatic variable procedure to search the final model.

\begin{itemize}
\tightlist
\item
  \textbf{Automatic Variable Selection}
\end{itemize}

R has an automatic variable selection function \textbf{step()} for searching the final model. We will start from the initial model and drop insignificant variables using AIC as an inclusion/exclusion criterion.

In practice, sometimes, there may be some practically important predictor variables. Practitioners want to include these practically important variables in the model regardless of their statistical significance. Therefore, we can fit the smallest model that includes only those practically important variables. The final model should be \textbf{between} the smallest model, which we will call a \textbf{reduced model}, and the initial model, which we will call a \textbf{full model}. For illustration, we assume \textbf{insect} and \textbf{range} are practically important, we want to include these two variables in the final model regardless of their statistical significance.

In summary, we define two models: the full model and the reduced model. The final best model will be the model between the full and reduced models. The summary table of significant tests is given below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{full.model }\OtherTok{=}\NormalTok{ initial.model  }\CommentTok{\# the *biggest model* that includes all predictor variables}
\NormalTok{reduced.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(status }\SpecialCharTok{\textasciitilde{}}\NormalTok{ range }\SpecialCharTok{+}\NormalTok{ insect , }\AttributeTok{family =}\NormalTok{ binomial, }\AttributeTok{data =}\NormalTok{ birds)}
\NormalTok{final.model }\OtherTok{=}  \FunctionTok{step}\NormalTok{(full.model, }
                    \AttributeTok{scope=}\FunctionTok{list}\NormalTok{(}\AttributeTok{lower=}\FunctionTok{formula}\NormalTok{(reduced.model),}\AttributeTok{upper=}\FunctionTok{formula}\NormalTok{(full.model)),}
                    \AttributeTok{data =}\NormalTok{ birds, }
                    \AttributeTok{direction =} \StringTok{"backward"}\NormalTok{,}
                    \AttributeTok{trace =} \DecValTok{0}\NormalTok{)   }\CommentTok{\# trace = 0: suppress the detailed selection process}
\NormalTok{final.model.coef }\OtherTok{=} \FunctionTok{summary}\NormalTok{(final.model)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{pander}\NormalTok{(final.model.coef , }\AttributeTok{caption =} \StringTok{"Summary table of significant tests"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2500}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1806}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1806}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1806}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1806}}@{}}
\caption{Summary table of significant tests}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Std. Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
z value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pr(\textgreater\textbar z\textbar)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Std. Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
z value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pr(\textgreater\textbar z\textbar)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{(Intercept)} & -3.438 & 2.138 & -1.608 & 0.1079 \\
\textbf{mass} & 0.001939 & 0.0007326 & 2.647 & 0.008119 \\
\textbf{range} & 0.00001412 & 0.3098 & 0.00004558 & 1 \\
\textbf{migr} & -2.024 & 0.9603 & -2.108 & 0.03506 \\
\textbf{insect} & 0.2705 & 0.1426 & 1.897 & 0.05785 \\
\textbf{wood} & 1.949 & 1.317 & 1.479 & 0.139 \\
\textbf{upland} & -4.731 & 2.098 & -2.255 & 0.02415 \\
\textbf{indiv} & 0.01381 & 0.004058 & 3.404 & 0.0006648 \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  \textbf{Interpretation - Association Analysis}
\end{itemize}

The summary table contains the two practically important variables \textbf{range} and \textbf{insect}. \textbf{range} does not achieve statistical significance (p-value \(\approx\) 1) and \textbf{insect} is slightly higher than the significance level 0.005. Both variables are seemingly positively associated with the response variable.

The following interpretation of the individual predictor variable assumes that other life-history variables and introduction effort variables.

\textbf{migr} and \textbf{upland} are negatively associated with the response variable. The odds of success of introducing migratory birds are lower than the sedentary birds. Similarly, birds using upland infrequently have lower odds of being successfully introduced than those using upland frequently.

\textbf{insect} is significant (p-value =0.058). The \textbf{odds of success} increase as the number of months of having insects in diet increases.

\textbf{mass} and \textbf{indiv} are positively associated with the response variable. The odds of success increase and the body mass increases Similarly, the odds of success increase as the number of minimum birds of the species increases.

\textbf{wood} does not achieve statistical significance but seems to be positively associated with the response variable.

\begin{itemize}
\tightlist
\item
  \textbf{Predictive Analysis}
\end{itemize}

As an illustration, we use the final model to predict the status of successful introduction based on the new values of the predictor variables associated with two species. See the numerical feature given in the code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mynewdata }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{mass=}\FunctionTok{c}\NormalTok{(}\DecValTok{560}\NormalTok{, }\DecValTok{921}\NormalTok{),}
                       \AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.75}\NormalTok{, }\FloatTok{1.2}\NormalTok{),}
                       \AttributeTok{migr =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                       \AttributeTok{insect =} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{12}\NormalTok{),}
                       \AttributeTok{wood =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                       \AttributeTok{upland =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                       \AttributeTok{indiv =} \FunctionTok{c}\NormalTok{(}\DecValTok{123}\NormalTok{, }\DecValTok{571}\NormalTok{))}
\NormalTok{pred.cuccess.prob }\OtherTok{=} \FunctionTok{predict}\NormalTok{(final.model, }\AttributeTok{newdata =}\NormalTok{ mynewdata, }\AttributeTok{type=}\StringTok{"response"}\NormalTok{)}
\DocumentationTok{\#\#}
\DocumentationTok{\#\# threshold probability}
\NormalTok{cut.off.prob }\OtherTok{=} \FloatTok{0.5}
\NormalTok{pred.response }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(pred.cuccess.prob }\SpecialCharTok{\textgreater{}}\NormalTok{ cut.off.prob, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}\CommentTok{\# predicts response}
\DocumentationTok{\#\# add the new predicted response to Mynewdata}
\NormalTok{mynewdata}\SpecialCharTok{$}\NormalTok{Pred.Response }\OtherTok{=}\NormalTok{ pred.response}
\DocumentationTok{\#\#}
\FunctionTok{pander}\NormalTok{(mynewdata, }\AttributeTok{caption =} \StringTok{"Predicted Value of response variable }
\StringTok{      with the given cut{-}off probability"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0972}}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1111}}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0972}}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0972}}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1111}}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.2222}}@{}}
\caption{Predicted Value of response variable
with the given cut-off probability}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
mass
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
range
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
migr
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
insect
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
wood
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
upland
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
indiv
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pred.Response
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
mass
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
range
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
migr
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
insect
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
wood
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
upland
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
indiv
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pred.Response
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
560 & 0.75 & 2 & 6 & 1 & 0 & 123 & 0 \\
921 & 1.2 & 1 & 12 & 1 & 1 & 571 & 1 \\
\end{longtable}

The predicted status of the successful introduction of the two species is attached to the two new data records.

\hypertarget{practice-problems-3}{%
\section{Practice Problems}\label{practice-problems-3}}

Framingham Heart Study (FHS), a long-term research project developed to identify risk factors of cardiovascular disease, the findings of which had far-reaching impacts on medicine. Indeed, much common knowledge about heart disease---including the effects of smoking, diet, and exercise---can be traced to the Framingham study. The study's findings further emphasized the need for preventing, detecting, and treating risk factors of cardiovascular disease in their earliest stages

The dataset is a rather small subset of possible FHS datasets, having 4240 observations and 16 variables. The variables are as follows:

\begin{itemize}
\tightlist
\item
  sex : the gender of the observations. The variable is a binary named ``male'' in the dataset.
\item
  age : Age at the time of medical examination in years.
\item
  education : A categorical variable of the participants' education, with the levels: Some high school (1), high school/GED (2), some college/vocational school (3), college (4) - caution: This is a numerically coded categorical variable, we need to use the form of factor(education) in the model formulas.
\item
  currentSmoker: Current cigarette smoking at the time of examinations
\item
  cigsPerDay: Number of cigarettes smoked each day
\item
  BPmeds: Use of Anti-hypertensive medication at exam
\item
  prevalentStroke: Prevalent Stroke (0 = free of disease)
\item
  prevalentHyp: Prevalent Hypertensive. The subject was defined as hypertensive if treated
\item
  diabetes: Diabetic according to criteria of the first exam treated
\item
  totChol: Total cholesterol (mg/dL)
\item
  sysBP: Systolic Blood Pressure (mmHg)
\item
  diaBP: Diastolic blood pressure (mmHg)
\item
  BMI: Body Mass Index, weight (kg)/height (m)\^{}2
\item
  heartRate: Heart rate (beats/minute)
\item
  glucose: Blood glucose level (mg/dL)
\end{itemize}

And finally the response variable:

\begin{itemize}
\tightlist
\item
  TenYearCHD: The 10-year risk of coronary heart disease(CHD).
\end{itemize}

3658 of these 4240 records are complete cases, and the rest have some missing values. The following code chunk will clean the data and only keep the complete records in the final data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fhs }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/Data/w11{-}FraminghamCHD.csv"}
\NormalTok{FHS.data }\OtherTok{=} \FunctionTok{na.omit}\NormalTok{(}\FunctionTok{read.csv}\NormalTok{(fhs))}
\end{Highlighting}
\end{Shaded}

The above \textbf{FHS.data} has 3658 complete records and 16 variables listed above. The goals of this assignment are both \textbf{association analysis} and \textbf{predictive analysis}. To be more specific, you are expected to follow the analysis in Section 4.2 in the class note.

\begin{itemize}
\tightlist
\item
  Use the above FHS.data to fit an initial model that includes all predictor variables.
\item
  BMI and glucose are two clinically important variables. Both variables will be included in the final model. That is, you can define the smallest model using the two important clinical variables.
\item
  Using an automatic variable selection procedure to identify the final model working model.
\item
  Interpret the regression model coefficients. You don't have to interpret all coefficients. Pick two coefficients associated with categorical and numerical predictor variables, respectively, to interpret.
\item
  Assume there are two incoming patients with the following demographics and clinical characteristics:
\end{itemize}

\begin{center}\includegraphics[width=0.9\linewidth]{img11/w11-AssignPredNewData} \end{center}

\hypertarget{contingency-table-analysis}{%
\chapter{Contingency Table Analysis}\label{contingency-table-analysis}}

We have introduced different modeling to solve the association and prediction analyses. The ANOVA is used when the response is a continuous normal random variable and the predictor variable is categorical. When the response variable is a continuous normal random variable and is also a continuous (non-random) variable, we use the simple linear regression model to address the associated problems. When predictor variables are hybrid (continuous and categorical), we have a multiple linear regression model (numerically coded categorical variables need to be specified during the modeling process). When the response variable is binary, we use logistic regression to study the association between the response and predictor variables.

The following table summarizes different models for different situations.

\begin{longtable}[]{@{}ccc@{}}
\toprule\noalign{}
Response variable & Predictor variable & Type of Models \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
continuous, normal & single categorical & ANOVA \\
continuous, normal & single continuous & SLR \\
continuous, normal & continuous or categorical & MLR \\
binary, categorical & continuous or categorical & logistic model \\
\end{longtable}

\begin{itemize}
\item
  \textbf{ANOVA}: Analysis of variance model
\item
  \textbf{SLR}: Simple linear regression model
\item
  \textbf{MLR}: multiple linear regression model, ANOVA is a special MLR. MLR with both categorical and numerical predictor variables is also an Analysis of covariance (ANCOVA).
\item
  \textbf{The logistic model} is also called the \textbf{logit model}.
\end{itemize}

We also discussed the relationship between two numerical variables using the Pearson correlation coefficients. When we use correlation coefficients to measure the strength of the linear correlation between two numerical variables, we don't specify the response variable.

In this note, we will discuss the association between two categorical variables. We first assess the association through \(\chi^2\) test of independence of the two variables. If they are dependent, we then define measures for the association.

\hypertarget{the-motivational-examples}{%
\section{The Motivational Examples}\label{the-motivational-examples}}

\textbf{Example 1}: A biologist might want to determine if two species of organisms associate (are found together) in a vegetation community. Two hypotheses could be set up in the following:

\textbf{Null Hypothesis (Ho)}: There is no significant association between Species A and Species B; the species are independent of each other. The location of Species A does not affect the location of Species B.

\textbf{Alternative Hypothesis (Ha)}: There is a significant association between Species A and Species B; the species are dependent. Either Species A significantly associates with Species B or Species A does not significantly associate with Species B.

\begin{center}\includegraphics[width=0.8\linewidth]{img12/w12-QuadrantSurvey} \end{center}

To test the hypotheses, we can use the quadrant random sampling method to collect the raw data. Assume we collect data in 9 randomly placed quadrants and classify the survey outcomes in the following:

\begin{itemize}
\tightlist
\item
  The number of quadrants with both species present
\item
  The number of quadrants with Species A but not Species B
\item
  The number of quadrants with Species B but not Species A
\item
  The number of quadrants with neither species
\end{itemize}

The above classification is summarized in the following table.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3472}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1528}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
..
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Presence of A
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Absence of A
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Total}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Presence of B} & 5 & 2 & \textbf{7} \\
\textbf{Absence of B} & 1 & 1 & \textbf{2} \\
\textbf{Total} & \textbf{6} & \textbf{3} & \textbf{9} \\
\end{longtable}

The above table is called the contingency table. Since this table has two rows and two columns, it is also called 2 by 2 contingency table.

In practice, we have a more general m-by-n contingency table - this is called the two-way table. This week, we focus on analyzing two-way tables.

\textbf{Example 2}: Arrington et al.~(2002) examined the frequency with which African, Neotropical and North American fishes have empty stomachs and found that the mean percentage of empty stomachs was around 16.2\%. As part of the investigation, they were interested in \textbf{whether the frequency of empty stomachs was related to dietary items}. The data were separated into four major trophic classifications (detritivores, omnivores, invertivores, and piscivores) and whether the fish species had greater or less than 16.2\% of individuals with empty stomachs. The number of fish species in each category combination was calculated and a subset of that (just the diurnal fish) was provided.

\textbf{STOMACH} Categorical listing of the proportion of individuals in the species with empty stomachs (\textless{} 16.2\% or \textgreater{} 16.2\%).

\textbf{TROPHIC} Categorical listing of the trophic classification (DET = detritovore, OMN = omnivore, INV = invertivore, PISC = piscivore).

Then the above information is summarized in the following table.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2841}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3068}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2955}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1136}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Trophic classification
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Stomachs empty (\textless{} 16.2\%)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Stomachs empty (\textgreater16.2\%)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Total
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
DET & 18 & 4 & \textbf{22} \\
OMN & 45 & 8 & \textbf{53} \\
INV & 58 & 15 & \textbf{73} \\
PISC & 16 & 34 & \textbf{50} \\
\textbf{Total} & \textbf{137} & \textbf{61} & \textbf{198} \\
\end{longtable}

\hypertarget{two-way-contingency-tables-and-analysis}{%
\section{Two-way Contingency Tables and Analysis}\label{two-way-contingency-tables-and-analysis}}

A k-way contingency table is used to summarize the relationship between k categorical variables. It is a special type of frequency distribution table that k variables are shown simultaneously.

In this note, We will introduce the general structure of two-way contingency tables and statistical tools for analyzing these tables. We will also introduce different study designs that generate two-way contingency tables containing different amounts of information.

\hypertarget{the-structure-of-two-way-contingency-table}{%
\subsection{The Structure of Two-way Contingency Table}\label{the-structure-of-two-way-contingency-table}}

Let \(X\) and \(Y\) be two categorical variables. \(Y\) is usually random (except in a case-control study) and is also called the response variable; \(X\) can be random or fixed, and usually acts as a predictor variable. Assume that \(X\) has \(I\) levels and \(Y\) has \(J\) levels. For ease of illustration, we consider the \(4\times3\) contingency table. The general structure of this \textbf{observed} two-way contingency table (also called \(4\times3\) table) is given by

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img12/w12-ObservedTable} 

}

\caption{Observed two-way table based on a random sample}\label{fig:unnamed-chunk-190}
\end{figure}

Where the figures in the bottom and right-most column are called marginal totals. For examples,

\begin{itemize}
\item
  \(n_{+ 2} = n_{12} + n_{22} + n_{32} + n_{42}\) is the column total of the second column.
\item
  \(n_{2+} = n_{21} + n_{22} + n_{23}\) is the row total of the second row.
\item
  \(n_{++} =\) total of all frequencies in the table. This is also commonly called the grand total.
\end{itemize}

If the grand total is a simple random sample from a population, then we can easily turn the above-observed frequency table to an estimated \textbf{true joint probability distribution table} of \(X\) and \(Y\) that is below.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img12/w12-ProbTable} 

}

\caption{The true joint probability distribution of two categorical variables}\label{fig:unnamed-chunk-191}
\end{figure}

For illustration, we look at a few examples in the following.

\begin{itemize}
\item
  \textbf{Joint Probability}: \(\hat{P}(X=2, Y=3) = \hat{p}_{23} = n_{23}/n_{++}\) is probability of observing \(Y=j\) and \(X=2\). We can also estimate the marginal probability from the above model.
\item
  \textbf{Marginal Probability}: \(\hat{P}(Y=3) = \hat{p}_{+3} = n_{+ 3}/n_{++}\) is the probablity of observing \(Y=3\). The probability of observing \(X=2\) is estimated by \(\hat{P}(X=2) = \hat{p}_{2+} = n_{2+}/n_{++}\) is the probability of observing \(X = 2\).
\end{itemize}

This means that we can estimate the \textbf{true joint distribution of \(X\) and \(Y\)} by using the observed table

\hypertarget{pearson-chi2-test-of-independence}{%
\subsection{\texorpdfstring{Pearson \(\chi^2\) Test of Independence}{Pearson \textbackslash chi\^{}2 Test of Independence}}\label{pearson-chi2-test-of-independence}}

As mentioned earlier, the null hypothesis of the Pearson \(\chi^2\) test of independence is the two categorical variables are independent. Since the general relationship between \(X\) and \(Y\) is completely determined by the above \textbf{joint probability distribution table}.

\textbf{Under the null hypothesis \(H_o\)}, the \textbf{joint probability distribution table of \(X\) and \(Y\)} has the following special structure.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img12/w12-NULLProbTable} 

}

\caption{The true joint probability distribution of two categorical variables}\label{fig:unnamed-chunk-192}
\end{figure}

This implies that, under the null hypothesis, the \textbf{estimated frequency} of any given cell, say (\(X=2,Y=3\)), is given by \(e_{23} = n_{++} \times \hat{p}_{2+}\times\hat{p}_{+3} = n_{2+}n_{+3}/n_{++}\). We can use the same formula to calculate all \textbf{estimated frequencies} - we call it the estimated table.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img12/w12-ExpectedTable} 

}

\caption{The true joint probability distribution of two categorical variables}\label{fig:unnamed-chunk-193}
\end{figure}

In other words, if the \textbf{observed} and \textbf{estimated} tables are close to each other, we intend to conclude the null hypothesis.

To answer how close is close, we define a \textbf{statistical distance} between the \textbf{observed} and \textbf{expected} tables as follows

\[
TS = \sum_{i=1}^4\sum_{j=1}^2 \left(\frac{n_{ij}-e_{ij}}{\sqrt{e_{ij}}} \right)^2 \to \chi^2_{(4-1)(2-1)}
\]

where \((4-1)(2-1)\) is the degrees of freedom of \(\chi^2_3\) distribution based on the above \(4\times2\) contingency table. In general, for an \(I\times J\) contingency table, the degrees of freedom are defined to be \((I-1)(J-1)\).

\(\chi^2\) is a distribution of a positive random variable such as the \textbf{distance} between \textbf{observed} and \textbf{estimated} tables. Different degrees of freedom define different \(\chi^2\) distributions. The following curve is a specific \(\chi^2\) distribution

\begin{figure}

{\centering \includegraphics{STA501EB_files/figure-latex/unnamed-chunk-194-1} 

}

\caption{Density curve of a chi-sqaure distribution}\label{fig:unnamed-chunk-194}
\end{figure}

Similar to normal and t-distributions, there are also R functions to find quantiles and tail probabilities.

\begin{verbatim}
pchisq(quantile, df)   # left-tail area of the chi-square distribution. 
qchisq(left.tail.area, df) # quantile of the chi-square distribution. 
\end{verbatim}

The chi-squared test is essentially always right-tailed since the chi-squared test measures the discrepancy between two distributions. A large \textbf{distance} implies rejecting the null hypothesis. Therefore, the rejection is always on the right tail of the \(\chi^2\) density curve.

\textbf{Example 2} (continued):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/ref/w12{-}table2x2Calculator.txt"}\NormalTok{)}
\DocumentationTok{\#\# define column vector}
\NormalTok{less.}\FloatTok{16.2} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{18}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{58}\NormalTok{, }\DecValTok{16}\NormalTok{)}
\NormalTok{bigger.}\FloatTok{16.2} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{15}\NormalTok{,}\DecValTok{34}\NormalTok{)}
\NormalTok{cont.table }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{less.16.2 =}\NormalTok{ less.}\FloatTok{16.2}\NormalTok{ , }\AttributeTok{bigger.16.2 =}\NormalTok{ bigger.}\FloatTok{16.2}\NormalTok{)}
\NormalTok{chi.test }\OtherTok{=} \FunctionTok{Pearson.chisq}\NormalTok{(cont.table)}\SpecialCharTok{$}\NormalTok{inference}
\FunctionTok{kable}\NormalTok{(chi.test, }\AttributeTok{caption=}\StringTok{"Pearson chi{-}square test of independence"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-195}Pearson chi-square test of independence}
\centering
\begin{tabular}[t]{l|l|l|l|l}
\hline
  & ts.stats & p.value & d.f & method\\
\hline
 & 43.8346 & 0 & 3 & Pearson's Chi-squared test\\
\hline
\end{tabular}
\end{table}

The small p-value implies we rejected the null hypothesis. Therefore, the frequency of empty stomachs was related to dietary items. We can also extract the expected frequency table from the above \(\chi^2\) test

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exp }\OtherTok{=} \FunctionTok{Pearson.chisq}\NormalTok{(cont.table)}\SpecialCharTok{$}\NormalTok{expected}
\FunctionTok{row.names}\NormalTok{(exp)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{"DET"}\NormalTok{, }\StringTok{"OMN"}\NormalTok{, }\StringTok{"INV"}\NormalTok{, }\StringTok{"PISC"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(exp, }\AttributeTok{caption =} \StringTok{"The expected table under the null hypothesis"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-196}The expected table under the null hypothesis}
\centering
\begin{tabular}[t]{l|r|r}
\hline
  & less.16.2 & bigger.16.2\\
\hline
DET & 15.22222 & 6.777778\\
\hline
OMN & 36.67172 & 16.328283\\
\hline
INV & 50.51010 & 22.489899\\
\hline
PISC & 34.59596 & 15.404040\\
\hline
\end{tabular}
\end{table}

The original observed table can also be retrieved from the above \(\chi^2\) test as follows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{obs }\OtherTok{=} \FunctionTok{Pearson.chisq}\NormalTok{(cont.table)}\SpecialCharTok{$}\NormalTok{observed}
\FunctionTok{row.names}\NormalTok{(obs) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"DET"}\NormalTok{, }\StringTok{"OMN"}\NormalTok{, }\StringTok{"INV"}\NormalTok{, }\StringTok{"PISC"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(obs, }\AttributeTok{caption =} \StringTok{"The original observed frequency table"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-197}The original observed frequency table}
\centering
\begin{tabular}[t]{l|r|r}
\hline
  & less.16.2 & bigger.16.2\\
\hline
DET & 18 & 4\\
\hline
OMN & 45 & 8\\
\hline
INV & 58 & 15\\
\hline
PISC & 16 & 34\\
\hline
\end{tabular}
\end{table}

\newpage

\textbf{Caution of Using Pearson \(\chi^2\) Test}: The Pearson \(\chi^2\) test assumes that the sample size must be large. The test result is not reliable if one or more of the cells of the contingency table is less than or equal to 5.

\hypertarget{measures-of-association}{%
\section{Measures of Association}\label{measures-of-association}}

Depending on the structure of the contingency table, we can define different measures for the association between the two categorical variables. In this section, we restrict our discussion to \(2\times2\) tables. The general two-way or three-way tables are more complex and will not be discussed in this note.

A \(2\times2\) table can be obtained from different study designs, hence, has a different amount of information.

\hypertarget{times2-table-under-different-study-designs}{%
\subsection{\texorpdfstring{\(2\times2\)-table Under Different Study Designs}{2\textbackslash times2-table Under Different Study Designs}}\label{times2-table-under-different-study-designs}}

We use the following table as a template to discuss the different amount of information in the \(2\times2\) tables based on the different study designs.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img12/w12-Typical2x2table} 

}

\caption{The true joint probability distribution of two categorical variables}\label{fig:unnamed-chunk-198}
\end{figure}

We can think about \textbf{Characteristic B} to be the status of a disease and \textbf{Characteristic A} as the status of exposure to some risk.

\begin{itemize}
\tightlist
\item
  \textbf{Cross-sectional Study Design}
\end{itemize}

This method assumes a random sample of n subjects from a large population \textbf{followed} by the determination for each subject of the presence or absence of characteristic A and the presence or absence of characteristic B. \textbf{Only the total sample size n can be specified before the collection of the data}.

\begin{itemize}
\tightlist
\item
  \textbf{Stratified Sampling Design}
\end{itemize}

The stratified sampling (also called purposive sampling) assumes that the original population is stratified by either \textbf{characteristic A} or \textbf{characteristic B}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Stratification with Characteristic B}. This design stratifies the population into exposure and non-exposure sub-populations. Two simple random samples were taken independently from the exposure and non-exposure populations respectively. This means that \(n_{1+}\) and \(n_{2+}\) are pre-determined sample sizes - the typical follow-up study is an example of this design.
\item
  \emph{Stratification with Characteristic A}. This design stratifies the population into diseased and disease-free sub-populations. Two simple random samples were taken independently from the diseased and disease-free populations respectively. This means that \(n_{+1}\) and \(n_{+2}\) are pre-determined sample sizes - the typical case-control study is an example of this design.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Randomized Clinical Trials}
\end{itemize}

Of a total of \(n_{++}\) subjects, \(n_{1+}\) are selected at random to be treated with the control treatment (placebo), and the remaining \(n_{2+}\) to be treated with test treatment.

\hypertarget{measures-of-association-1}{%
\subsection{Measures of Association}\label{measures-of-association-1}}

We now define several measures of association. The inference of all three measures of association assumes that the sample size is large. \textbf{By convention, each cell of the observed frequency table must be bigger than or equal to 5 in order to yield reliable results}. For example, motivational example 1 has several small cells. Pearson \(\chi^2\) test and inference on measures of association are reliable. In order to perform the test of independence, we need to increase the sample size until all cell sizes are bigger than or equal to 5.

\begin{itemize}
\tightlist
\item
  \textbf{Relative Risk (also Risk Ratio)}: is defined to be the ratio of proportions of disease in exposure and non-exposure populations.
\end{itemize}

The relative risk (RR) is estimated by

\[
\widehat{RR} = \frac{n_{11}/n_{1+}}{n_{21}/n_{2+}}
\]
Note that the definition of RR requires fixed \(n_{1+}\) and \(n_{2+}\). If \(RR = 1\), there is no association between characteristics A and B. We will use the R program to calculate the confidence interval.

\begin{itemize}
\tightlist
\item
  \textbf{Risk Difference} is defined to be the difference of proportions of disease of exposure and non-exposure populations.
\end{itemize}

The risk difference (RD) can be estimated by

\[
RD = n_{11}/n_{1+} - n_{21}/n_{2+}
\]

Note also that the definition of RD requires fixed \(n_{1+}\) and \(n_{2+}\). If \(RD = 0\), there is no association between characteristics A and B.

\begin{itemize}
\tightlist
\item
  The \textbf{odds} of disease is the ratio of the probability of being disease divided by the probability of being disease-free.
\end{itemize}

For example, we can define odds of disease in exposure population to be \(O(D+)_{T+} = [n_{11}/n_{1+}/[n_{12}/n_{1+}] = n_{11}/n_{12}\), and \(O(D+)_{T-} = [n_{21}/n_{2+}/[n_{22}/n_{2+}] = n_{21}/n_{22}\) for the non-exposure population. \textbf{Note that the odds of an event are not a probability since it can be bigger than 1}.

From the definition of the odds in the above examples, we don't require fixed marginal totals.

\begin{itemize}
\tightlist
\item
  The \textbf{odds Ratio} of a disease is the ratio of the odds of disease in the exposed population and the odds of disease in the non-exposed population.
\end{itemize}

For example, \(OR(disease) = O(D+)_{T+}/O(D+)_{T-} = n_{11}n_{22}/n_{12}n_{21}\). From this definition, we can see that there is no requirement for the marginal totals.

\hypertarget{validity-of-measures-of-association}{%
\subsection{Validity of Measures of Association}\label{validity-of-measures-of-association}}

We have defined several commonly used measures of association for \(2\times2\) tables. Since the definition of different measures requires to have different fixed marginal totals. Therefore, one that is valid under one design might not be valid for another study design. The following is the summary of the validity of measures of association.

\begin{itemize}
\item
  Under the \textbf{cross-sectional design}, all measures of association (RD, RR, and OR) are valid.
\item
  Under the \textbf{case-control study design} (fixed column totals), relative risk (RR) and risk difference (RD) are \textbf{invalid} since their definitions require fixed marginal totals of both exposure and non-exposure populations. The odds ratio (OR) is valid in the case-control study.
\item
  Under the \textbf{follow-up study design} (fixed row totals), all three measures (RR, RD, and OR) are valid.
\item
  Under the \textbf{randomized clinical trial (RCT)}, all three measures (RR, RD, and OR) are valid since the RCT is the combination of random sampling and (random) stratification for follow-up.
\end{itemize}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3095}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2262}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2381}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2262}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Study Design
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Relative Risk
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Risk Difference
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Odds Ratio
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
cross-section & Valid & valid & valid \\
Stratified follow-up & valid & valid & valid \\
Stratified case-control & invalid & invalid & valid \\
RCT & valid & valid & valid \\
\end{longtable}

We will present examples in the next section of case studies using the two R functions \textbf{Pearson.chisq()} and \textbf{table2x2()} that can be sourced at \href{source(\%22https://raw.githubusercontent.com/pengdsci/STA501/main/ref/w12-table2x2Calculator.txt\%22)}{source(``https://raw.githubusercontent.com/pengdsci/STA501/main/ref/w12-table2x2Calculator.txt'')}. Whenever you use the function for the first time in the current version, you simply place the one-line code \textbf{source(``source(''\url{https://raw.githubusercontent.com/pengdsci/STA501/main/ref/w12-table2x2Calculator.txt}``)'')} in the code chunk before calling the above two R functions. In the rest of the current session, you don't need to source the code!

\hypertarget{case-studies-2}{%
\section{Case Studies}\label{case-studies-2}}

When conducting the independence test, we need to check the assumption of sample size in order to make the correct decision. For the analyzing \(2\times2\) table, we need to know the study designs since some measures may not be valid for all designs.

\hypertarget{case-study-1-1}{%
\subsection{Case Study 1}\label{case-study-1-1}}

\textbf{Example 1}. In 1992, the U.S. Public Health Service and the Centers for Disease Control and Prevention recommended that all women of childbearing age consume 400mg of folic acid daily to reduce the risk of having a pregnancy that is affected by a neural tube defect such as spina bifida or anencephaly. In a study by Stepanuk et al, 693 pregnant women called a teratology information service about their use of folic acid supplementation. The researchers wished to determine if preconceptional use of folic acid and race are independent. The data appear in the following table.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img12/w12-casestusy01} 

}

\caption{Use of folic acid supplementation data table}\label{fig:unnamed-chunk-199}
\end{figure}

We use the R function \textbf{Pearson.chisq()} to test the independence between the race and the use of folic acid.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/ref/w12{-}table2x2Calculator.txt"}\NormalTok{)}
\DocumentationTok{\#\# 2{-}by{-}2 table must be defined as a data frame}
\NormalTok{case1 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Yes=}\FunctionTok{c}\NormalTok{(}\DecValTok{260}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{7}\NormalTok{),}\AttributeTok{No=}\FunctionTok{c}\NormalTok{(}\DecValTok{299}\NormalTok{, }\DecValTok{41}\NormalTok{,}\DecValTok{14}\NormalTok{))}
\FunctionTok{rownames}\NormalTok{(case1) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"white"}\NormalTok{,}\StringTok{"Black"}\NormalTok{, }\StringTok{"Other"}\NormalTok{)}
\NormalTok{Pearson.chi.test }\OtherTok{=} \FunctionTok{Pearson.chisq}\NormalTok{(case1)}
\DocumentationTok{\#\#}
\FunctionTok{pander}\NormalTok{(Pearson.chi.test}\SpecialCharTok{$}\NormalTok{inference, }\AttributeTok{caption=}\StringTok{"Summary of Pearson chi{-}square test of independence"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1528}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1389}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0833}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4028}}@{}}
\caption{Summary of Pearson chi-square test of independence}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
ts.stats
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
p.value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
d.f
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
method
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
ts.stats
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
p.value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
d.f
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
method
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
9.0913 & 0.0106 & 2 & Pearson's Chi-squared test \\
\end{longtable}

The above Pearson \(\chi^2\) test independence indicates that the pre-conceptional use of folic acid and race are dependent with \(\chi_2^2 = 9.0913\) with p-value = 0.0106.

To compare the observed frequency table and the expected frequency table, we can extract them from the above \textbf{Pearson.chisq()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pander}\NormalTok{(Pearson.chi.test}\SpecialCharTok{$}\NormalTok{observed, }\AttributeTok{caption =} \StringTok{"The observed frequency table"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1667}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0833}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0833}}@{}}
\caption{The observed frequency table}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Yes
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
No
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Yes
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
No
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{white} & 260 & 299 \\
\textbf{Black} & 15 & 41 \\
\textbf{Other} & 7 & 14 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pander}\NormalTok{(Pearson.chi.test}\SpecialCharTok{$}\NormalTok{expected, }\AttributeTok{caption =} \StringTok{"The expected frequency table"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1667}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1111}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1111}}@{}}
\caption{The expected frequency table}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Yes
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
No
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Yes
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
No
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{white} & 247.9 & 311.1 \\
\textbf{Black} & 24.83 & 31.17 \\
\textbf{Other} & 9.311 & 11.69 \\
\end{longtable}

We can see that the above two tables are different. The second row of the expected table is significantly different from that of the observed table.

\hypertarget{case-study-2-1}{%
\subsection{Case Study 2}\label{case-study-2-1}}

\textbf{Example 2}: The food-frequency questionnaire is widely used to measure dietary intake. A person specifies the number of servings consumed per day of each of many different food items. The total nutrient composition is then calculated from the specific dietary components of each food item. One way to judge how well a questionnaire measures dietary intake is by its reproducibility. To assess reproducibility the questionnaire is administered at two different times to 50 people and the reported nutrient intakes from the two questionnaires are compared. Suppose dietary cholesterol is quantified on each questionnaire as high if it exceeds 300 mg/day and as normal otherwise. The contingency table in the following table is a natural way to compare the results of the two surveys. Notice that this example has no natural denominator. We simply want to test whether there is some association between the two reported measures of dietary cholesterol for the same person. More specifically, we want to assess how unlikely it is that 15 women will report high dietary cholesterol intake on both questionnaires, given that 20 of 50 women report high intake on the first questionnaire and 24 of 50 women report high intake on the second questionnaire. This test is called a test of independence or a test of association between the two characteristics.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img12/w12-casestudy02} 

}

\caption{ dietary intake data table}\label{fig:unnamed-chunk-203}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/ref/w12{-}table2x2Calculator.txt"}\NormalTok{)}
\DocumentationTok{\#\# 2{-}by{-}2 table must be defined as a data frame}
\NormalTok{case2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{High=}\FunctionTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{9}\NormalTok{),}\AttributeTok{Normal=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{21}\NormalTok{))}
\FunctionTok{rownames}\NormalTok{(case2) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"High"}\NormalTok{,}\StringTok{"Normal"}\NormalTok{)}
\NormalTok{Pearson.chi.test02 }\OtherTok{=} \FunctionTok{Pearson.chisq}\NormalTok{(case2)}
\DocumentationTok{\#\#}
\FunctionTok{pander}\NormalTok{(Pearson.chi.test02}\SpecialCharTok{$}\NormalTok{inference, }\AttributeTok{caption=}\StringTok{"Summary of Pearson chi{-}square }
\StringTok{      test of independence"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1528}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1389}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0833}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3333}}@{}}
\caption{Summary of Pearson chi-square
test of independence}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
ts.stats
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
p.value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
d.f
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
method
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
ts.stats
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
p.value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
d.f
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
method
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
8.0162 & 0.0046 & 1 & Pearson's Chi-squared
test(with correction) \\
\end{longtable}

The Pearson \(\chi^2\) test indicates that the first and second questionnaires are dependent (\(\chi^2_1 = 8.0163\), p-value = 0.0046). Next, we use measures of association to assess the strength of the association.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/ref/w12{-}table2x2Calculator.txt"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{pander}\NormalTok{(}\FunctionTok{table2x2}\NormalTok{(case2), }\AttributeTok{caption=}\StringTok{"Measures of association"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3014}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1370}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1233}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2192}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2192}}@{}}
\caption{Measures of association (continued below)}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
measure
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
SE
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Lower(95\%).CI
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Upper(95\%).CI
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
measure
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
SE
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Lower(95\%).CI
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Upper(95\%).CI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{relative.risk} & 2.5 & 0.3073 & 1.3689 & 4.5658 \\
\textbf{risk.difference} & 0.45 & 0.128 & 0.1991 & 0.7009 \\
\textbf{odds.ratio} & 7 & 0.6522 & 1.9496 & 25.1334 \\
\end{longtable}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3056}}
  >{\centering\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3056}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
significance(0.05)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{relative.risk} & Yes \\
\textbf{risk.difference} & Yes \\
\textbf{odds.ratio} & Yes \\
\end{longtable}

Since the above \(2\times2\) table was generated based on cross-sectional design (assuming the random sampling). All measures of association are valid. The above results based on measures of association are consistent with that of the Pearson \(\chi^2\) test. Moreover, we can explicitly interpret the strength of the association between the two survey results.

\begin{itemize}
\item
  \textbf{relative risk}: the percentage of those who reported \textbf{high-dietary-intake} in the first survey also reported the \textbf{same} in the second survey is 2.5 times the percentage of those who reported \textbf{low-dietary-intake} in the first survey but reported \textbf{high-dietary-intake} in the second survey. The corresponding \(95\%\) of the confidence interval is \([1.37, 4.57]\).
\item
  \textbf{risk difference}: the percentage of those who reported \textbf{high-dietary-intake} in the first survey also reported the \textbf{same} in the second survey is \(45\%\) higher than the percentage of those who reported \textbf{low-dietary-intake} in the first survey but reported \textbf{high-dietary-intake} in the second survey. The corresponding \(95\%\) of confidence interval is \([19.9\%, 70.1\%]\).
\item
  \textbf{odds ratio}: The odds of reporting \textbf{high-dietary-intake} among those who reported \textbf{high-dietary-intake} in the first survey is 7 times that among those who reported \textbf{low-dietary-intake} in their first survey.
\end{itemize}

\hypertarget{practice-problems-4}{%
\section{Practice Problems}\label{practice-problems-4}}

In this assignment, we focus on the Pearson \(\chi^2\) test and the measures of association. The level of detail should be similar to what you have seen in the case study (Section 5 of the class note). You can also use the two R functions that were used in the class note by running the following one-line code in the code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/ref/w12{-}table2x2Calculator.txt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Problem 1}.

In a prospective, randomized, double-blind study, Stanley et al.~examined the relative efficacy and side effects of morphine and pethidine, drugs commonly used for patient-controlled analgesia (PCA). Subjects were 40 women, between the ages of 20 and 65 years, undergoing total abdominal hysterectomy. Patients were allocated randomly to receive morphine or pethidine by PCA. At the end of the study, subjects described their appreciation of nausea and vomiting, pain, and satisfaction by means of a three-point verbal scale. We only focus on the association between the severity of nausea and the use of the drug. The observed table is given below.

\begin{center}\includegraphics[width=0.8\linewidth]{img12/w12-assin-prob01} \end{center}

Is there any association between appreciation of nausea and satisfaction? Use the Pearson \(\chi^2\) test to justify your conclusion and interpret the test results.

\textbf{Problem 2}

The following data table was collected to study the obesity status of children ages 5--6 years and the smoking status of the mother during the pregnancy, also reported on another outcome variable: whether the child was born premature (37 weeks or fewer of gestation). The following table summarizes the results of this aspect of the study. The same risk factor (smoking during pregnancy) is considered, but a case is now defined as a mother who gave birth prematurely.

\begin{center}\includegraphics[width=0.8\linewidth]{img12/w12-assin-prob02} \end{center}

Use the chi-square test of independence to determine if one may conclude that there is an association between smoking throughout pregnancy and premature birth. Let \(\alpha = 05\). Compute the odds ratio to determine if smoking throughout pregnancy is related to premature birth.

\hypertarget{analysis-of-counts-and-rates}{%
\chapter{Analysis of Counts and Rates}\label{analysis-of-counts-and-rates}}

This module considers the relationship between a discrete response variable and other numeric or categorical predictor variables. The analysis of frequency counts and rates is also one of the important statistical tools in life science. The appropriate model we will discuss is a small family of generalized linear models - The Poisson regression model. It has wide applications for both laboratory experimental data and field data which involve count or rate data.

We add this model to the summary table of models in the previous module as follows

\begin{longtable}[]{@{}ccc@{}}
\toprule\noalign{}
Response variable & Predictor variable & Type of Models \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
continuous, normal & single categorical & ANOVA \\
continuous, normal & single continuous & SLR \\
continuous, normal & continuous or categorical & MLR (ANCOVA) \\
binary, categorical & continuous or categorical & logistic model \\
numeric, discrete & continuous or categorical & Poisson model \\
\end{longtable}

\hypertarget{motivational-examples}{%
\section{Motivational Examples}\label{motivational-examples}}

\textbf{Example 1}: Aerial counts of harbor seals (Phora vitulzna concoZw) on ledges along the Maine coast were conducted during the pupping season in 1981, 1986, 1993, 1997, and 2001 to study the changes in abundance of harbor seals. Detailed information on the study can be found in the published work of Gilbert et al (2005) \href{https://stat501.s3.amazonaws.com/w13-CHANGES-IN-ABUNDANCE-OF-HARBOR.pdf}{see the link of the article}.

\begin{center}\includegraphics[width=0.8\linewidth]{img13/w13-HarborSeal} \end{center}

We are interested in whether the counts of harbor seal counts changed significantly over the years. The data used to answer this question is taken from the first half of Table 6.

\begin{center}\includegraphics[width=0.8\linewidth]{img13/w13-HarborSealDataTable} \end{center}

We construct the R data set in the following based on the above data table that is suitable for modeling.

\textbf{Example 2}: This example is based on a study that investigated the cytotoxic and genotoxic effects of soluble and particulate hexavalent chromium in sperm whale skin fibroblasts. The data were extracted from a line plot in the published work of \href{https://stat501.s3.amazonaws.com/w13-TheGenotoxicityofParticulate+andSolubleChromate.pdf}{Wise et al (Fig. 1)}. In the first experiment, Particulate Cr(VI) induced a clear concentration-dependent decrease in cell survival over a range of 0.05 to 0.5 lg/cm2. Concentrations of 0.05, 0.1, 0.5, 1, and 5 lg/cm2 lead chromate induced 85\%, 86\%, 63\%, 56\%, and 26\% relative survival. This information is given in the following figure 1 of the published paper.

\begin{center}\includegraphics[width=0.8\linewidth]{img13/w13-CytotoxicityDataTable} \end{center}

\begin{longtable}[]{@{}ccc@{}}
\toprule\noalign{}
Dose level & survived cells & total cells \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 100 & 100 \\
0.05 & 85 & 100 \\
0.1 & 86 & 100 \\
0.5 & 63 & 100 \\
1 & 56 & 100 \\
5 & 26 & 100 \\
\end{longtable}

The question is whether the survival rates are associated with the dose level.

\hypertarget{poisson-regression-for-counts-and-rates}{%
\section{Poisson Regression for Counts and Rates}\label{poisson-regression-for-counts-and-rates}}

The Poisson regression model assumes the random response variable to be a frequency count or a rate of an uncommon event such as COVID-19 positivity rates, COVID-19 death mortality, etc. As in the linear and logistic regression models, we also assume that predictor variables are non-random.

\hypertarget{structure-and-interpretations}{%
\subsection{Structure and Interpretations}\label{structure-and-interpretations}}

Let \(Y\) be the response variable that takes on frequency counts as values and \(X\) be the set of predictor variables such as demographics and social determinants. Further, let \(\mu=E[Y]\) be the mean of the response variable.

\textbf{Poisson Regression for Counts}

The Poisson regression model is defined in the following analytic expression.

\[
\log(\mu) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p,
\]
where \(\beta_0, \beta_1, \cdots, \beta_p\) are coefficients of the Poisson regression model.

\textbf{Poisson Regression for Rates}

The Poisson regression model for rates is defined in the following analytic expression.

\[
\log(\mu/t) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p,
\]

where \(\beta_0, \beta_1, \cdots, \beta_p\) are coefficients of the Poisson regression model. \(t\) is called the \textbf{offset} variable. The offset variable serves to normalize the fitted cell means per some space, grouping, or time interval in order to define the meaningful rates.

\textbf{Interpretation of Regression Coefficients}

The interpretation of the regression coefficient \(\beta_i\) is as follows

\begin{itemize}
\item
  \(\beta_0\) = the baseline logarithm of the mean of \(Y\), \(\log(\mu)\), when all predictor variables \(x_i = 0\), for \(i = 1, 2, \cdots, p\). As usual, we are not interested in the inference of the intercept parameter.
\item
  \(\beta_i\) = is the change of the \textbf{logarithm of the mean count} due to one unit increases in \(x_i\) with all other \(x_j\) being fixed, for \(j\ne i\).
\end{itemize}

\textbf{Estimation of Regression Coefficients}

Estimating Poisson regression coefficients requires numerical optimization. We will not go into detail about how to estimate the regression coefficients and perform model diagnostics in this module. Instead, we will focus on data analysis, in particular, the interpretation of regression coefficients.

\hypertarget{assumptions-and-goodness-of-fit}{%
\subsection{Assumptions and Goodness-of-fit}\label{assumptions-and-goodness-of-fit}}

Like other statistical models, there are some assumptions for the Poisson regression model:

\begin{itemize}
\item
  The response variable is a frequency count (or rate variable) that follows the Poisson distribution.
\item
  The mean of the and the variance are equal.
\item
  The relationship between the mean of the response and the predictor variables is correct.
\item
  For a given value of the predictor variable, the mean and variance of the response variable are \textbf{equal}. - Unfortunately, this assumption is frequently violated. This type of violation is serious since it produces wrong estimates of the standard errors and, hence, yields wrong p-values.
\end{itemize}

\hypertarget{dispersion-issue-and-remedies}{%
\subsection{Dispersion Issue and Remedies}\label{dispersion-issue-and-remedies}}

The first three assumptions mentioned above are regular for all regression models. The violation of the last assumption is directly associated with the distribution of the response variable. Different causes lead to the violation. For example, the data is not from Poisson distribution or a mixture distribution of Poisson and other distributions. Therefore, there are different ways we can consider to fix the problem.

Although the detailed discussion of remedies is not the focus of this course, it is useful to know some of the available remedies for dispersion (either over-dispersion or under-dispersion).

\begin{itemize}
\item
  \textbf{quasi-Poisson regression} sticks to the simple structure of the Poisson regression and adjusts the dispersed standard error to obtain the valid p-values. We will use this approach in this class.
\item
  \textbf{negative binomial regression}, another family regression model for the discrete response variable, relaxes Poisson's assumption on equality of mean and variance. In the \textbf{negative binomial regression}, the variance is a function of the mean. It could also have dispersion problems if the variance function is not correct. The \textbf{negative binomial regression} is implemented in R.
\item
  \textbf{Zero-inflated family of regression models} assumes the data come from the mixture distribution of Poisson and other distributions such as binomial or negative binomial and binomial distributions. R also has libraries to fit several zero-inflated regression models.
\item
  The \textbf{Hurdle model} also handles the issue of excess zeros in the data but assumes the sources of zeros in the data are different. Hurdle assumes structural zero and the regular zero-inflated model assumes sampling zeros.
\end{itemize}

We will use \textbf{Poisson regression} to detect potential \textbf{dispersion} and then decide whether to use the regular Poisson regression model to report and implement in practical applications.

\hypertarget{data-structure-of-poisson-regression}{%
\subsection{Data Structure of Poisson Regression}\label{data-structure-of-poisson-regression}}

The Poisson regression is a subfamily of generalized linear regressions(GLM). The logistic regression is also a member of GLM. Similar to the structure used in the logistic regression, Poisson regression also requires the same data structure usually called the \textbf{long table}.

The data table in the first motivational example is not a \textbf{long table}. It is actually a \textbf{wide table}. The table in example 2 is a \textbf{long table}. When using R to build models in GLM, the data should always be in the form of a \textbf{long table}. Therefore, the data table in motivational example 1 \textbf{cannot} be used to build GLMs. The code for turning the wide table into a long table is given in Section 2. The resulting \textbf{long table} (partial table) is shown below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y1981}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{13}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{75}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{y1986}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{72}\NormalTok{, }\DecValTok{54}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{y1993}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{, }\DecValTok{41}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{44}\NormalTok{, }\DecValTok{148}\NormalTok{, }\DecValTok{123}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{38}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{19}\NormalTok{)}
\NormalTok{y1997}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{18}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{44}\NormalTok{, }\DecValTok{138}\NormalTok{, }\DecValTok{113}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{16}\NormalTok{)}
\NormalTok{y2001}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{18}\NormalTok{, }\DecValTok{43}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{47}\NormalTok{, }\DecValTok{125}\NormalTok{, }\DecValTok{107}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{41}\NormalTok{, }\DecValTok{25}\NormalTok{)}
\NormalTok{seal.vec }\OtherTok{=} \FunctionTok{c}\NormalTok{(y1981, y1986, y1993, y1997, y2001)}
\NormalTok{time.vec }\OtherTok{=} \FunctionTok{sort}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"y1981"}\NormalTok{, }\StringTok{"y1986"}\NormalTok{, }\StringTok{"y1993"}\NormalTok{, }\StringTok{"y1997"}\NormalTok{, }\StringTok{"y2001"}\NormalTok{), }\DecValTok{12}\NormalTok{))}
\NormalTok{pois.data}\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{seal=}\NormalTok{seal.vec, }\AttributeTok{time=}\NormalTok{time.vec))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n}\OtherTok{=}\FunctionTok{dim}\NormalTok{(pois.data)[}\DecValTok{1}\NormalTok{]}
\NormalTok{partial.long.table }\OtherTok{=}\NormalTok{ pois.data[}\FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, n, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{), ][}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{,]}
\FunctionTok{pander}\NormalTok{(partial.long.table, }\AttributeTok{caption=}\StringTok{"Part of the converted long table from the harbor seal data table"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1250}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0972}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1111}}@{}}
\caption{Part of the converted long table from the harbor seal data table}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
seal
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
time
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
seal
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
time
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{23} & 15 & y1986 \\
\textbf{12} & 10 & y1981 \\
\textbf{2} & 26 & y1981 \\
\textbf{14} & 22 & y1986 \\
\textbf{46} & 27 & y1997 \\
\textbf{6} & 75 & y1981 \\
\textbf{54} & 107 & y2001 \\
\textbf{33} & 30 & y1993 \\
\textbf{24} & 10 & y1986 \\
\textbf{36} & 19 & y1993 \\
\end{longtable}

\hypertarget{case-studies-3}{%
\section{Case Studies}\label{case-studies-3}}

We will use the two motivational examples in this section. As mentioned earlier, the Quasi-Poisson Regression is a generalization of the Poisson regression and is used when modeling an overdispersed count variable.

The Poisson model assumes that the variance is equal to the mean, which is not always a fair assumption. When the variance is greater than the mean, a Quasi-Poisson model, which assumes that the variance is a linear function of the mean, is more appropriate.

For each example, we will fit both Poisson and quasi-Poisson regression models.

\hypertarget{harbor-seal-data}{%
\subsection{Harbor Seal Data}\label{harbor-seal-data}}

We will use the \textbf{long table} to fit the two models. Which is to choose to report will depend on the dispersion parameter. Next, we use the R function \textbf{glm()} to fit Posson and quasi-Poisson models in the following.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pois.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(seal.vec }\SpecialCharTok{\textasciitilde{}}\NormalTok{ time.vec, }\AttributeTok{family=}\NormalTok{poisson, }\AttributeTok{data =}\NormalTok{ pois.data)}
\NormalTok{summary.table.pois }\OtherTok{=} \FunctionTok{summary}\NormalTok{(pois.model)}
\DocumentationTok{\#\#}
\NormalTok{quasi.pois.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(seal.vec }\SpecialCharTok{\textasciitilde{}}\NormalTok{ time.vec, }\AttributeTok{family=}\NormalTok{quasipoisson, }\AttributeTok{data =}\NormalTok{ pois.data)}
\NormalTok{summary.table.quasi.pois }\OtherTok{=} \FunctionTok{summary}\NormalTok{(quasi.pois.model)}
\end{Highlighting}
\end{Shaded}

The complete outputs of the two models from R function \textbf{glm()} are given in the following figure.

\begin{center}\includegraphics[width=0.8\linewidth]{img13/w13-PoissonQuasiPois-Output} \end{center}

We can observe several pieces of information from the above figure.

\begin{itemize}
\item
  The regression coefficients in both Poisson and quasi-Poisson regression models are identical.
\item
  The standard errors in the Poisson regression model are less than their corresponding standard errors in the quasi-Poisson regression model.
\item
  The dispersion parameter is forced to be 1 in the Poisson regression model. However, the dispersion parameter is calculated through the quasi-likelihood that yields the value of dispersion parameter 26.0157. This is much bigger than 1 (for the Poisson regression model). Therefore, the p-values in the output of the regular Poisson regression model are not reliable. The inference should be based on the output of the quasi-Poisson model.
\end{itemize}

The p-values in the quasi-Poisson regression can be extracted in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example.coef.table }\OtherTok{=}\NormalTok{ summary.table.quasi.pois}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{pander}\NormalTok{(example.coef.table, }\AttributeTok{caption=}\StringTok{"The summary statistics based on }
\StringTok{      the quasi{-}Poisson regression model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2532}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1392}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1646}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1266}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3165}}@{}}
\caption{The summary statistics based on
the quasi-Poisson regression model}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Std. Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
t value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pr(\textgreater\textbar t\textbar)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Std. Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
t value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pr(\textgreater\textbar t\textbar)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{(Intercept)} & 3.332 & 0.2783 & 11.98 & 0.00000000000000005951 \\
\textbf{time.vecy1986} & -0.1646 & 0.4107 & -0.4008 & 0.6901 \\
\textbf{time.vecy1993} & 0.5355 & 0.3504 & 1.528 & 0.1321 \\
\textbf{time.vecy1997} & 0.4763 & 0.3543 & 1.344 & 0.1843 \\
\textbf{time.vecy2001} & 0.5232 & 0.3512 & 1.49 & 0.1419 \\
\end{longtable}

All p-values associated with the survey year are insignificant. The baseline year is 1981 (which is not in the output), this means that counts of harbor seals in any of the survey years were \textbf{not significantly} from the baseline year (1981).

\hypertarget{toxicity-study}{%
\subsection{Toxicity Study}\label{toxicity-study}}

This case study is based on the second motivational example. The analysis in the original article involves errors. The statistical problem is a rate regression, the analysis in the original paper used ANOVA which led to a wrong conclusion. Two different methods will be used to assess the association between the concentration and the survival of cells.

\hypertarget{poisson-regression}{%
\subsubsection{Poisson Regression}\label{poisson-regression}}

We can simply use the summarized table extracted from the original article given in Section 2. This is a small data set with only 6 observations and three variables: dose(continuous), survival(discrete - count), and total (discrete - count, which can only be used as an offset variable in the model).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dose }\OtherTok{=}   \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{,  }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{,   }\DecValTok{5}\NormalTok{)}
\NormalTok{survived}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{85}\NormalTok{,  }\DecValTok{86}\NormalTok{,  }\DecValTok{63}\NormalTok{,  }\DecValTok{56}\NormalTok{,  }\DecValTok{26}\NormalTok{)}
\NormalTok{total }\OtherTok{=}  \FunctionTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{pois.data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{survived=}\NormalTok{survived, }\AttributeTok{dose=}\NormalTok{dose, }\AttributeTok{total =}\NormalTok{ total))}
\end{Highlighting}
\end{Shaded}

Next, we fit the quasi-Poisson to the above data and check the dispersion parameter to see whether the regular Poisson regression is appropriate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quasi.pois}\OtherTok{=}\FunctionTok{glm}\NormalTok{(survived }\SpecialCharTok{\textasciitilde{}}\NormalTok{ dose }\SpecialCharTok{+} \FunctionTok{offset}\NormalTok{(total), }\AttributeTok{family =}\NormalTok{ quasipoisson, }\AttributeTok{data =}\NormalTok{ pois.data)}
\NormalTok{disp }\OtherTok{=} \FunctionTok{summary}\NormalTok{(quasi.pois)}\SpecialCharTok{$}\NormalTok{dispersion}
\FunctionTok{pander}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{dispersion=}\NormalTok{disp), }\AttributeTok{caption =} \StringTok{"The dispersion paramter of the Poisson regression"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{0.1806}}@{}}
\caption{The dispersion paramter of the Poisson regression
The value of the dispersion is slightly bigger than 1 (if there is no dispersion, the dispersion parameter = 1). We only need to fit the Poisson regression to the data and use the fitted Poisson regression model to address the association between the concentration level and the survival rate.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
dispersion
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
dispersion
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1.579 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pois}\OtherTok{=}\FunctionTok{glm}\NormalTok{(survived }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(dose) }\SpecialCharTok{+} \FunctionTok{offset}\NormalTok{(total), }\AttributeTok{family =}\NormalTok{ poisson, }\AttributeTok{data =}\NormalTok{ pois.data)}
\NormalTok{coef}\OtherTok{=}\FunctionTok{summary}\NormalTok{(pois)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{pander}\NormalTok{(coef, }\AttributeTok{caption =} \StringTok{"Summary statistics of the regression coefficients"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3067}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1467}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1733}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1333}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2400}}@{}}
\caption{Summary statistics of the regression coefficients
Contrary to what was concluded in the original article, the concentration of lead chromate significantly affects the survival of the cells (p-value \(\approx\) 0). As the concentration level increases, the survival rate decreases significantly. To be more specific, we can exponentiate the coefficient of the Poisson regression associated with \textbf{dose} and obtain \(exp(-0.2625) = 0.7691 = 1 - 0.2309\). This means that, as the concentration increases by one unit, the \textbf{survival rate} of the skin cells \textbf{decreases} by 23.09\%.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Std. Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
z value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pr(\textgreater\textbar z\textbar)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Std. Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
z value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pr(\textgreater\textbar z\textbar)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{(Intercept)} & -95.39 & 0.1 & -953.9 & 0 \\
\textbf{factor(dose)0.05} & -0.1625 & 0.1475 & -1.102 & 0.2706 \\
\textbf{factor(dose)0.1} & -0.1508 & 0.1471 & -1.026 & 0.3051 \\
\textbf{factor(dose)0.5} & -0.462 & 0.1609 & -2.872 & 0.004073 \\
\textbf{factor(dose)1} & -0.5798 & 0.1669 & -3.474 & 0.0005129 \\
\textbf{factor(dose)5} & -1.347 & 0.2201 & -6.119 & 0.0000000009406 \\
\end{longtable}

\hypertarget{logistic-regression-approach-optional}{%
\subsubsection{Logistic Regression Approach (optional)}\label{logistic-regression-approach-optional}}

Since the logistic regression requires the response to be binary, we need to perform some data management to create a suitable data set for the logistic regression model.

\textbf{Data Preparation}

The data reported in the experiment are the percentage of survival of cells with the given total number of cells that died and survived respectively at different levels of concentration. We assume 100 cells were used at each concentration. So we retrieved the data table from the chart in the original paper and summarized it in Section 2. Next, we create a \textbf{long table} and fit the Poisson model to the data. The idea is each column will record the information of each cell. The layout of the long table to be used in the Poisson and quasi-Poisson regression model is depicted in the following:

\begin{longtable}[]{@{}cccc@{}}
\toprule\noalign{}
cell ID & dose & survival & total \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 0 & 1 & 1 \\
2 & 0 & 1 & 1 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} \\
100 & 0 & 1 & 1 \\
101 & 0.05 & 1 & 1 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} \\
185 & 0.05 & 1 & 1 \\
186 & 0.05 & 0 & 1 \\
187 & 0.05 & 0 & 1 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} \\
501 & 5 & 1 & 1 \\
502 & 5 & 1 & 1 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} \\
526 & 5 & 1 & 1 \\
527 & 5 & 0 & 1 \\
526 & 5 & 0 & 1 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} \\
599 & 5 & 0 & 1 \\
600 & 5 & 0 & 1 \\
\end{longtable}

We define the long table in the following code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cell.id }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\DecValTok{600}                     \CommentTok{\# cell ID, not a meaningful variable!}
\NormalTok{total }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{600}\NormalTok{)                  }\CommentTok{\# the "total" of each cell is simply equal to 1.}
\NormalTok{dose}\FloatTok{.0} \OtherTok{=} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{)                }\CommentTok{\# all 100 cells survived with concentration level 0}
\NormalTok{dose}\FloatTok{.005} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{85}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{15}\NormalTok{))  }\CommentTok{\# 85 cells survived and 15 died at concentration level 0.05}
\NormalTok{dose.}\FloatTok{0.1} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{86}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{14}\NormalTok{))  }\CommentTok{\# 86 cells survived and 14 died at concentration level 0.1}
\NormalTok{dose.}\FloatTok{0.5} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{63}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{37}\NormalTok{))  }\CommentTok{\# 63 cells survived and 37 died at concentration level 0.5}
\NormalTok{dose}\FloatTok{.1} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{56}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{44}\NormalTok{))   }\CommentTok{\# 56 cells survived and 44 died at concentration level 1}
\NormalTok{dose}\FloatTok{.5} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{26}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{74}\NormalTok{))    }\CommentTok{\# 26 cells survived and 74 died at concentration level 5}
\NormalTok{survival }\OtherTok{=} \FunctionTok{c}\NormalTok{(dose}\FloatTok{.0}\NormalTok{, dose}\FloatTok{.005}\NormalTok{, dose.}\FloatTok{0.1}\NormalTok{, dose.}\FloatTok{0.5}\NormalTok{, dose}\FloatTok{.1}\NormalTok{, dose}\FloatTok{.5}\NormalTok{)  }
\CommentTok{\# next line of code defines an indicator telling the concentration level of each cell}
\NormalTok{dose }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\DecValTok{100}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\DecValTok{100}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\DecValTok{100}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{100}\NormalTok{))}
\DocumentationTok{\#\# The long table is defined in the following one line of code}
\NormalTok{toxic.data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{( }\AttributeTok{cell.id =}\NormalTok{ cell.id, }\AttributeTok{survival =}\NormalTok{ survival, }\AttributeTok{dose=}\NormalTok{dose, }\AttributeTok{total =}\NormalTok{ total))}
\end{Highlighting}
\end{Shaded}

\textbf{Model Building}

This is a typical Poisson rate regression problem. As usual, we will fit both Poisson rate and quasi-Poisson rate models to the data set and then look at the dispersion parameter to decide which model should be used.

Based on the extracted data, the sample size is 600. The following quasi-Poisson regression model indicates that there is no significant dispersion issue for the Poisson regression model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logit.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(survival }\SpecialCharTok{\textasciitilde{}}\NormalTok{ dose, }\AttributeTok{family =}\NormalTok{ binomial, }\AttributeTok{data =}\NormalTok{ toxic.data)}
\NormalTok{logit.summary }\OtherTok{=} \FunctionTok{summary}\NormalTok{(logit.model)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{pander}\NormalTok{(logit.summary, }\AttributeTok{caption =} \StringTok{"Summary statistics on the regression coefficients"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2169}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1325}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1566}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1205}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3735}}@{}}
\caption{Summary statistics on the regression coefficients}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Std. Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
z value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pr(\textgreater\textbar z\textbar)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Std. Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
z value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pr(\textgreater\textbar z\textbar)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{(Intercept)} & 1.519 & 0.1199 & 12.67 & 9.23e-37 \\
\textbf{dose} & -0.5635 & 0.05662 & -9.952 & 0.00000000000000000000002462 \\
\end{longtable}

The regression coefficient associated with \textbf{dose} is negative meaning that as the dose increases the log-odds of survival decrease. To be more specific, we exponentiate the coefficient of \emph{dose} and obtain \$exp(-0.5635) = 0.5692 = 1 -43.08 \$. This means that, as the dose increase by one unit, the \textbf{odds of survival} of the skin cells \textbf{decreases} about 43.08\%.

\textbf{Conclusion}

In summary, the logistic regression yields the same result as that from the Poisson regression. The concentration of lead chromate is negatively associated with the survival of the skin cells of the sperm whale.

\hypertarget{pearson-chi2-test-of-independence-approach}{%
\subsubsection{\texorpdfstring{Pearson \(\chi^2\) Test of Independence Approach}{Pearson \textbackslash chi\^{}2 Test of Independence Approach}}\label{pearson-chi2-test-of-independence-approach}}

We can also answer the original question by testing the hypothesis that the concentration does not affect survival (independence test). This method only provides whether there is an association between concentration and survival but not the direction and magnitude of the association. To prepare for the Pearson \(\chi^2\) test, we need to construct a \(2\times k\) table in the following.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1625}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1375}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1875}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1375}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1375}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1125}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1250}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
surv.status
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
level-0.0
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
level-0.05
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
level-0.1
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
level-0.5
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
level-1
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
level-5
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
survived & 100 & 85 & 86 & 63 & 56 & 26 \\
died & 0 & 15 & 14 & 37 & 44 & 74 \\
\end{longtable}

We next construct the observed table in R and then perform the \(\chi^2\) test.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/STA501/main/ref/w12{-}table2x2Calculator.txt"}\NormalTok{)}
\NormalTok{survived}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{85}\NormalTok{,  }\DecValTok{86}\NormalTok{,  }\DecValTok{63}\NormalTok{,  }\DecValTok{56}\NormalTok{,  }\DecValTok{26}\NormalTok{)}
\NormalTok{died }\OtherTok{=} \DecValTok{100} \SpecialCharTok{{-}}\NormalTok{ survived}
\NormalTok{obs.table }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(}\AttributeTok{survived =}\NormalTok{ survived, }\AttributeTok{died =}\NormalTok{ died)}
\FunctionTok{colnames}\NormalTok{(obs.table) }\OtherTok{=} \FunctionTok{as.character}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{chi.test }\OtherTok{=} \FunctionTok{Pearson.chisq}\NormalTok{(obs.table)}\SpecialCharTok{$}\NormalTok{inference}
\FunctionTok{pander}\NormalTok{(chi.test, }\AttributeTok{caption=}\StringTok{"Pearson chi{-}square test of independence of concentration and survival"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1528}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1389}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0833}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4028}}@{}}
\caption{Pearson chi-square test of independence of concentration and survival}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
ts.stats
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
p.value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
d.f
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
method
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
ts.stats
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
p.value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
d.f
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
method
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
167.4018 & 0 & 5 & Pearson's Chi-squared test \\
\end{longtable}

The Pearson \(\chi^2\) test indicates that the concentration level of lead chromate is \textbf{NOT} independent of the survival of the skin cell of the sperm whale. Although the test itself does not give the direction of the association, we can find the information plot plotting the concentrations and survival rates.

\hypertarget{concluding-remarks-1}{%
\section{Concluding Remarks}\label{concluding-remarks-1}}

Comparing multiple unrelated proportions (rates) is one of the important methods in analyzing laboratory data. We have summarized several different methods above to address different types of comparison questions that may arise in the actual research hypotheses. It is not as straightforward as the comparison for multiple population means since it requires different and more advanced statistical tools to address the specific comparison questions.

There are several other recently developed procedures in the literature. Some of these new methods have been implemented in software packages.

\begin{itemize}
\item
  we can also use the command \textbf{prop.test()} to test the equality of multiple proportions.
\item
  The Marascuilo procedure enables us to simultaneously test the differences of all pairs of proportions when there are several populations under investigation
\item
  One most recently (2017) developed one-way ANOVA-like method uses the idea of the likelihood ratio test.
\end{itemize}

\hypertarget{practice-problems-5}{%
\section{Practice Problems}\label{practice-problems-5}}

We have assessed the association between the harbor seal count observed in Maine's coastal regions and time (different survey years between 1981 and 2001) using both Poisson and quasi-Poisson regression. In this assignment, you assess the association between the counts of harbor seal pubs and the time. The data table is given below (the frequency counts are in the red box of the following table)

\begin{center}\includegraphics[width=0.8\linewidth]{img13/w13-Assigment-HarborSeal} \end{center}

To save your time, define a vector for each survey year in the following code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y}\FloatTok{.1981}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{49}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{y}\FloatTok{.1986}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{39}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{7}\NormalTok{)}
\NormalTok{y}\FloatTok{.1993}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{112}\NormalTok{, }\DecValTok{97}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{12}\NormalTok{)}
\NormalTok{y}\FloatTok{.1997}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{92}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{9}\NormalTok{)}
\NormalTok{y}\FloatTok{.2001}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{17}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{113}\NormalTok{, }\DecValTok{101}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Please refer to the case study in the class note to analyze the data and draw conclusions to address the research question. To be more specific, you are expected to answer the following question.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Fit both regular and quasi-Poisson regression models.
\item
  Pick a model as the final model and justify your choice.
\item
  Comment on the dispersion and interpret the output of the final model.
\item
  Write a separate paragraph to summarize the results and draw a conclusion.
\end{enumerate}

\hypertarget{power-calculation-and-sample-size-determination}{%
\chapter{Power Calculation and Sample Size Determination}\label{power-calculation-and-sample-size-determination}}

Estimating the sample size for a prospective study is one of the first and most important tasks in classical statistics. In clinical studies, researchers need to determine the number of subjects to enroll in a clinical trial to guarantee the reliability of the study results. If the sample size is too small, it will not be able to achieve the power of the analysis. If the size is excessively larger than needed, it will waste resources.

Estimating the optimal number of patients is necessary for developing a Statistical Analysis Plan (SAP) in all prospective studies. In retrospective studies, we can determine how much power of the analysis (for example, the power of detecting a difference if one existed).

This chapter focuses on how to

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  estimate sample size for a prospective study (e.g., randomized controlled trial)
\item
  determine how much power we have to detect a difference if one existed (post hoc analysis)
\end{enumerate}

\textbf{\color{red}We also restrict our discussion within two-sample tests}.

\hypertarget{two-sample-proportion-problems}{%
\section{Two-sample Proportion Problems}\label{two-sample-proportion-problems}}

Before enrolling patients in a clinical trial, we want to make sure that we have the optimal number of patients for the study. Different statistical procedures require different optimal sample sizes to attain the target power. Next, we discuss sample size determination for several commonly used statistical procedures.

Before moving forward, let's recall two error probabilities associated with a statistical test of hypothesis \(\text{H}_{\text{0}}\):

\(\alpha = \text{P[type I error]} = \text{P[reject Ho | Ho is true]}\) = false positive rate

\(\beta = \text{P[type II error]} = \text{P[fail to reject Ho | Ho is false]}\) = false negative.

\(\text{power of a test} = 1 - \beta =\text{P[reject Ho | Ho is false]}\)

Several observations about \(\alpha\) and \(\beta\):

\begin{itemize}
\item
  \(alpha\) and \(\beta\) are negatively dependent on each other (not linearly dependent).
\item
  Estimated \(alpha\) and \(\beta\) are dependent on the sample size.
\end{itemize}

\begin{quote}
In general, if more than one test for testing a given hypothesis \(Ho\) under the same \(\alpha\), the one that achieves the lowest \(\beta\) is the best. This tells the relationship among estimated \(\alpha\), \(\beta\), power, and the sample size \(n\).
\end{quote}

\hypertarget{sample-size-estimations-for-two-proportions}{%
\subsection{\texorpdfstring{Sample size estimations for \textbf{two proportions}}{Sample size estimations for two proportions}}\label{sample-size-estimations-for-two-proportions}}

This is a common sample size calculation when you have two groups and the outcome is dichotomous (e.g., Yes / No)
Assume that we have a study that aims to estimate the efficacy of Treatment A versus Treatment B. The main endpoint is a dichotomous variable: ``Response'' or ``No response.''

The null hypothesis is:

\(H_{0}\): There is no difference in the proportion of responders between Treatment A and Treatment B

The alternative hypothesis is:

\(H_{a}\): There is a difference in the proportion of responders between Treatment A and Treatment B

\hfill\break

To determine the sample size needed to detect a meaningful difference in the proportion of responders between Treatment A and Treatment B, we need the following information:

\begin{itemize}
\item
  \textbf{\(\alpha\) level} (type I error) is the threshold where we determine whether something is statistically significantly different. We normally use an alpha level of 0.05 (type I error).
\item
  \textbf{Effect size} is the standardized difference one would expect to see between treatment groups. This is an estimate and something that is usually based on past studies.
\item
  \textbf{Power (1 - \(\beta\))} is the ability to correctly reject the null hypothesis if the actual effect of the population is equal to or greater than the specified effect size.
\end{itemize}

We'll set the two-tailed alpha level to 0.05 estimate size \(h\) using the following equation:

\[
 h = \varphi_{1} - \varphi_{2}
 \]
where \(\varphi_{i} = 2 * arcsine(\sqrt{p_{i}})\) and \(p_{i}\) denotes the proportion in treatment \(i\) that were classified as ``responders.'' In general, the power is set to 80\%.

With these pieces of information, we can estimate the sample size for a study with two proportions as the outcome using the following formula:

\[
\begin{aligned}
n_{i} = \frac{(Z_{\alpha/2} + Z_{1 - \beta})^2 + (p_{1}(1 - p_{1})) + (p_{2}(1 - p_{2}))}{(p_{1} - p_{2})^2}
\end{aligned}
\]

where \(n_{i}\) is the sample size for one group.

Use the \texttt{pwr.2p.test()} function in \texttt{library("pwr")} to find out the optimal sample size that meets the requirements on the alpha level (\texttt{alpha\ =\ 0.05}), effect size \(h\), and power level (\texttt{power\ =\ 80\%}).

\textbf{Example 1}: We consider Treatment A and Treatment B in a survey study. Set Treatment B as the reference (e.g., control) with a 50\% response rate. Assume that Treatment A is slightly better with a response rate of 60\%. With these pieces of information, we can estimate the sample size required to detect a difference in the ``response'' rate between Treatment A and Treatment B that is 10\% (\(p_{1}\) - \(p_{2}\)) with an alpha of 5\% and power of 80\%.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\# alpha = sig.level option and is equal to 0.05}
\DocumentationTok{\#\#\# power = 0.80}
\DocumentationTok{\#\#\# p1 = 0.60 }
\DocumentationTok{\#\#\# p2 = 0.50}
\NormalTok{power1 }\OtherTok{\textless{}{-}}\FunctionTok{pwr.2p.test}\NormalTok{(}\AttributeTok{h =} \FunctionTok{ES.h}\NormalTok{(}\AttributeTok{p1 =} \FloatTok{0.60}\NormalTok{, }\AttributeTok{p2 =} \FloatTok{0.50}\NormalTok{), }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{power =}\NormalTok{ .}\DecValTok{80}\NormalTok{)}
\NormalTok{power1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Difference of proportion power calculation for binomial distribution (arcsine transformation) 
## 
##               h = 0.2013579
##               n = 387.1677
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: same sample sizes
\end{verbatim}

The effect size \texttt{h} is 0.201, and the sample \texttt{n} is 387.2 or 388 rounded to the nearest whole number. This \texttt{n} is only for one group. Hence, based on the parameters of our study, we need approximately 388 patients in Treatment A and 388 patients in Treatment B to detect a difference of 10\% response or greater with an alpha of 0.05 and power of 80\%.

\hypertarget{power-analysis-for-two-proportions}{%
\subsection{\texorpdfstring{Power analysis for \textbf{two proportions}}{Power analysis for two proportions}}\label{power-analysis-for-two-proportions}}

Power (1 - \(\beta\)) is the ability to correctly reject the null hypothesis if the actual effect of the population is equal to or greater than the specified effect size. In other words, if you conclude that there is no difference in the sample, then there is no true difference in the population conditioned on the specified effect size

We can also use the plot feature to see how the power level changes with varying sample sizes. As the sample size goes up, power increases. As the sample size goes down, power decreases. This is important to understand. As we increase our sample size, we reduce the uncertainty around the estimates. By reducing this uncertainty, we gain greater precision in our estimates, which results in greater confidence in our ability to avoid making a type II error.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\# We can plot the power relative to different levels of the sample size. }
\FunctionTok{plot}\NormalTok{(power1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-228-1.pdf}

Let's change the \(p_{i}\) and see how the power level changes; we are fixing our sample size at 388 for each group with an alpha of 0.05.

We create a sequence of values by varying the proportion of ``responders'' for Treatment A. We will change these from 50\% to 100\% in intervals of 5\%.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\NormalTok{power1 }\OtherTok{\textless{}{-}}\FunctionTok{pwr.2p.test}\NormalTok{(}\AttributeTok{h =} \FunctionTok{ES.h}\NormalTok{(}\AttributeTok{p1 =}\NormalTok{ p1, }\AttributeTok{p2 =} \FloatTok{0.50}\NormalTok{),}
                     \AttributeTok{n =} \DecValTok{388}\NormalTok{,}
                     \AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{powerchange }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(p1, }\AttributeTok{power =}\NormalTok{ power1}\SpecialCharTok{$}\NormalTok{power }\SpecialCharTok{*} \DecValTok{100}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(powerchange}\SpecialCharTok{$}\NormalTok{p1, }
\NormalTok{     powerchange}\SpecialCharTok{$}\NormalTok{power, }
     \AttributeTok{type =} \StringTok{"b"}\NormalTok{, }
     \AttributeTok{xlab =} \StringTok{"Proportion of Responders in Treatment A"}\NormalTok{, }
     \AttributeTok{ylab =} \StringTok{"Power (\%)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-229-1.pdf}

We can also write a function for this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iteration }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(p\_i, P\_i, i\_i, p2, n, alpha) \{}
\NormalTok{              p1 }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(p\_i, P\_i, i\_i)}
\NormalTok{              power1 }\OtherTok{\textless{}{-}}\FunctionTok{pwr.2p.test}\NormalTok{(}\AttributeTok{h =} \FunctionTok{ES.h}\NormalTok{(}\AttributeTok{p1 =}\NormalTok{ p1, }\AttributeTok{p2 =}\NormalTok{ p2),}
                                   \AttributeTok{n =}\NormalTok{ n,}
                                   \AttributeTok{sig.level =}\NormalTok{ alpha)}
\NormalTok{              powerchange }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(p1, }\AttributeTok{power =}\NormalTok{ power1}\SpecialCharTok{$}\NormalTok{power }\SpecialCharTok{*} \DecValTok{100}\NormalTok{)}
\NormalTok{              powerchange}
\NormalTok{\}}

\FunctionTok{iteration}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.50}\NormalTok{, }\DecValTok{388}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      p1     power
## 1  0.50   5.00000
## 2  0.55  28.65038
## 3  0.60  80.08415
## 4  0.65  98.88117
## 5  0.70  99.99190
## 6  0.75 100.00000
## 7  0.80 100.00000
## 8  0.85 100.00000
## 9  0.90 100.00000
## 10 0.95 100.00000
## 11 1.00 100.00000
\end{verbatim}

\hypertarget{two-sample-mean-problems}{%
\section{Two-sample Mean Problems}\label{two-sample-mean-problems}}

\hypertarget{sample-size-estimations-for-two-averages}{%
\subsection{\texorpdfstring{Sample size estimations for \textbf{two averages}}{Sample size estimations for two averages}}\label{sample-size-estimations-for-two-averages}}

This is another common sample size estimation for two groups when the outcome is a continuous variable.

Now let's estimate the sample size for a study where we are comparing the averages between two groups. Let's suppose that we are working on a randomized controlled trial that seeks to evaluate the difference in the average change in hemoglobin A1c (HbA1c) from baseline between Treatment A and Treatment B.

\hfill\break

The null hypothesis is:

\(H_{0}\): There is no difference in the average change in HbA1c from baseline between Treatment A and Treatment B

The alternative hypothesis is:

\(H_{a}\): There is a difference in the average change in HbA1c from baseline between Treatment A and Treatment B

To determine the sample size needed to detect a meaningful difference in the average HbA1c change from baseline between Treatment A and Treatment B, we can use the following formula:

\[
\begin{aligned}
n_{i} = \frac{2 \sigma^2 * (Z_{\alpha/2} + Z_{1 - \beta})^2}{({\mu_{1} - \mu_{2}})^2}
\end{aligned}
\]

where \(n_{i}\) is the sample size for one of the groups, \(\mu_{1}\) and \(\mu_{2}\) are the average changes in HbA1c from baseline for Treatment A and Treatment B, respectively.

As usual, we'll set the two-tailed alpha level to 0.05. The effect size, also known as Cohen's \(d\), is estimated using the following equation:

\[
\begin{aligned}
d = \frac{\mu_{1} - \mu_{2}}{\sigma_{pooled}}
\end{aligned}
\]

The pooled standard deviation is estimated using the following formula:

\[
\begin{aligned}
\sigma_{pooled} = \sqrt{\frac{sd_{1}^2 + sd_{2}^2}{2}}
\end{aligned}
\]

Once again, the R \texttt{pwr} package can make this task easy for us. However, we'll need to estimate the Cohen's \(d\).

\textbf{Example 2}: Since we haven't started the study, we have to make some assumptions about each treatment strategy's change in HbA1c. Let's assume that the expected average change in HbA1c from baseline for Treatment A was 1.5\% with a standard deviation of 0.25\%. Additionally, let's assume that the expected average change in HbA1c from baseline for Treatment B was 1.0\% with a standard deviation of 0.20.

First, we'll calculate the pooled standard deviation (\(\sigma_{pooled}\)):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sd1 }\OtherTok{=} \FloatTok{0.25}
\NormalTok{sd2 }\OtherTok{=} \FloatTok{0.30}
\NormalTok{sd.pooled }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{((sd1}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{sd2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{)}
\NormalTok{sd.pooled}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.276134
\end{verbatim}

Once we have the \(\sigma_{pooled}\), we can estimate the Cohen's \(d\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu1 }\OtherTok{=} \FloatTok{1.5}
\NormalTok{mu2 }\OtherTok{=} \FloatTok{1.0}
\NormalTok{d }\OtherTok{=}\NormalTok{ (mu1 }\SpecialCharTok{{-}}\NormalTok{ mu2) }\SpecialCharTok{/}\NormalTok{ sd.pooled}
\NormalTok{d}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.810715
\end{verbatim}

Cohen's \(d\) is 1.81, which is considered a large effect size.

Now, we can take advantage of the \texttt{pwr} package and estimate the sample size needed to detect a difference of 0.5\% (1.5\% - 1.0\%) in the average HbA1c change from baseline between Treatment A and Treatment B with 80\% power and a significance threshold of 5\%.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\# d = Cohen\textquotesingle{}s d}
\DocumentationTok{\#\#\# power = 0.80}
\DocumentationTok{\#\#\# alpha = 0.05}
\NormalTok{n.i }\OtherTok{\textless{}{-}} \FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ d, }\AttributeTok{power =} \FloatTok{0.80}\NormalTok{, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{n.i}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Two-sample t test power calculation 
## 
##               n = 5.921286
##               d = 1.810715
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group
\end{verbatim}

Based on our study parameters, we need 6 patients in each group to detect a difference of 0.5\% or greater with 80\% power and a significance threshold of 5\%.

\hypertarget{power-analysis-for-two-averages}{%
\subsection{Power analysis for two averages}\label{power-analysis-for-two-averages}}

\textbf{Power} (1 - \(\beta\)) is the ability to correctly reject the null hypothesis if the actual effect of the population is equal to or greater than the specified effect size.

We can plot how the power will change as the sample size changes. As the sample size increases, power increases. This should make sense. Like our previous example with two proportions, as we increase our sample size, we reduce the uncertainty around the estimates. By reducing this uncertainty, we gain greater precision in our estimates, which results in greater confidence in our ability to avoid making a type II error.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\# We can plot the power relative to different levels of the sample size. }
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{nchange }\OtherTok{\textless{}{-}} \FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ d, }\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{nchange.df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(n, }\AttributeTok{power =}\NormalTok{ nchange}\SpecialCharTok{$}\NormalTok{power }\SpecialCharTok{*} \DecValTok{100}\NormalTok{)}
\NormalTok{nchange.df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     n    power
## 1   1      NaN
## 2   2 19.03307
## 3   3 39.61785
## 4   4 57.33850
## 5   5 70.87945
## 6   6 80.64997
## 7   7 87.42531
## 8   8 91.98145
## 9   9 94.96979
## 10 10 96.88938
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(nchange.df}\SpecialCharTok{$}\NormalTok{n, }
\NormalTok{     nchange.df}\SpecialCharTok{$}\NormalTok{power, }
     \AttributeTok{type =} \StringTok{"b"}\NormalTok{, }
     \AttributeTok{xlab =} \StringTok{"Sample size, n"}\NormalTok{, }
     \AttributeTok{ylab =} \StringTok{"Power (\%)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-235-1.pdf}

\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-236-1.pdf}

As the sample size increases, our power increases. This makes sense because we have more patients to detect differences that may be smaller. But we fixed our effect size (Cohen's \(d\)), so as we increase the sample size, our power to detect that difference ultimately increases.

Let's change the \(\mu_{i}\) and see how the power level changes; we are fixing our sample size at 6 for each group with an alpha of 0.05.

We create a sequence of values by varying the average change in HbA1c from the baseline for Treatment A. We will change these from 0\% to 2\% in intervals of 0.1\%.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu1 }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}

\NormalTok{d }\OtherTok{\textless{}{-}}\NormalTok{ (mu1 }\SpecialCharTok{{-}}\NormalTok{ mu2) }\SpecialCharTok{/}\NormalTok{ sd.pooled}

\NormalTok{power1 }\OtherTok{\textless{}{-}} \FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ d, }\AttributeTok{n =} \DecValTok{6}\NormalTok{, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{powerchange }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(d, }\AttributeTok{power =}\NormalTok{ power1}\SpecialCharTok{$}\NormalTok{power }\SpecialCharTok{*} \DecValTok{100}\NormalTok{)}
\NormalTok{powerchange}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            d     power
## 1  -3.621430 99.986261
## 2  -3.259287 99.898978
## 3  -2.897144 99.437136
## 4  -2.535001 97.615372
## 5  -2.172858 92.271971
## 6  -1.810715 80.649971
## 7  -1.448572 61.960800
## 8  -1.086429 39.817661
## 9  -0.724286 20.610368
## 10 -0.362143  8.785855
## 11  0.000000  5.000000
## 12  0.362143  8.785855
## 13  0.724286 20.610368
## 14  1.086429 39.817661
## 15  1.448572 61.960800
## 16  1.810715 80.649971
## 17  2.172858 92.271971
## 18  2.535001 97.615372
## 19  2.897144 99.437136
## 20  3.259287 99.898978
## 21  3.621430 99.986261
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(powerchange}\SpecialCharTok{$}\NormalTok{d, }
\NormalTok{     powerchange}\SpecialCharTok{$}\NormalTok{power, }
     \AttributeTok{type =} \StringTok{"b"}\NormalTok{, }
     \AttributeTok{xlab =} \StringTok{"Cohen\textquotesingle{}s d"}\NormalTok{, }
     \AttributeTok{ylab =} \StringTok{"Power (\%)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-238-1.pdf}

This figure shows how the power changes with Cohen's \(d\). It has a symmetrical pattern because of the negative and positive range associated with Cohen's \(d\). But the story is the same. As the effect size increases (negative and positive signs do not matter; we only care about the absolute values), the power increases. This makes sense because we only have enough power to detect large differences with the current sample size (which is fixed in this case). If the differences are small, then we do not have enough power with the current sample size of 6.

\hypertarget{paired-sample-problems}{%
\section{Paired-sample Problems}\label{paired-sample-problems}}

\hypertarget{sample-size-estimation-for-paired-data-before-and-after}{%
\subsection{\texorpdfstring{Sample size estimation for \textbf{paired data (before and after)}}{Sample size estimation for paired data (before and after)}}\label{sample-size-estimation-for-paired-data-before-and-after}}

So far, we discussed how to perform sample size estimations for ``between-groups'' comparisons. However, many studies investigate the ``within-group'' changes. These are paired data, which means that the observation for one data point is dependent on another observation. A common study design where paired data is collected is longitudinal studies. These types of studies involve repeated measures. For example, a pretest-posttest study design will measure a data point for a patient at baseline and then repeat that measurement at another point in time. In the figure, Patient A has two measurements at \(t_{0}\) and \(t_{f}\). Since these measurements were made in the same person, the change is ``within'' the patient. Alternatively, we can think of this as a ``repeated'' measure since the patient had the measurement performed twice.

\begin{figure}
\includegraphics[width=1\linewidth]{img14/w14-BeforeAfter} \caption{Figure caption: Pretest-Posttest (repeated measures) framework}\label{fig:unnamed-chunk-239}
\end{figure}

When you have a study where you are performing a paired t-test, you can use the same \texttt{pwr.t.test()} function for a two-sample test from the \texttt{pwr} package.

Let's assume that we want to conduct a prospective study to measure the weight change of a cohort of patients who started a diet. You want to enroll enough patients to detect a 5 lb reduction in weight 3 weeks after the diet started. Let's assume that at baseline the expected average weight for the cohort was 130 lbs with a standard deviation of 11. After 3 weeks of diet, the expected average weight was 125 lbs with a standard deviation of 12.

We can estimate the effect size (Cohen's \(d_{z}\)) for a paired t-test.

\[
\begin{aligned}
d_{z} = \frac{ | \mu_{z} |}{\sigma_{z}} = \frac{| \mu_{x} - \mu_{y} | }{ \sqrt{\sigma_{x}^{2} + \sigma_{y}^{2} - 2 \rho_{x, y} \sigma_{x} \sigma_{y}}}
\end{aligned}
\]

where \(x\) denotes ``before'' (or baseline), \(y\) denotes ``after'', \(d_{z}\) denotes Cohen's \(d\) for paired analysis \(\rho\) denotes the correlation between the measures before and after the diet. (For simplicity, I use 0.50 if I don't have prior information about this correlation.)

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\# Parameters for paired analysis or a pretest{-}posttest study design}

\NormalTok{mu\_x }\OtherTok{\textless{}{-}} \DecValTok{130}     \DocumentationTok{\#\#\# Average weight before the diet (baseline)}
\NormalTok{mu\_y }\OtherTok{\textless{}{-}} \DecValTok{125}     \DocumentationTok{\#\#\# Average weight after the diet}

\NormalTok{sd\_x }\OtherTok{\textless{}{-}} \DecValTok{11}      \DocumentationTok{\#\#\# Standard deviation before the diet}
\NormalTok{sd\_y }\OtherTok{\textless{}{-}} \DecValTok{12}      \DocumentationTok{\#\#\# Standard deviation after the diet}

\NormalTok{rho }\OtherTok{\textless{}{-}} \FloatTok{0.5}      \DocumentationTok{\#\#\# Correlation between measures before and after the diet}

\NormalTok{sd\_z }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(sd\_x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ sd\_y}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{rho}\SpecialCharTok{*}\NormalTok{sd\_x}\SpecialCharTok{*}\NormalTok{sd\_y)}
  
\NormalTok{d\_z }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(mu\_x }\SpecialCharTok{{-}}\NormalTok{ mu\_y) }\SpecialCharTok{/}\NormalTok{ sd\_z}
\NormalTok{d\_z}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.433555
\end{verbatim}

The Cohen's \(d_{z}\) is 0.433. We can input this into the \texttt{pwr.t.test()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n.paired }\OtherTok{\textless{}{-}} \FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ d\_z, }\AttributeTok{power =} \FloatTok{0.80}\NormalTok{, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{type =} \StringTok{"paired"}\NormalTok{)}
\NormalTok{n.paired}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Paired t test power calculation 
## 
##               n = 43.71557
##               d = 0.433555
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number of *pairs*
\end{verbatim}

We need 44 patients with two measurements (before and after) they implement their diet to detect a difference of 5 lbs or greater with 80\% power and a significance level of 0.05.

\hypertarget{power-analysis-of-paired-samples-paired-t-test}{%
\subsection{\texorpdfstring{Power analysis of \textbf{paired samples (paired t-test)}}{Power analysis of paired samples (paired t-test)}}\label{power-analysis-of-paired-samples-paired-t-test}}

We can plot how the power will change as the sample size changes for the paired t-test analysis. As the sample size increases, power increases. This should make sense. Like our previous examples, as we increase our sample size, we reduce the uncertainty around the estimates. By reducing this uncertainty, we gain greater precision in our estimates, which results in greater confidence in our ability to avoid making a type II error.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\# We can plot the power relative to different levels of the sample size for paired analysis. }
\NormalTok{n\_z }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{n\_z.change }\OtherTok{\textless{}{-}} \FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ d\_z, }\AttributeTok{n =}\NormalTok{ n\_z, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{type =} \StringTok{"paired"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in qt(sig.level/tside, nu, lower = FALSE): NaNs
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_z.change.df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(n\_z, }\AttributeTok{power =}\NormalTok{ n\_z.change}\SpecialCharTok{$}\NormalTok{power }\SpecialCharTok{*} \DecValTok{100}\NormalTok{)}
\NormalTok{n\_z.change.df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    n_z    power
## 1    1      NaN
## 2    6 14.03624
## 3   11 25.58334
## 4   16 36.84309
## 5   21 47.26307
## 6   26 56.56985
## 7   31 64.66154
## 8   36 71.54769
## 9   41 77.30572
## 10  46 82.04980
## 11  51 85.90929
## 12  56 89.01478
## 13  61 91.48950
## 14  66 93.44465
## 15  71 94.97744
## 16  76 96.17076
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(n\_z.change.df}\SpecialCharTok{$}\NormalTok{n, }
\NormalTok{     n\_z.change.df}\SpecialCharTok{$}\NormalTok{power, }
     \AttributeTok{type =} \StringTok{"b"}\NormalTok{, }
     \AttributeTok{xlab =} \StringTok{"Sample size, n"}\NormalTok{, }
     \AttributeTok{ylab =} \StringTok{"Power (\%)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-243-1.pdf}
\caption{\label{fig:unnamed-chunk-243}We increase the sample size from 1 to 80 at 5-unit intervals.}
\end{figure}

As the sample size increases, we generate more power to detect a difference of 5 lbs with a significance level of 0.05 and a fixed sample size of 44 patients with two measurements (before and after) they implement their diet.

Let's see how power changes when we change the effect size. Let's change the average weight after the patients implement their diet. Instead of an average of 125 lbs, let's see how the power will change when we reduce that to 100 lbs.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\# Vary the mu\_y from 50 lbs to 130 lbs in intervals of 5 lbs.}
\NormalTok{mu\_y }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{130}\NormalTok{, }\DecValTok{5}\NormalTok{)}

\NormalTok{d\_z }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(mu\_x }\SpecialCharTok{{-}}\NormalTok{ mu\_y) }\SpecialCharTok{/}\NormalTok{ sd\_z}

\NormalTok{n\_z.change }\OtherTok{\textless{}{-}} \FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ d\_z, }\AttributeTok{n =} \DecValTok{44}\NormalTok{, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{n\_z.change.df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(d\_z, }\AttributeTok{power =}\NormalTok{ n\_z.change}\SpecialCharTok{$}\NormalTok{power }\SpecialCharTok{*} \DecValTok{100}\NormalTok{)}
\NormalTok{n\_z.change.df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         d_z     power
## 1  6.936880 100.00000
## 2  6.503325 100.00000
## 3  6.069770 100.00000
## 4  5.636215 100.00000
## 5  5.202660 100.00000
## 6  4.769105 100.00000
## 7  4.335550 100.00000
## 8  3.901995 100.00000
## 9  3.468440 100.00000
## 10 3.034885 100.00000
## 11 2.601330 100.00000
## 12 2.167775 100.00000
## 13 1.734220 100.00000
## 14 1.300665  99.99766
## 15 0.867110  98.03639
## 16 0.433555  52.03146
## 17 0.000000   5.00000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(n\_z.change.df}\SpecialCharTok{$}\NormalTok{d\_z, }
\NormalTok{     n\_z.change.df}\SpecialCharTok{$}\NormalTok{power, }
     \AttributeTok{type =} \StringTok{"b"}\NormalTok{, }
     \AttributeTok{xlab =} \StringTok{"Cohen\textquotesingle{}s d\_z"}\NormalTok{, }
     \AttributeTok{ylab =} \StringTok{"Power (\%)"}\NormalTok{,}
     \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-244-1.pdf}
\caption{\label{fig:unnamed-chunk-244}We vary the average weight \(\mu_{y}\) between 50 lbs and 130 lbs in intervals of 5 lbs.}
\end{figure}

When we increase the effect size (Cohen's \(d_{z}\)), our power goes up; recall that the sample size is fixed at 44 and the significance level is 0.05. But when the effect size gets smaller (or when the average weight loss shrinks), we lose the power to detect a difference because our sample size is too small. We'll need to increase our sample size to have a reasonable power to detect small differences.

We can also see how power changes when we vary \(\rho\). If we set \(\rho\) = 0, then the Cohen's \(d_{z}\) = 0.307. If we set \(\rho\) = 1, then the Cohen's \(d_{z}\) = 5.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu\_x }\OtherTok{\textless{}{-}} \DecValTok{130}     \DocumentationTok{\#\#\# Average weight before the diet (baseline)}
\NormalTok{mu\_y }\OtherTok{\textless{}{-}} \DecValTok{125}     \DocumentationTok{\#\#\# Average weight after the diet}

\NormalTok{sd\_x }\OtherTok{\textless{}{-}} \DecValTok{11}      \DocumentationTok{\#\#\# Standard deviation before the diet}
\NormalTok{sd\_y }\OtherTok{\textless{}{-}} \DecValTok{12}      \DocumentationTok{\#\#\# Standard deviation after the diet}

\NormalTok{sd\_z\_1 }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(sd\_x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ sd\_y}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{2}\SpecialCharTok{*}\DecValTok{1}\SpecialCharTok{*}\NormalTok{sd\_x}\SpecialCharTok{*}\NormalTok{sd\_y)}
\NormalTok{sd\_z\_0 }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(sd\_x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ sd\_y}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{2}\SpecialCharTok{*}\DecValTok{0}\SpecialCharTok{*}\NormalTok{sd\_x}\SpecialCharTok{*}\NormalTok{sd\_y)}

\NormalTok{d\_z\_1 }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(mu\_x }\SpecialCharTok{{-}}\NormalTok{ mu\_y) }\SpecialCharTok{/}\NormalTok{ sd\_z\_1}
\NormalTok{d\_z\_0 }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(mu\_x }\SpecialCharTok{{-}}\NormalTok{ mu\_y) }\SpecialCharTok{/}\NormalTok{ sd\_z\_0}

\NormalTok{d\_z\_1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d\_z\_0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3071476
\end{verbatim}

So, higher \(\rho\) results in large \(d_{z}\) and smaller \(\rho\) results in small \(d_{z}\) values.

Let's see how power changes when we change the \(\rho\) range from 0 to 1 in intervals of 0.1 units.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rho }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}

\NormalTok{sd\_z }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(sd\_x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ sd\_y}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{rho}\SpecialCharTok{*}\NormalTok{sd\_x}\SpecialCharTok{*}\NormalTok{sd\_y)}

\NormalTok{d\_z }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(mu\_x }\SpecialCharTok{{-}}\NormalTok{ mu\_y) }\SpecialCharTok{/}\NormalTok{ sd\_z}

\NormalTok{rho.change }\OtherTok{\textless{}{-}} \FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ d\_z, }\AttributeTok{n =} \DecValTok{44}\NormalTok{, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{rho.change.df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(d\_z, }\AttributeTok{power =}\NormalTok{ rho.change}\SpecialCharTok{$}\NormalTok{power }\SpecialCharTok{*} \DecValTok{100}\NormalTok{)}
\NormalTok{rho.change.df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          d_z     power
## 1  0.3071476  29.65480
## 2  0.3236941  32.35129
## 3  0.3432395  35.66281
## 4  0.3668151  39.80733
## 5  0.3960280  45.10546
## 6  0.4335550  52.03146
## 7  0.4842743  61.25990
## 8  0.5583195  73.54735
## 9  0.6816774  88.52181
## 10 0.9552009  99.32401
## 11 5.0000000 100.00000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(rho.change.df}\SpecialCharTok{$}\NormalTok{d\_z, }
\NormalTok{     rho.change.df}\SpecialCharTok{$}\NormalTok{power, }
     \AttributeTok{type =} \StringTok{"b"}\NormalTok{, }
     \AttributeTok{xlab =} \StringTok{"Cohen\textquotesingle{}s d\_z"}\NormalTok{, }
     \AttributeTok{ylab =} \StringTok{"Power (\%)"}\NormalTok{,}
     \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{STA501EB_files/figure-latex/unnamed-chunk-247-1.pdf}
\caption{\label{fig:unnamed-chunk-247}We vary \(\rho\) from 0 to 1 at intervals of 0.1 unit.}
\end{figure}

As \(\rho\) increases, our power increases. This makes sense because we are nearing ``perfect'' correlation, which would require less sample to detect a difference if one existed. As the correlation becomes less ``perfect'' our power drops suggesting that we need to increase our sample size to make up for this poor correlation.

\hypertarget{unequal-sample-sizes-problems}{%
\section{Unequal Sample Sizes Problems}\label{unequal-sample-sizes-problems}}

\hypertarget{power-analysis-with-unequal-sample-sizes}{%
\subsection{\texorpdfstring{Power analysis with \textbf{unequal sample sizes}}{Power analysis with unequal sample sizes}}\label{power-analysis-with-unequal-sample-sizes}}

It is common for the sample size to be different. The \texttt{pwr.t2n.test()} is a useful tool to help estimate the power given the sample sizes of the study.

It is common to perform power analysis on a study where the sample sizes between groups are different.

Suppose you have a retrospective study where the patients were prescribed Treatment A and Treatment B. There were 130 patients in Treatment A (\(n_{A}\) = 130) and 120 patients in Treatment B (\(n_{B}\) = 120). The average change in HbA1c was 1.5\% with a standard deviation of 1.25\% in Treatment A, and the average change in HbA1c was 1.4\% with a standard deviation of 1.01\% in Treatment B.

First, we'll calculate the pooled standard deviation (\(\sigma_{pooled}\)):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sd1 }\OtherTok{\textless{}{-}} \FloatTok{1.25}
\NormalTok{sd2 }\OtherTok{\textless{}{-}} \FloatTok{1.01}
\NormalTok{sd\_pooled }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{((sd1}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{sd2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{)}
\NormalTok{sd\_pooled}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.136354
\end{verbatim}

Once we have the \(\sigma_{pooled}\), we can estimate the Cohen's \(d\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu1 }\OtherTok{\textless{}{-}} \FloatTok{1.5}
\NormalTok{mu2 }\OtherTok{\textless{}{-}} \FloatTok{1.4}
\NormalTok{d }\OtherTok{\textless{}{-}}\NormalTok{ (mu1 }\SpecialCharTok{{-}}\NormalTok{ mu2) }\SpecialCharTok{/}\NormalTok{ sd\_pooled}
\NormalTok{d}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.08800076
\end{verbatim}

Now, we can estimate the power with the different sample sizes across the groups (\(n_{A}\) = 130, \(n_{B}\) = 120).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n1 }\OtherTok{\textless{}{-}} \DecValTok{130}
\NormalTok{n2 }\OtherTok{\textless{}{-}} \DecValTok{120}

\NormalTok{power.diff\_n }\OtherTok{\textless{}{-}} \FunctionTok{pwr.t2n.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ d, }\AttributeTok{n1 =}\NormalTok{ n1, }\AttributeTok{n2 =}\NormalTok{ n2, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{power.diff\_n}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      t test power calculation 
## 
##              n1 = 130
##              n2 = 120
##               d = 0.08800076
##       sig.level = 0.05
##           power = 0.1064836
##     alternative = two.sided
\end{verbatim}

Since the average HbA1c change from baseline for Treatment A is 1.5\% and 1.4\% for Treatment B, the average difference in the HbA1c change from baseline is 0.1\%. This is a difference (difference between the groups) of the differences (difference from baseline within the group) calculation.

You only have 11\% power to detect a difference of 0.10\% or greater in the HbA1c change from baseline. This means that you are underpowered to detect a difference of 0.10\% or greater in the HbA1c change from baseline with \(n_{A}\) = 130, \(n_{B}\) = 120, and a significance level of 0.05. When studies are underpowered, there is a high potential for type II error. The only way to address this problem is to enroll more patients or expand the sample by relaxing inclusion criteria. However, this may increase the threats to the study's internal validity.

\hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

Sample size estimations and power analysis are very useful tools to determine how many patients you need in your study and how confident you are that you didn't make a type II error. Depending on the type of study, you will need to use different functions from the \texttt{pwr} package. I highly encourage you to explore the other functions of the \texttt{pwr} package to see if those fit the study design you have planned.

  \bibliography{book.bib,packages.bib}

\end{document}
